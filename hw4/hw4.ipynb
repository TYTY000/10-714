{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset. \n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /root/miniconda3/lib/python3.11/site-packages/pybind11/include (found version \"2.11.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Tue Feb 20 22:15:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.33                 Driver Version: 546.24       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   37C    P8               3W / 140W |    977MiB /  8188MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "-- Configuring done (0.9s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /mnt/d/project/10-714/hw4/build\n",
      "make[1]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[1]: Warning: File 'Makefile' has modification time 78 s in the future\n",
      "make[2]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[2]: Warning: File 'CMakeFiles/Makefile2' has modification time 78 s in the future\n",
      "make[3]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Warning: File 'CMakeFiles/ndarray_backend_cpu.dir/progress.make' has modification time 78 s in the future\n",
      "make[3]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[3]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Warning: File 'CMakeFiles/ndarray_backend_cpu.dir/progress.make' has modification time 78 s in the future\n",
      "make[3]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[3]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Warning: File 'CMakeFiles/ndarray_backend_cuda.dir/progress.make' has modification time 78 s in the future\n",
      "make[3]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[3]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Entering directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[3]: Warning: File 'CMakeFiles/ndarray_backend_cuda.dir/progress.make' has modification time 78 s in the future\n",
      "make[3]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[3]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[2]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n",
      "make[1]: warning:  Clock skew detected.  Your build may be incomplete.\n",
      "make[1]: Leaving directory '/mnt/d/project/10-714/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n",
    "\n",
    "Fill in the following classes in `python/needle/ops_logarithmic.py` and `python/needle/ops_mathematic.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend. \n",
    "\n",
    "`TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  0%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  1%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  2%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  3%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  4%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  7%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  9%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 12%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 14%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 15%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 17%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 18%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 19%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 20%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 21%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 23%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 24%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 26%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 28%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 29%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 30%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 31%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 32%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 34%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 35%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 36%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 37%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 39%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 40%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 41%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 42%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 43%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 45%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 46%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 47%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 48%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 49%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 51%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 52%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 53%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 54%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 56%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 57%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 63%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 64%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 65%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 67%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 68%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 69%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 80%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 81%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 84%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 85%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 86%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 87%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 89%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 90%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 91%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 92%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 93%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 95%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 96%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 97%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 98%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 99%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m118 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[32m in 5.77s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n",
    "Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 10%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 20%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] ^C\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"test_cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops_mathematic.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn/nn_conv.py`, implement:\n",
    "- `Conv`\n",
    "\n",
    "In `apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_ml.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation. \n",
    "\n",
    "**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 2.74s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m40 passed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[32m in 4.25s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m26 passed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[32m in 4.02s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m34 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[32m in 3.47s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
    "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
    "\n",
    "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m34 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[32m in 3.86s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not None, then ignore `fan_in` and `fan_out` but use the value of `shape` for initializations.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 3.25s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 3.71s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"train_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.__init__() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\\\n\u001b[1;32m     11\u001b[0m          dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     12\u001b[0m          batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     13\u001b[0m          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet9(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain_cifar10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m evaluate_cifar10(model, dataloader)\n",
      "File \u001b[0;32m/mnt/d/project/10-714/hw4/./apps/simple_ml.py:160\u001b[0m, in \u001b[0;36mtrain_cifar10\u001b[0;34m(model, dataloader, n_epochs, optimizer, lr, weight_decay, loss_fn)\u001b[0m\n\u001b[1;32m    158\u001b[0m avg_acc, avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 160\u001b[0m     avg_acc, avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_general_cifar10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Accuracy=> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss=> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_acc, avg_loss\n",
      "File \u001b[0;32m/mnt/d/project/10-714/hw4/./apps/simple_ml.py:125\u001b[0m, in \u001b[0;36mepoch_general_cifar10\u001b[0;34m(dataloader, model, loss_fn, opt)\u001b[0m\n\u001b[1;32m    123\u001b[0m y \u001b[38;5;241m=\u001b[39m Tensor(y, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m--> 125\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt:\n\u001b[1;32m    127\u001b[0m     opt\u001b[38;5;241m.\u001b[39mreset_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.__init__() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_ml import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,)\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1163 deselected / 640 selected                          \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m==================== \u001b[32m\u001b[1m640 passed\u001b[0m, \u001b[33m1163 deselected\u001b[0m\u001b[32m in 10.39s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1483 deselected / 320 selected                          \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m=============== \u001b[32m\u001b[1m320 passed\u001b[0m, \u001b[33m1483 deselected\u001b[0m\u001b[32m in 183.50s (0:03:03)\u001b[0m\u001b[32m ===============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    "┌ a g m s ┐\n",
    "│ b h n t │\n",
    "│ c i o u │\n",
    "│ d j p v │\n",
    "│ e k q w │\n",
    "└ f l r x ┘\n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    "┌ a g m s ┐ ┌ b h n t ┐\n",
    "└ b h n t ┘ └ c i o u ┘\n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m     [  7%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 42%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m26 passed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[32m in 26.25s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1291 deselected / 512 selected                          \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-1] _________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[0.3503833]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[0.446467]]])\n",
      "h0         = needle.Tensor([[[0.446467]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5664a410>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[0.446467]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5664a410>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[0.446467]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd5664a410>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.20401418]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.20401418]]]), needle.Tensor([[[0.446467]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5664a950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.20401418]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.446467]])\n",
      "        h0         = (needle.Tensor([[0.446467]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.20401418]])\n",
      "        inputs     = [needle.Tensor([[0.20401418]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5664ab50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5664a950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.20401418]]), needle.Tensor([[0.446467]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5664ab50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.20401418]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.1426965]])\n",
      "        bias_ih    = needle.Tensor([[-0.19517577]])\n",
      "        h          = needle.Tensor([[0.446467]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5664ab50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.31126958]])\n",
      "        self       = needle.Tensor([[0.20401418]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.20401418]]), needle.Tensor([[-0.31126958]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57bcd210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.20401418]]), needle.Tensor([[-0.31126958]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57bcd210>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd562203b0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd57bce7d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd56220630>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd57bce7d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.20401418]], device=cpu())\n",
      "        b          = NDArray([[-0.31126958]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57bcd210>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.20401418]], device=cpu())\n",
      "        b          = NDArray([[-0.31126958]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.20401418]], device=cpu())\n",
      "other = NDArray([[-0.31126958]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57bcd030>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55cdbd70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57bceab0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.31126958]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.20401418]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.585165]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[1.3610331]]])\n",
      "h0         = needle.Tensor([[[1.3610331]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd55d1ee90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[1.3610331]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55d1ee90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[1.3610331]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd55d1ee90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.80...63]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]]), needle.Tensor([[[1.3610331]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55c8fe10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]\n",
      "\n",
      " [[0.8037463]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[1.3610331]])\n",
      "        h0         = (needle.Tensor([[1.3610331]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8037463]])\n",
      "        inputs     = [needle.Tensor([[0.8037463]]), needle.Tensor([[0.8037463]]), needle.Tensor([[0.8037463]]), needle.Tensor([[0.8037463]]), needle.Tensor([[0.8037463]]), needle.Tensor([[0.8037463]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55c54590>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55c8fe10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8037463]]), needle.Tensor([[1.3610331]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55c54590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8037463]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.80971545]])\n",
      "        bias_ih    = needle.Tensor([[-0.21621543]])\n",
      "        h          = needle.Tensor([[1.3610331]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55c54590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.43450856]])\n",
      "        self       = needle.Tensor([[0.8037463]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8037463]]), needle.Tensor([[-0.43450856]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c5c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8037463]]), needle.Tensor([[-0.43450856]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c5c50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd561d00f0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd561c5c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd561d1970>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd561c5c90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8037463]], device=cpu())\n",
      "        b          = NDArray([[-0.43450856]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c5c50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8037463]], device=cpu())\n",
      "        b          = NDArray([[-0.43450856]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8037463]], device=cpu())\n",
      "other = NDArray([[-0.43450856]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a0d8b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55c56430>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561c71f0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.43450856]], device=cuda())\n",
      "out        = NDArray([[6.979243e-35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8037463]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-1] _________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.2272418 ]]\n",
      "\n",
      " [[ 0.61170876]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[0.24183077]]\n",
      "\n",
      " [[0.52630097]]])\n",
      "h0         = needle.Tensor([[[0.24183077]]\n",
      "\n",
      " [[0.52630097]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57b52210>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[0.24183077]]\n",
      "\n",
      " [[0.52630097]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57b52210>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[0.24183077]]\n",
      "\n",
      " [[0.52630097]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57b52210>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8317771]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8317771]]]), needle.Tensor([[[0.24183077]]\n",
      "\n",
      " [[0.52630097]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57b53810>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8317771]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.24183077]])\n",
      "        h0         = (needle.Tensor([[0.24183077]]), needle.Tensor([[0.52630097]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8317771]])\n",
      "        inputs     = [needle.Tensor([[0.8317771]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57b51450>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57b53810>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8317771]]), needle.Tensor([[0.24183077]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57b51450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8317771]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.7149757]])\n",
      "        bias_ih    = needle.Tensor([[0.23751783]])\n",
      "        h          = needle.Tensor([[0.24183077]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57b51450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.47756577]])\n",
      "        self       = needle.Tensor([[0.8317771]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8317771]]), needle.Tensor([[0.47756577]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57b52350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8317771]]), needle.Tensor([[0.47756577]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57b52350>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5617df30>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd57b52b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5617c1f0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd57b52b90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8317771]], device=cpu())\n",
      "        b          = NDArray([[0.47756577]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57b52350>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8317771]], device=cpu())\n",
      "        b          = NDArray([[0.47756577]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8317771]], device=cpu())\n",
      "other = NDArray([[0.47756577]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57b539b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57b51ab0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57b51bb0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.47756577]], device=cuda())\n",
      "out        = NDArray([[2.0018632e-34]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8317771]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.5021566 ]]\n",
      "\n",
      " [[-0.00181767]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-1.7308563]]\n",
      "\n",
      " [[-1.2284259]]])\n",
      "h0         = needle.Tensor([[[-1.7308563]]\n",
      "\n",
      " [[-1.2284259]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58cc4050>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-1.7308563]]\n",
      "\n",
      " [[-1.2284259]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58cc4050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-1.7308563]]\n",
      "\n",
      " [[-1.2284259]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58cc4050>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " ... [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]]), needle.Tensor([[[-1.7308563]]\n",
      "\n",
      " [[-1.2284259]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58954050>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]\n",
      "\n",
      " [[0.48849142]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-1.7308563]])\n",
      "        h0         = (needle.Tensor([[-1.7308563]]), needle.Tensor([[-1.2284259]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.48849142]])\n",
      "        inputs     = [needle.Tensor([[0.48849142]]), needle.Tensor([[0.48849142]]), needle.Tensor([[0.48849142]]), needle.Tensor([[0.48849142]]), needle.Tensor([[0.48849142]]), needle.Tensor([[0.48849142]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56307e50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58954050>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.48849142]]), needle.Tensor([[-1.7308563]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56307e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.48849142]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.597139]])\n",
      "        bias_ih    = needle.Tensor([[-0.57403016]])\n",
      "        h          = needle.Tensor([[-1.7308563]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56307e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.98810494]])\n",
      "        self       = needle.Tensor([[0.48849142]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.48849142]]), needle.Tensor([[0.98810494]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58695710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.48849142]]), needle.Tensor([[0.98810494]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58695710>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd566c5fb0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58697d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd566c58f0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58697d90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.48849142]], device=cpu())\n",
      "        b          = NDArray([[0.98810494]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58695710>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.48849142]], device=cpu())\n",
      "        b          = NDArray([[0.98810494]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.48849142]], device=cpu())\n",
      "other = NDArray([[0.98810494]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56306330>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56307330>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd586968f0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.98810494]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.48849142]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.5094704 ]\n",
      "  [ 0.16905887]\n",
      "  [-0.51277554]\n",
      "  [-0.36127254]\n",
      "  [ 0.7451835 ]\n",
      "  [-0.80277014]\n",
      "  [ 1.38...19 ]\n",
      "  [ 0.62007385]\n",
      "  [-0.5450092 ]\n",
      "  [-0.03422209]\n",
      "  [-1.3134365 ]\n",
      "  [-0.15523662]\n",
      "  [ 0.7821316 ]\n",
      "  [ 0.5372081 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 0.20648915]\n",
      "  [ 0.73045623]\n",
      "  [ 0.34197843]\n",
      "  [ 0.22317761]\n",
      "  [ 1.1291592 ]\n",
      "  [-0.65776926]\n",
      "  [-0.47...28 ]\n",
      "  [-0.055869  ]\n",
      "  [-1.684367  ]\n",
      "  [ 1.1544775 ]\n",
      "  [-2.488423  ]\n",
      "  [ 0.4969537 ]\n",
      "  [ 1.8054023 ]\n",
      "  [-1.041959  ]]])\n",
      "h0         = needle.Tensor([[[ 0.20648915]\n",
      "  [ 0.73045623]\n",
      "  [ 0.34197843]\n",
      "  [ 0.22317761]\n",
      "  [ 1.1291592 ]\n",
      "  [-0.65776926]\n",
      "  [-0.47...28 ]\n",
      "  [-0.055869  ]\n",
      "  [-1.684367  ]\n",
      "  [ 1.1544775 ]\n",
      "  [-2.488423  ]\n",
      "  [ 0.4969537 ]\n",
      "  [ 1.8054023 ]\n",
      "  [-1.041959  ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58081b10>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 0.20648915]\n",
      "  [ 0.73045623]\n",
      "  [ 0....8 ]\n",
      "  [-0.055869  ]\n",
      "  [-1.684367  ]\n",
      "  [ 1.1544775 ]\n",
      "  [-2.488423  ]\n",
      "  [ 0.4969537 ]\n",
      "  [ 1.8054023 ]\n",
      "  [-1.041959  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58081b10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.20648915]\n",
      "  [ 0.73045623]\n",
      "  [ 0.34197843]\n",
      "  [ 0.22317761]\n",
      "  [ 1.1291592 ]\n",
      "  [-0.65776926]\n",
      "  [-0.47...28 ]\n",
      "  [-0.055869  ]\n",
      "  [-1.684367  ]\n",
      "  [ 1.1544775 ]\n",
      "  [-2.488423  ]\n",
      "  [ 0.4969537 ]\n",
      "  [ 1.8054023 ]\n",
      "  [-1.041959  ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58081b10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0....8 ]\n",
      "  [-0.055869  ]\n",
      "  [-1.684367  ]\n",
      "  [ 1.1544775 ]\n",
      "  [-2.488423  ]\n",
      "  [ 0.4969537 ]\n",
      "  [ 1.8054023 ]\n",
      "  [-1.041959  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58082650>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]\n",
      "  [0.7362121]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.20648915]\n",
      " [ 0.73045623]\n",
      " [ 0.34197843]\n",
      " [ 0.22317761]\n",
      " [ 1.1291592 ]\n",
      " [-0.65776926]\n",
      " [-0.47573593]\n",
      " [ 0.6968828 ]\n",
      " [-0.055869  ]\n",
      " [-1.684367  ]\n",
      " [ 1.1544775 ]\n",
      " [-2.488423  ]\n",
      " [ 0.4969537 ]\n",
      " [ 1.8054023 ]\n",
      " [-1.041959  ]])\n",
      "        h0         = (needle.Tensor([[ 0.20648915]\n",
      " [ 0.73045623]\n",
      " [ 0.34197843]\n",
      " [ 0.22317761]\n",
      " [ 1.1291592 ]\n",
      " [-0.65776926]\n",
      " [-0.47573593....6968828 ]\n",
      " [-0.055869  ]\n",
      " [-1.684367  ]\n",
      " [ 1.1544775 ]\n",
      " [-2.488423  ]\n",
      " [ 0.4969537 ]\n",
      " [ 1.8054023 ]\n",
      " [-1.041959  ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]])\n",
      "        inputs     = [needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58083a90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58082650>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]...0.6968828 ]\n",
      " [-0.055869  ]\n",
      " [-1.684367  ]\n",
      " [ 1.1544775 ]\n",
      " [-2.488423  ]\n",
      " [ 0.4969537 ]\n",
      " [ 1.8054023 ]\n",
      " [-1.041959  ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58083a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]\n",
      " [-0.7895299]])\n",
      "        bias_ih    = needle.Tensor([[-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]\n",
      " [-0.11520284]])\n",
      "        h          = needle.Tensor([[ 0.20648915]\n",
      " [ 0.73045623]\n",
      " [ 0.34197843]\n",
      " [ 0.22317761]\n",
      " [ 1.1291592 ]\n",
      " [-0.65776926]\n",
      " [-0.47573593]\n",
      " [ 0.6968828 ]\n",
      " [-0.055869  ]\n",
      " [-1.684367  ]\n",
      " [ 1.1544775 ]\n",
      " [-2.488423  ]\n",
      " [ 0.4969537 ]\n",
      " [ 1.8054023 ]\n",
      " [-1.041959  ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58083a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.7685168]])\n",
      "        self       = needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]...7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]]), needle.Tensor([[0.7685168]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580838d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]...7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]]), needle.Tensor([[0.7685168]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580838d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e887b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58083c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e8bc70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58083c50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]], device=cpu())\n",
      "        b          = NDArray([[0.7685168]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580838d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]], device=cpu())\n",
      "        b          = NDArray([[0.7685168]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]], device=cpu())\n",
      "other = NDArray([[0.7685168]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58080b30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58080df0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58083bb0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.7685168]], device=cuda())\n",
      "out        = NDArray([[4.4841551e-44]\n",
      " [0.0000000e+00]\n",
      " [4.4841551e-44]\n",
      " [0.0000000e+00]\n",
      " [7.6520504e-35]\n",
      " [0.0000000e+00]\n",
      " [1.0642...]\n",
      " [4.5823861e-41]\n",
      " [2.2560905e-43]\n",
      " [0.0000000e+00]\n",
      " [8.9608302e-35]\n",
      " [0.0000000e+00]\n",
      " [8.7370271e-35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]\n",
      " [0.7362121]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.73650444]\n",
      "  [-0.8977197 ]\n",
      "  [ 0.921901  ]\n",
      "  [-1.4237138 ]\n",
      "  [-0.6488349 ]\n",
      "  [-0.938424  ]\n",
      "  [-0.59...08 ]\n",
      "  [-0.17362688]\n",
      "  [-2.0941951 ]\n",
      "  [ 0.63685656]\n",
      "  [-0.03546188]\n",
      "  [ 0.12432864]\n",
      "  [ 0.3384291 ]\n",
      "  [ 0.7519424 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.57879376]\n",
      "  [ 1.7894843 ]\n",
      "  [ 0.10956614]\n",
      "  [-0.82337594]\n",
      "  [ 0.39646655]\n",
      "  [-0.6403089 ]\n",
      "  [ 0.78...85 ]\n",
      "  [ 1.1466236 ]\n",
      "  [ 0.50334096]\n",
      "  [-0.17573486]\n",
      "  [-0.00584446]\n",
      "  [-2.500862  ]\n",
      "  [ 1.1373756 ]\n",
      "  [-0.76147944]]])\n",
      "h0         = needle.Tensor([[[-0.57879376]\n",
      "  [ 1.7894843 ]\n",
      "  [ 0.10956614]\n",
      "  [-0.82337594]\n",
      "  [ 0.39646655]\n",
      "  [-0.6403089 ]\n",
      "  [ 0.78...85 ]\n",
      "  [ 1.1466236 ]\n",
      "  [ 0.50334096]\n",
      "  [-0.17573486]\n",
      "  [-0.00584446]\n",
      "  [-2.500862  ]\n",
      "  [ 1.1373756 ]\n",
      "  [-0.76147944]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56f2d7d0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....5 ]\n",
      "  [ 1.1466236 ]\n",
      "  [ 0.50334096]\n",
      "  [-0.17573486]\n",
      "  [-0.00584446]\n",
      "  [-2.500862  ]\n",
      "  [ 1.1373756 ]\n",
      "  [-0.76147944]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56f2d7d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.57879376]\n",
      "  [ 1.7894843 ]\n",
      "  [ 0.10956614]\n",
      "  [-0.82337594]\n",
      "  [ 0.39646655]\n",
      "  [-0.6403089 ]\n",
      "  [ 0.78...85 ]\n",
      "  [ 1.1466236 ]\n",
      "  [ 0.50334096]\n",
      "  [-0.17573486]\n",
      "  [-0.00584446]\n",
      "  [-2.500862  ]\n",
      "  [ 1.1373756 ]\n",
      "  [-0.76147944]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56f2d7d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4...461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0....5 ]\n",
      "  [ 1.1466236 ]\n",
      "  [ 0.50334096]\n",
      "  [-0.17573486]\n",
      "  [-0.00584446]\n",
      "  [-2.500862  ]\n",
      "  [ 1.1373756 ]\n",
      "  [-0.76147944]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56f2d710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4...461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]\n",
      "  [0.4117461]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.57879376]\n",
      " [ 1.7894843 ]\n",
      " [ 0.10956614]\n",
      " [-0.82337594]\n",
      " [ 0.39646655]\n",
      " [-0.6403089 ]\n",
      " [ 0.78062916]\n",
      " [ 1.4814785 ]\n",
      " [ 1.1466236 ]\n",
      " [ 0.50334096]\n",
      " [-0.17573486]\n",
      " [-0.00584446]\n",
      " [-2.500862  ]\n",
      " [ 1.1373756 ]\n",
      " [-0.76147944]])\n",
      "        h0         = (needle.Tensor([[-0.57879376]\n",
      " [ 1.7894843 ]\n",
      " [ 0.10956614]\n",
      " [-0.82337594]\n",
      " [ 0.39646655]\n",
      " [-0.6403089 ]\n",
      " [ 0.78062916....4814785 ]\n",
      " [ 1.1466236 ]\n",
      " [ 0.50334096]\n",
      " [-0.17573486]\n",
      " [-0.00584446]\n",
      " [-2.500862  ]\n",
      " [ 1.1373756 ]\n",
      " [-0.76147944]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]])\n",
      "        inputs     = [needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]...117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2dc50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56f2d710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]...1.4814785 ]\n",
      " [ 1.1466236 ]\n",
      " [ 0.50334096]\n",
      " [-0.17573486]\n",
      " [-0.00584446]\n",
      " [-2.500862  ]\n",
      " [ 1.1373756 ]\n",
      " [-0.76147944]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2dc50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]\n",
      " [0.88082373]])\n",
      "        bias_ih    = needle.Tensor([[0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]\n",
      " [0.8413302]])\n",
      "        h          = needle.Tensor([[-0.57879376]\n",
      " [ 1.7894843 ]\n",
      " [ 0.10956614]\n",
      " [-0.82337594]\n",
      " [ 0.39646655]\n",
      " [-0.6403089 ]\n",
      " [ 0.78062916]\n",
      " [ 1.4814785 ]\n",
      " [ 1.1466236 ]\n",
      " [ 0.50334096]\n",
      " [-0.17573486]\n",
      " [-0.00584446]\n",
      " [-2.500862  ]\n",
      " [ 1.1373756 ]\n",
      " [-0.76147944]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2dc50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.28399992]])\n",
      "        self       = needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]...117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]]), needle.Tensor([[0.28399992]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5733b910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]...117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]]), needle.Tensor([[0.28399992]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5733b910>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56484a70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5733a790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56487ef0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5733a790>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]], device=cpu())\n",
      "        b          = NDArray([[0.28399992]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5733b910>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]], device=cpu())\n",
      "        b          = NDArray([[0.28399992]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]], device=cpu())\n",
      "other = NDArray([[0.28399992]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56f2d770>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56f2fd30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5733adb0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.28399992]], device=cuda())\n",
      "out        = NDArray([[1.1941826e-34]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]\n",
      " [0.4117461]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.40218168]\n",
      "  [ 0.16761588]\n",
      "  [ 1.0042379 ]\n",
      "  [ 0.9487831 ]\n",
      "  [ 0.4395785 ]\n",
      "  [-0.6965187 ]\n",
      "  [-0.46...72 ]\n",
      "  [ 1.4392622 ]\n",
      "  [ 0.40637502]\n",
      "  [ 0.82106537]\n",
      "  [-0.9656121 ]\n",
      "  [-0.17459098]\n",
      "  [ 0.7331772 ]\n",
      "  [ 0.71473974]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-1.4067618 ]\n",
      "  [-0.9775395 ]\n",
      "  [-0.46827447]\n",
      "  [-0.15879032]\n",
      "  [-0.65082264]\n",
      "  [-0.49099648]\n",
      "  [ 0.31...365]\n",
      "  [ 1.6300967 ]\n",
      "  [ 0.4835447 ]\n",
      "  [ 0.8744795 ]\n",
      "  [-1.0299038 ]\n",
      "  [-0.06889666]\n",
      "  [ 0.7614741 ]\n",
      "  [-0.4759166 ]]])\n",
      "h0         = needle.Tensor([[[-1.4067618 ]\n",
      "  [-0.9775395 ]\n",
      "  [-0.46827447]\n",
      "  [-0.15879032]\n",
      "  [-0.65082264]\n",
      "  [-0.49099648]\n",
      "  [ 0.31...365]\n",
      "  [ 1.6300967 ]\n",
      "  [ 0.4835447 ]\n",
      "  [ 0.8744795 ]\n",
      "  [-1.0299038 ]\n",
      "  [-0.06889666]\n",
      "  [ 0.7614741 ]\n",
      "  [-0.4759166 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd580f2650>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[-1.4067618 ]\n",
      "  [-0.9775395 ]\n",
      "  [-0....65]\n",
      "  [ 1.6300967 ]\n",
      "  [ 0.4835447 ]\n",
      "  [ 0.8744795 ]\n",
      "  [-1.0299038 ]\n",
      "  [-0.06889666]\n",
      "  [ 0.7614741 ]\n",
      "  [-0.4759166 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd580f2650>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-1.4067618 ]\n",
      "  [-0.9775395 ]\n",
      "  [-0.46827447]\n",
      "  [-0.15879032]\n",
      "  [-0.65082264]\n",
      "  [-0.49099648]\n",
      "  [ 0.31...365]\n",
      "  [ 1.6300967 ]\n",
      "  [ 0.4835447 ]\n",
      "  [ 0.8744795 ]\n",
      "  [-1.0299038 ]\n",
      "  [-0.06889666]\n",
      "  [ 0.7614741 ]\n",
      "  [-0.4759166 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd580f2650>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]...0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772...65]\n",
      "  [ 1.6300967 ]\n",
      "  [ 0.4835447 ]\n",
      "  [ 0.8744795 ]\n",
      "  [-1.0299038 ]\n",
      "  [-0.06889666]\n",
      "  [ 0.7614741 ]\n",
      "  [-0.4759166 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580f0990>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]...0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]\n",
      "  [0.38402772]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-1.4067618 ]\n",
      " [-0.9775395 ]\n",
      " [-0.46827447]\n",
      " [-0.15879032]\n",
      " [-0.65082264]\n",
      " [-0.49099648]\n",
      " [ 0.31267115]\n",
      " [-0.4937729 ]\n",
      " [ 1.0212444 ]\n",
      " [-0.785699  ]\n",
      " [ 0.50945914]\n",
      " [-1.1218549 ]\n",
      " [ 0.07419463]\n",
      " [-0.6889115 ]\n",
      " [-0.61420673]])\n",
      "        h0         = (needle.Tensor([[-1.4067618 ]\n",
      " [-0.9775395 ]\n",
      " [-0.46827447]\n",
      " [-0.15879032]\n",
      " [-0.65082264]\n",
      " [-0.49099648]\n",
      " [ 0.31267115...0.25697365]\n",
      " [ 1.6300967 ]\n",
      " [ 0.4835447 ]\n",
      " [ 0.8744795 ]\n",
      " [-1.0299038 ]\n",
      " [-0.06889666]\n",
      " [ 0.7614741 ]\n",
      " [-0.4759166 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]])\n",
      "        inputs     = [needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1c10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580f0990>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.3...0.4937729 ]\n",
      " [ 1.0212444 ]\n",
      " [-0.785699  ]\n",
      " [ 0.50945914]\n",
      " [-1.1218549 ]\n",
      " [ 0.07419463]\n",
      " [-0.6889115 ]\n",
      " [-0.61420673]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1c10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]\n",
      " [0.28077936]])\n",
      "        bias_ih    = needle.Tensor([[-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]\n",
      " [-0.7371919]])\n",
      "        h          = needle.Tensor([[-1.4067618 ]\n",
      " [-0.9775395 ]\n",
      " [-0.46827447]\n",
      " [-0.15879032]\n",
      " [-0.65082264]\n",
      " [-0.49099648]\n",
      " [ 0.31267115]\n",
      " [-0.4937729 ]\n",
      " [ 1.0212444 ]\n",
      " [-0.785699  ]\n",
      " [ 0.50945914]\n",
      " [-1.1218549 ]\n",
      " [ 0.07419463]\n",
      " [-0.6889115 ]\n",
      " [-0.61420673]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.95446676]])\n",
      "        self       = needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.3...\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]]), needle.Tensor([[-0.95446676]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580f1b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.3...\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]]), needle.Tensor([[-0.95446676]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580f1b50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57acb1f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd580f3110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57acbcf0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd580f3110>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]], device=cpu())\n",
      "        b          = NDArray([[-0.95446676]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580f1b50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]], device=cpu())\n",
      "        b          = NDArray([[-0.95446676]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]], device=cpu())\n",
      "other = NDArray([[-0.95446676]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580f3270>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580f0030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580f17f0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.95446676]], device=cuda())\n",
      "out        = NDArray([[1.3377452e-33]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]\n",
      " [0.38402772]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.6546257 ]\n",
      "  [-0.2837119 ]\n",
      "  [-1.0157758 ]\n",
      "  [-1.558625  ]\n",
      "  [-0.17222346]\n",
      "  [ 1.278186  ]\n",
      "  [-2.14...24 ]\n",
      "  [ 1.9782244 ]\n",
      "  [-0.00421227]\n",
      "  [ 1.2621858 ]\n",
      "  [-0.26343155]\n",
      "  [ 1.571898  ]\n",
      "  [ 0.88116616]\n",
      "  [-1.0157233 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 0.8873421 ]\n",
      "  [-0.76047516]\n",
      "  [ 0.3506621 ]\n",
      "  [ 1.1881185 ]\n",
      "  [-1.0022851 ]\n",
      "  [-0.90799516]\n",
      "  [-1.95...81 ]\n",
      "  [ 0.5580481 ]\n",
      "  [-0.31562746]\n",
      "  [ 0.26863307]\n",
      "  [ 0.34731415]\n",
      "  [ 0.22251922]\n",
      "  [-0.5598705 ]\n",
      "  [ 1.0601238 ]]])\n",
      "h0         = needle.Tensor([[[ 0.8873421 ]\n",
      "  [-0.76047516]\n",
      "  [ 0.3506621 ]\n",
      "  [ 1.1881185 ]\n",
      "  [-1.0022851 ]\n",
      "  [-0.90799516]\n",
      "  [-1.95...81 ]\n",
      "  [ 0.5580481 ]\n",
      "  [-0.31562746]\n",
      "  [ 0.26863307]\n",
      "  [ 0.34731415]\n",
      "  [ 0.22251922]\n",
      "  [-0.5598705 ]\n",
      "  [ 1.0601238 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58093dd0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....1 ]\n",
      "  [ 0.5580481 ]\n",
      "  [-0.31562746]\n",
      "  [ 0.26863307]\n",
      "  [ 0.34731415]\n",
      "  [ 0.22251922]\n",
      "  [-0.5598705 ]\n",
      "  [ 1.0601238 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58093dd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.8873421 ]\n",
      "  [-0.76047516]\n",
      "  [ 0.3506621 ]\n",
      "  [ 1.1881185 ]\n",
      "  [-1.0022851 ]\n",
      "  [-0.90799516]\n",
      "  [-1.95...81 ]\n",
      "  [ 0.5580481 ]\n",
      "  [-0.31562746]\n",
      "  [ 0.26863307]\n",
      "  [ 0.34731415]\n",
      "  [ 0.22251922]\n",
      "  [-0.5598705 ]\n",
      "  [ 1.0601238 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58093dd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]...0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255...1 ]\n",
      "  [ 0.5580481 ]\n",
      "  [-0.31562746]\n",
      "  [ 0.26863307]\n",
      "  [ 0.34731415]\n",
      "  [ 0.22251922]\n",
      "  [-0.5598705 ]\n",
      "  [ 1.0601238 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580922d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]...0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]\n",
      "  [0.49799255]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.8873421 ]\n",
      " [-0.76047516]\n",
      " [ 0.3506621 ]\n",
      " [ 1.1881185 ]\n",
      " [-1.0022851 ]\n",
      " [-0.90799516]\n",
      " [-1.9539402 ]\n",
      " [ 1.2470886 ]\n",
      " [ 0.99303687]\n",
      " [ 0.56668055]\n",
      " [ 0.41608775]\n",
      " [ 0.31295285]\n",
      " [ 1.7346221 ]\n",
      " [ 1.3636057 ]\n",
      " [ 0.11780114]])\n",
      "        h0         = (needle.Tensor([[ 0.8873421 ]\n",
      " [-0.76047516]\n",
      " [ 0.3506621 ]\n",
      " [ 1.1881185 ]\n",
      " [-1.0022851 ]\n",
      " [-0.90799516]\n",
      " [-1.9539402 ...1.1341981 ]\n",
      " [ 0.5580481 ]\n",
      " [-0.31562746]\n",
      " [ 0.26863307]\n",
      " [ 0.34731415]\n",
      " [ 0.22251922]\n",
      " [-0.5598705 ]\n",
      " [ 1.0601238 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]])\n",
      "        inputs     = [needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.4... [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58093c90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580922d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.4...1.2470886 ]\n",
      " [ 0.99303687]\n",
      " [ 0.56668055]\n",
      " [ 0.41608775]\n",
      " [ 0.31295285]\n",
      " [ 1.7346221 ]\n",
      " [ 1.3636057 ]\n",
      " [ 0.11780114]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58093c90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]\n",
      " [-0.53632563]])\n",
      "        bias_ih    = needle.Tensor([[-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]\n",
      " [-0.48801845]])\n",
      "        h          = needle.Tensor([[ 0.8873421 ]\n",
      " [-0.76047516]\n",
      " [ 0.3506621 ]\n",
      " [ 1.1881185 ]\n",
      " [-1.0022851 ]\n",
      " [-0.90799516]\n",
      " [-1.9539402 ]\n",
      " [ 1.2470886 ]\n",
      " [ 0.99303687]\n",
      " [ 0.56668055]\n",
      " [ 0.41608775]\n",
      " [ 0.31295285]\n",
      " [ 1.7346221 ]\n",
      " [ 1.3636057 ]\n",
      " [ 0.11780114]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58093c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.5658451]])\n",
      "        self       = needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.4...]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]]), needle.Tensor([[-0.5658451]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5855ae50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.4...]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]]), needle.Tensor([[-0.5658451]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5855ae50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd584b00f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5855b010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd584b2930>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5855b010>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]], device=cpu())\n",
      "        b          = NDArray([[-0.5658451]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5855ae50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]], device=cpu())\n",
      "        b          = NDArray([[-0.5658451]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]], device=cpu())\n",
      "other = NDArray([[-0.5658451]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58091830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58093170>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58558ef0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.5658451]], device=cuda())\n",
      "out        = NDArray([[1.5745401e-34]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]\n",
      " [0.49799255]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[1.3998199]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[1.0016823]]])\n",
      "h0         = needle.Tensor([[[1.0016823]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd580f2050>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[1.0016823]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd580f2050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[1.0016823]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd580f2050>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "   0.13664354 0.14686193 0.5345408 ...   0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "   0.88903445 0.16504526 0.7558898  0.00268222]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "   0.13664354 0.14686193 0.5345408...3  0.18222246 0.2655107  0.36602557\n",
      "   0.88903445 0.16504526 0.7558898  0.00268222]]]), needle.Tensor([[[1.0016823]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580f3e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "   0.13664354 0.14686193 0.5345408 ...   0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "   0.88903445 0.16504526 0.7558898  0.00268222]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[1.0016823]])\n",
      "        h0         = (needle.Tensor([[1.0016823]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0...03\n",
      "  0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]])\n",
      "        inputs     = [needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  ...3\n",
      "  0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1610>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580f3e50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  ...97743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]]), needle.Tensor([[1.0016823]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0...03\n",
      "  0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.8768133]])\n",
      "        bias_ih    = needle.Tensor([[0.89701045]])\n",
      "        h          = needle.Tensor([[1.0016823]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580f1610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.5444815 ]\n",
      " [ 0.36692882]\n",
      " [-0.66496813]\n",
      " [-0.05269653]\n",
      " [ 0.08022678]\n",
      " [-0.5814962 ]\n",
      " [-0.74384266]... 0.25042117]\n",
      " [-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]])\n",
      "        self       = needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0...03\n",
      "  0.07204755 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  ...0.25042117]\n",
      " [-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563052d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  ...0.25042117]\n",
      " [-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563052d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e47b70>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd563053d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e45030>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd563053d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0.99192...5 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]], device=cpu())\n",
      "        b          = NDArray([[ 0.5444815 ]\n",
      " [ 0.36692882]\n",
      " [-0.66496813]\n",
      " [-0.05269653]\n",
      " [ 0.08022678]\n",
      " [-0.5814962 ]\n",
      " [-0.74384266]\n",
      " [ 0....-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563052d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0.99192...5 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]], device=cpu())\n",
      "        b          = NDArray([[ 0.5444815 ]\n",
      " [ 0.36692882]\n",
      " [-0.66496813]\n",
      " [-0.05269653]\n",
      " [ 0.08022678]\n",
      " [-0.5814962 ]\n",
      " [-0.74384266]\n",
      " [ 0....-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0.99192...5 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]], device=cpu())\n",
      "other = NDArray([[ 0.5444815 ]\n",
      " [ 0.36692882]\n",
      " [-0.66496813]\n",
      " [-0.05269653]\n",
      " [ 0.08022678]\n",
      " [-0.5814962 ]\n",
      " [-0.74384266]\n",
      " [ 0....-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56307130>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580f0ff0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56304070>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.5444815 ]\n",
      " [ 0.36692882]\n",
      " [-0.66496813]\n",
      " [-0.05269653]\n",
      " [ 0.08022678]\n",
      " [-0.5814962 ]\n",
      " [-0.74384266]\n",
      " [ 0....-0.10426342]\n",
      " [ 0.58641016]\n",
      " [-0.7122114 ]\n",
      " [-0.42324525]\n",
      " [ 0.43005455]\n",
      " [-0.30315894]\n",
      " [-0.05488449]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.58734775 0.6874519  0.12232213 0.41029683 0.98082083 0.24692674\n",
      "  0.13664354 0.14686193 0.5345408  0.99192...5 0.7091849  0.9997743  0.18222246 0.2655107  0.36602557\n",
      "  0.88903445 0.16504526 0.7558898  0.00268222]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.0040753]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.9755133]]])\n",
      "h0         = needle.Tensor([[[-0.9755133]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58771f90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-0.9755133]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58771f90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.9755133]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58771f90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "   0.23757139 0.31168053 0.5956819 ...   0.4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "   0.46452335 0.25549245 0.23367205 0.78480804]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "   0.23757139 0.31168053 0.5956819...6 0.06968955 0.28992552 0.17334431\n",
      "   0.46452335 0.25549245 0.23367205 0.78480804]]]), needle.Tensor([[[-0.9755133]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58770b50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "   0.23757139 0.31168053 0.5956819 ...   0.4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "   0.46452335 0.25549245 0.23367205 0.78480804]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.9755133]])\n",
      "        h0         = (needle.Tensor([[-0.9755133]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0...39\n",
      "  0.4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]])\n",
      "        inputs     = [needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  ....4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58771dd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58770b50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  ...79256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]]), needle.Tensor([[-0.9755133]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58771dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0...39\n",
      "  0.4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.35733408]])\n",
      "        bias_ih    = needle.Tensor([[0.98782706]])\n",
      "        h          = needle.Tensor([[-0.9755133]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58771dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.4397775 ]\n",
      " [-0.667007  ]\n",
      " [-0.85368544]\n",
      " [ 0.96644306]\n",
      " [-0.4400953 ]\n",
      " [-0.28791022]\n",
      " [-0.84006083]...-0.93861824]\n",
      " [-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]])\n",
      "        self       = needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0...39\n",
      "  0.4251436  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  ...0.93861824]\n",
      " [-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6e590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  ...0.93861824]\n",
      " [-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6e590>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bcc2b0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56d6d3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bceff0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56d6d3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0.69015...  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]], device=cpu())\n",
      "        b          = NDArray([[-0.4397775 ]\n",
      " [-0.667007  ]\n",
      " [-0.85368544]\n",
      " [ 0.96644306]\n",
      " [-0.4400953 ]\n",
      " [-0.28791022]\n",
      " [-0.84006083]\n",
      " [ 0....-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6e590>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0.69015...  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]], device=cpu())\n",
      "        b          = NDArray([[-0.4397775 ]\n",
      " [-0.667007  ]\n",
      " [-0.85368544]\n",
      " [ 0.96644306]\n",
      " [-0.4400953 ]\n",
      " [-0.28791022]\n",
      " [-0.84006083]\n",
      " [ 0....-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0.69015...  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]], device=cpu())\n",
      "other = NDArray([[-0.4397775 ]\n",
      " [-0.667007  ]\n",
      " [-0.85368544]\n",
      " [ 0.96644306]\n",
      " [-0.4400953 ]\n",
      " [-0.28791022]\n",
      " [-0.84006083]\n",
      " [ 0....-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58772bf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58772c70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56d6c470>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.4397775 ]\n",
      " [-0.667007  ]\n",
      " [-0.85368544]\n",
      " [ 0.96644306]\n",
      " [-0.4400953 ]\n",
      " [-0.28791022]\n",
      " [-0.84006083]\n",
      " [ 0....-0.29815555]\n",
      " [-0.91981107]\n",
      " [ 0.62828994]\n",
      " [ 0.9214088 ]\n",
      " [-0.49766648]\n",
      " [ 0.57336414]\n",
      " [ 0.91053164]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.6946921  0.3442474  0.8885596  0.6961646  0.07362133 0.25910118\n",
      "  0.23757139 0.31168053 0.5956819  0.69015...  0.49464262 0.03779256 0.06968955 0.28992552 0.17334431\n",
      "  0.46452335 0.25549245 0.23367205 0.78480804]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.1692309]]\n",
      "\n",
      " [[-1.7880409]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[0.19790626]]\n",
      "\n",
      " [[0.8837496 ]]])\n",
      "h0         = needle.Tensor([[[0.19790626]]\n",
      "\n",
      " [[0.8837496 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd585ff9d0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[0.19790626]]\n",
      "\n",
      " [[0.8837496 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585ff9d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[0.19790626]]\n",
      "\n",
      " [[0.8837496 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd585ff9d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "   0.4510549  0.14536208 0.6637942 ...   0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "   0.724404   0.28446028 0.7863351  0.10567546]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "   0.4510549  0.14536208 0.6637942...042084 0.25947413\n",
      "   0.724404   0.28446028 0.7863351  0.10567546]]]), needle.Tensor([[[0.19790626]]\n",
      "\n",
      " [[0.8837496 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fecd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "   0.4510549  0.14536208 0.6637942 ...   0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "   0.724404   0.28446028 0.7863351  0.10567546]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.19790626]])\n",
      "        h0         = (needle.Tensor([[0.19790626]]), needle.Tensor([[0.8837496]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0...83\n",
      "  0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]])\n",
      "        inputs     = [needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  ...3\n",
      "  0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585ffcd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fecd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  ...71776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]]), needle.Tensor([[0.19790626]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585ffcd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0...83\n",
      "  0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.82721007]])\n",
      "        bias_ih    = needle.Tensor([[0.42391908]])\n",
      "        h          = needle.Tensor([[0.19790626]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585ffcd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.7805215 ]\n",
      " [ 0.59023714]\n",
      " [-0.02223128]\n",
      " [-0.3958304 ]\n",
      " [ 0.2462759 ]\n",
      " [ 0.3686521 ]\n",
      " [ 0.2992885 ]...-0.63459754]\n",
      " [ 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]])\n",
      "        self       = needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0...83\n",
      "  0.71797854 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  ...0.63459754]\n",
      " [ 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585ff890>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  ...0.63459754]\n",
      " [ 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585ff890>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd58922130>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd585fc750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd58923df0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd585fc750>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0.81118...4 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]], device=cpu())\n",
      "        b          = NDArray([[ 0.7805215 ]\n",
      " [ 0.59023714]\n",
      " [-0.02223128]\n",
      " [-0.3958304 ]\n",
      " [ 0.2462759 ]\n",
      " [ 0.3686521 ]\n",
      " [ 0.2992885 ]\n",
      " [-0.... 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585ff890>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0.81118...4 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]], device=cpu())\n",
      "        b          = NDArray([[ 0.7805215 ]\n",
      " [ 0.59023714]\n",
      " [-0.02223128]\n",
      " [-0.3958304 ]\n",
      " [ 0.2462759 ]\n",
      " [ 0.3686521 ]\n",
      " [ 0.2992885 ]\n",
      " [-0.... 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0.81118...4 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]], device=cpu())\n",
      "other = NDArray([[ 0.7805215 ]\n",
      " [ 0.59023714]\n",
      " [-0.02223128]\n",
      " [-0.3958304 ]\n",
      " [ 0.2462759 ]\n",
      " [ 0.3686521 ]\n",
      " [ 0.2992885 ]\n",
      " [-0.... 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585fef70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585ff670>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585fcf70>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.7805215 ]\n",
      " [ 0.59023714]\n",
      " [-0.02223128]\n",
      " [-0.3958304 ]\n",
      " [ 0.2462759 ]\n",
      " [ 0.3686521 ]\n",
      " [ 0.2992885 ]\n",
      " [-0.... 0.22927618]\n",
      " [-0.6871743 ]\n",
      " [-0.32224154]\n",
      " [ 0.25990582]\n",
      " [ 0.23748934]\n",
      " [ 0.30338585]\n",
      " [-0.21648955]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.22748564 0.94888186 0.71749514 0.5484538  0.15997037 0.04962892\n",
      "  0.4510549  0.14536208 0.6637942  0.81118...4 0.34016016 0.77871776 0.56231534 0.06042084 0.25947413\n",
      "  0.724404   0.28446028 0.7863351  0.10567546]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.43156418]]\n",
      "\n",
      " [[-0.5520752 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.9592265]]\n",
      "\n",
      " [[ 1.6628146]]])\n",
      "h0         = needle.Tensor([[[-0.9592265]]\n",
      "\n",
      " [[ 1.6628146]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56abd510>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-0.9592265]]\n",
      "\n",
      " [[ 1.6628146]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56abd510>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.9592265]]\n",
      "\n",
      " [[ 1.6628146]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56abd510>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "   0.09201106 0.3576666  0.9175442  ...   0.1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "   0.0380483  0.18918912 0.37044397 0.41002837]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "   0.09201106 0.3576666  0.9175442 ...147796 0.23833181\n",
      "   0.0380483  0.18918912 0.37044397 0.41002837]]]), needle.Tensor([[[-0.9592265]]\n",
      "\n",
      " [[ 1.6628146]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56abc410>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "   0.09201106 0.3576666  0.9175442  ...   0.1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "   0.0380483  0.18918912 0.37044397 0.41002837]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.9592265]])\n",
      "        h0         = (needle.Tensor([[-0.9592265]]), needle.Tensor([[1.6628146]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0....87\n",
      "  0.1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]])\n",
      "        inputs     = [needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0....1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56abdd10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56abc410>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0...68537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]]), needle.Tensor([[-0.9592265]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56abdd10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0....87\n",
      "  0.1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.65926075]])\n",
      "        bias_ih    = needle.Tensor([[-0.7217338]])\n",
      "        h          = needle.Tensor([[-0.9592265]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56abdd10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.8454052 ]\n",
      " [ 0.34938467]\n",
      " [-0.532816  ]\n",
      " [ 0.37389505]\n",
      " [ 0.94444096]\n",
      " [-0.5724901 ]\n",
      " [-0.19164878]... 0.6327151 ]\n",
      " [-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]])\n",
      "        self       = needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0....87\n",
      "  0.1554271  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0...0.6327151 ]\n",
      " [-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0...0.6327151 ]\n",
      " [-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090310>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd58013a30>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58090090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd580113b0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58090090>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0.201476...  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]], device=cpu())\n",
      "        b          = NDArray([[ 0.8454052 ]\n",
      " [ 0.34938467]\n",
      " [-0.532816  ]\n",
      " [ 0.37389505]\n",
      " [ 0.94444096]\n",
      " [-0.5724901 ]\n",
      " [-0.19164878]\n",
      " [-0....-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090310>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0.201476...  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]], device=cpu())\n",
      "        b          = NDArray([[ 0.8454052 ]\n",
      " [ 0.34938467]\n",
      " [-0.532816  ]\n",
      " [ 0.37389505]\n",
      " [ 0.94444096]\n",
      " [-0.5724901 ]\n",
      " [-0.19164878]\n",
      " [-0....-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0.201476...  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]], device=cpu())\n",
      "other = NDArray([[ 0.8454052 ]\n",
      " [ 0.34938467]\n",
      " [-0.532816  ]\n",
      " [ 0.37389505]\n",
      " [ 0.94444096]\n",
      " [-0.5724901 ]\n",
      " [-0.19164878]\n",
      " [-0....-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56abddb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56abc3b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58090a70>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.8454052 ]\n",
      " [ 0.34938467]\n",
      " [-0.532816  ]\n",
      " [ 0.37389505]\n",
      " [ 0.94444096]\n",
      " [-0.5724901 ]\n",
      " [-0.19164878]\n",
      " [-0....-0.00414652]\n",
      " [-0.8114557 ]\n",
      " [ 0.11097276]\n",
      " [-0.59902334]\n",
      " [-0.2388879 ]\n",
      " [ 0.7910764 ]\n",
      " [ 0.35563183]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.19475874 0.8828877  0.8333857  0.00310978 0.9748757  0.7853289\n",
      "  0.09201106 0.3576666  0.9175442  0.201476...  0.5395393  0.57668537 0.3949396  0.62147796 0.23833181\n",
      "  0.0380483  0.18918912 0.37044397 0.41002837]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.48199603]\n",
      "  [ 1.4778796 ]\n",
      "  [ 1.6038605 ]\n",
      "  [-1.3766159 ]\n",
      "  [ 0.7402674 ]\n",
      "  [-0.6253953 ]\n",
      "  [-1.43...386]\n",
      "  [ 1.5650531 ]\n",
      "  [ 1.2404282 ]\n",
      "  [-0.25557154]\n",
      "  [ 0.23184088]\n",
      "  [ 1.3332863 ]\n",
      "  [ 0.54525775]\n",
      "  [ 0.5895396 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.2200683 ]\n",
      "  [-0.17271781]\n",
      "  [-0.1629711 ]\n",
      "  [ 0.5758373 ]\n",
      "  [-0.12698984]\n",
      "  [-0.7688145 ]\n",
      "  [-0.64...61 ]\n",
      "  [-0.12116445]\n",
      "  [ 0.6436668 ]\n",
      "  [-0.3840611 ]\n",
      "  [-0.01358598]\n",
      "  [-0.2532902 ]\n",
      "  [-0.8187917 ]\n",
      "  [ 1.2365669 ]]])\n",
      "h0         = needle.Tensor([[[ 0.2200683 ]\n",
      "  [-0.17271781]\n",
      "  [-0.1629711 ]\n",
      "  [ 0.5758373 ]\n",
      "  [-0.12698984]\n",
      "  [-0.7688145 ]\n",
      "  [-0.64...61 ]\n",
      "  [-0.12116445]\n",
      "  [ 0.6436668 ]\n",
      "  [-0.3840611 ]\n",
      "  [-0.01358598]\n",
      "  [-0.2532902 ]\n",
      "  [-0.8187917 ]\n",
      "  [ 1.2365669 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd566c6d50>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 0.2200683 ]\n",
      "  [-0.17271781]\n",
      "  [-0....1 ]\n",
      "  [-0.12116445]\n",
      "  [ 0.6436668 ]\n",
      "  [-0.3840611 ]\n",
      "  [-0.01358598]\n",
      "  [-0.2532902 ]\n",
      "  [-0.8187917 ]\n",
      "  [ 1.2365669 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd566c6d50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.2200683 ]\n",
      "  [-0.17271781]\n",
      "  [-0.1629711 ]\n",
      "  [ 0.5758373 ]\n",
      "  [-0.12698984]\n",
      "  [-0.7688145 ]\n",
      "  [-0.64...61 ]\n",
      "  [-0.12116445]\n",
      "  [ 0.6436668 ]\n",
      "  [-0.3840611 ]\n",
      "  [-0.01358598]\n",
      "  [-0.2532902 ]\n",
      "  [-0.8187917 ]\n",
      "  [ 1.2365669 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd566c6d50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "   0.8575634  0.5962979  0.8909687 ...   0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "   0.6241365  0.8722237  0.98830324 0.04524771]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "   0.8575634  0.5962979  0.8909687...1 ]\n",
      "  [-0.12116445]\n",
      "  [ 0.6436668 ]\n",
      "  [-0.3840611 ]\n",
      "  [-0.01358598]\n",
      "  [-0.2532902 ]\n",
      "  [-0.8187917 ]\n",
      "  [ 1.2365669 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd566c53d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "   0.8575634  0.5962979  0.8909687 ...   0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "   0.6241365  0.8722237  0.98830324 0.04524771]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.2200683 ]\n",
      " [-0.17271781]\n",
      " [-0.1629711 ]\n",
      " [ 0.5758373 ]\n",
      " [-0.12698984]\n",
      " [-0.7688145 ]\n",
      " [-0.6462428 ]\n",
      " [-0.5423961 ]\n",
      " [-0.12116445]\n",
      " [ 0.6436668 ]\n",
      " [-0.3840611 ]\n",
      " [-0.01358598]\n",
      " [-0.2532902 ]\n",
      " [-0.8187917 ]\n",
      " [ 1.2365669 ]])\n",
      "        h0         = (needle.Tensor([[ 0.2200683 ]\n",
      " [-0.17271781]\n",
      " [-0.1629711 ]\n",
      " [ 0.5758373 ]\n",
      " [-0.12698984]\n",
      " [-0.7688145 ]\n",
      " [-0.6462428 ....5423961 ]\n",
      " [-0.12116445]\n",
      " [ 0.6436668 ]\n",
      " [-0.3840611 ]\n",
      " [-0.01358598]\n",
      " [-0.2532902 ]\n",
      " [-0.8187917 ]\n",
      " [ 1.2365669 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0...03\n",
      "  0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]])\n",
      "        inputs     = [needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  ...3\n",
      "  0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566c62d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd566c53d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  ...0.5423961 ]\n",
      " [-0.12116445]\n",
      " [ 0.6436668 ]\n",
      " [-0.3840611 ]\n",
      " [-0.01358598]\n",
      " [-0.2532902 ]\n",
      " [-0.8187917 ]\n",
      " [ 1.2365669 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566c62d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0...03\n",
      "  0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]\n",
      " [0.3508718]])\n",
      "        bias_ih    = needle.Tensor([[-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]\n",
      " [-0.09607565]])\n",
      "        h          = needle.Tensor([[ 0.2200683 ]\n",
      " [-0.17271781]\n",
      " [-0.1629711 ]\n",
      " [ 0.5758373 ]\n",
      " [-0.12698984]\n",
      " [-0.7688145 ]\n",
      " [-0.6462428 ]\n",
      " [-0.5423961 ]\n",
      " [-0.12116445]\n",
      " [ 0.6436668 ]\n",
      " [-0.3840611 ]\n",
      " [-0.01358598]\n",
      " [-0.2532902 ]\n",
      " [-0.8187917 ]\n",
      " [ 1.2365669 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566c62d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.6162292 ]\n",
      " [-0.6246674 ]\n",
      " [-0.83470845]\n",
      " [ 0.79476774]\n",
      " [-0.84491473]\n",
      " [ 0.42064202]\n",
      " [-0.4006663 ]...-0.12794185]\n",
      " [-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]])\n",
      "        self       = needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0...03\n",
      "  0.7614769  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  ...0.12794185]\n",
      " [-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566c46d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  ...0.12794185]\n",
      " [-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566c46d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56b4bab0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd566c6290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56b48830>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd566c6290>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0.66490...  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]], device=cpu())\n",
      "        b          = NDArray([[ 0.6162292 ]\n",
      " [-0.6246674 ]\n",
      " [-0.83470845]\n",
      " [ 0.79476774]\n",
      " [-0.84491473]\n",
      " [ 0.42064202]\n",
      " [-0.4006663 ]\n",
      " [-0....-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566c46d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0.66490...  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]], device=cpu())\n",
      "        b          = NDArray([[ 0.6162292 ]\n",
      " [-0.6246674 ]\n",
      " [-0.83470845]\n",
      " [ 0.79476774]\n",
      " [-0.84491473]\n",
      " [ 0.42064202]\n",
      " [-0.4006663 ]\n",
      " [-0....-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0.66490...  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]], device=cpu())\n",
      "other = NDArray([[ 0.6162292 ]\n",
      " [-0.6246674 ]\n",
      " [-0.83470845]\n",
      " [ 0.79476774]\n",
      " [-0.84491473]\n",
      " [ 0.42064202]\n",
      " [-0.4006663 ]\n",
      " [-0....-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd566c4630>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd566c72b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd566c4f30>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.6162292 ]\n",
      " [-0.6246674 ]\n",
      " [-0.83470845]\n",
      " [ 0.79476774]\n",
      " [-0.84491473]\n",
      " [ 0.42064202]\n",
      " [-0.4006663 ]\n",
      " [-0....-0.19174337]\n",
      " [ 0.49627888]\n",
      " [-0.81236786]\n",
      " [-0.21948367]\n",
      " [-0.60656357]\n",
      " [-0.89217556]\n",
      " [-0.93824196]], device=cuda())\n",
      "out        = NDArray([[3.105123e-34]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00...00e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.34640646 0.4874857  0.04513441 0.6612325  0.45664775 0.41244608\n",
      "  0.8575634  0.5962979  0.8909687  0.66490...  0.34983802 0.10656265 0.24233273 0.36021098 0.89710045\n",
      "  0.6241365  0.8722237  0.98830324 0.04524771]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.0912275 ]\n",
      "  [ 0.05444085]\n",
      "  [ 1.564883  ]\n",
      "  [ 0.7134532 ]\n",
      "  [-0.12313681]\n",
      "  [ 0.93767595]\n",
      "  [ 0.81...958]\n",
      "  [ 1.4053437 ]\n",
      "  [-1.5316435 ]\n",
      "  [ 2.0887873 ]\n",
      "  [-0.16445717]\n",
      "  [-1.5734272 ]\n",
      "  [ 0.4262983 ]\n",
      "  [-1.6421407 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.7495341 ]\n",
      "  [-0.54208475]\n",
      "  [-0.34864154]\n",
      "  [ 1.2472072 ]\n",
      "  [ 0.5576382 ]\n",
      "  [-0.5678704 ]\n",
      "  [ 0.64...42 ]\n",
      "  [-0.67750746]\n",
      "  [-0.00582893]\n",
      "  [ 1.0894984 ]\n",
      "  [-1.1941113 ]\n",
      "  [ 2.930395  ]\n",
      "  [ 1.6130655 ]\n",
      "  [-0.4464845 ]]])\n",
      "h0         = needle.Tensor([[[-0.7495341 ]\n",
      "  [-0.54208475]\n",
      "  [-0.34864154]\n",
      "  [ 1.2472072 ]\n",
      "  [ 0.5576382 ]\n",
      "  [-0.5678704 ]\n",
      "  [ 0.64...42 ]\n",
      "  [-0.67750746]\n",
      "  [-0.00582893]\n",
      "  [ 1.0894984 ]\n",
      "  [-1.1941113 ]\n",
      "  [ 2.930395  ]\n",
      "  [ 1.6130655 ]\n",
      "  [-0.4464845 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd564130d0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....2 ]\n",
      "  [-0.67750746]\n",
      "  [-0.00582893]\n",
      "  [ 1.0894984 ]\n",
      "  [-1.1941113 ]\n",
      "  [ 2.930395  ]\n",
      "  [ 1.6130655 ]\n",
      "  [-0.4464845 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd564130d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.7495341 ]\n",
      "  [-0.54208475]\n",
      "  [-0.34864154]\n",
      "  [ 1.2472072 ]\n",
      "  [ 0.5576382 ]\n",
      "  [-0.5678704 ]\n",
      "  [ 0.64...42 ]\n",
      "  [-0.67750746]\n",
      "  [-0.00582893]\n",
      "  [ 1.0894984 ]\n",
      "  [-1.1941113 ]\n",
      "  [ 2.930395  ]\n",
      "  [ 1.6130655 ]\n",
      "  [-0.4464845 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd564130d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.83192635 0.35676605 0.7843149  ... 0.8488065  0.14158577 0.83602446]\n",
      "  [0.83192635 0.35676605 0.784...149  ... 0.8488065  0.14158577 0.83602446]\n",
      "  [0.83192635 0.35676605 0.7843149  ... 0.8488065  0.14158577 0.83602446]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.83192635 0.35676605 0.7843149  ... 0.8488065  0.14158577 0.83602446]\n",
      "  [0.83192635 0.35676605 0.78...2 ]\n",
      "  [-0.67750746]\n",
      "  [-0.00582893]\n",
      "  [ 1.0894984 ]\n",
      "  [-1.1941113 ]\n",
      "  [ 2.930395  ]\n",
      "  [ 1.6130655 ]\n",
      "  [-0.4464845 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56413590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.83192635 0.35676605 0.7843149  ... 0.8488065  0.14158577 0.83602446]\n",
      "  [0.83192635 0.35676605 0.784...149  ... 0.8488065  0.14158577 0.83602446]\n",
      "  [0.83192635 0.35676605 0.7843149  ... 0.8488065  0.14158577 0.83602446]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.7495341 ]\n",
      " [-0.54208475]\n",
      " [-0.34864154]\n",
      " [ 1.2472072 ]\n",
      " [ 0.5576382 ]\n",
      " [-0.5678704 ]\n",
      " [ 0.6494658 ]\n",
      " [-1.4375442 ]\n",
      " [-0.67750746]\n",
      " [-0.00582893]\n",
      " [ 1.0894984 ]\n",
      " [-1.1941113 ]\n",
      " [ 2.930395  ]\n",
      " [ 1.6130655 ]\n",
      " [-0.4464845 ]])\n",
      "        h0         = (needle.Tensor([[-0.7495341 ]\n",
      " [-0.54208475]\n",
      " [-0.34864154]\n",
      " [ 1.2472072 ]\n",
      " [ 0.5576382 ]\n",
      " [-0.5678704 ]\n",
      " [ 0.6494658 ....4375442 ]\n",
      " [-0.67750746]\n",
      " [-0.00582893]\n",
      " [ 1.0894984 ]\n",
      " [-1.1941113 ]\n",
      " [ 2.930395  ]\n",
      " [ 1.6130655 ]\n",
      " [-0.4464845 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0...779\n",
      "  0.9564049  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]])\n",
      "        inputs     = [needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 ...0.9564049  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412650>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56413590>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 ...1.4375442 ]\n",
      " [-0.67750746]\n",
      " [-0.00582893]\n",
      " [ 1.0894984 ]\n",
      " [-1.1941113 ]\n",
      " [ 2.930395  ]\n",
      " [ 1.6130655 ]\n",
      " [-0.4464845 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412650>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0...779\n",
      "  0.9564049  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]\n",
      " [-0.9970795]])\n",
      "        bias_ih    = needle.Tensor([[0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]\n",
      " [0.19123161]])\n",
      "        h          = needle.Tensor([[-0.7495341 ]\n",
      " [-0.54208475]\n",
      " [-0.34864154]\n",
      " [ 1.2472072 ]\n",
      " [ 0.5576382 ]\n",
      " [-0.5678704 ]\n",
      " [ 0.6494658 ]\n",
      " [-1.4375442 ]\n",
      " [-0.67750746]\n",
      " [-0.00582893]\n",
      " [ 1.0894984 ]\n",
      " [-1.1941113 ]\n",
      " [ 2.930395  ]\n",
      " [ 1.6130655 ]\n",
      " [-0.4464845 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.9138844 ]\n",
      " [ 0.69394803]\n",
      " [ 0.27159822]\n",
      " [ 0.07652831]\n",
      " [ 0.5917301 ]\n",
      " [-0.19863546]\n",
      " [ 0.5237256 ]...-0.02832788]\n",
      " [-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]])\n",
      "        self       = needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0...779\n",
      "  0.9564049  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 ...0.02832788]\n",
      " [-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58700a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 ...0.02832788]\n",
      " [-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58700a50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58800e70>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58701e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd588036b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58701e50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0.58829...9  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]], device=cpu())\n",
      "        b          = NDArray([[ 0.9138844 ]\n",
      " [ 0.69394803]\n",
      " [ 0.27159822]\n",
      " [ 0.07652831]\n",
      " [ 0.5917301 ]\n",
      " [-0.19863546]\n",
      " [ 0.5237256 ]\n",
      " [-0....-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58700a50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0.58829...9  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]], device=cpu())\n",
      "        b          = NDArray([[ 0.9138844 ]\n",
      " [ 0.69394803]\n",
      " [ 0.27159822]\n",
      " [ 0.07652831]\n",
      " [ 0.5917301 ]\n",
      " [-0.19863546]\n",
      " [ 0.5237256 ]\n",
      " [-0....-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0.58829...9  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]], device=cpu())\n",
      "other = NDArray([[ 0.9138844 ]\n",
      " [ 0.69394803]\n",
      " [ 0.27159822]\n",
      " [ 0.07652831]\n",
      " [ 0.5917301 ]\n",
      " [-0.19863546]\n",
      " [ 0.5237256 ]\n",
      " [-0....-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56410eb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd564111f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587000b0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.9138844 ]\n",
      " [ 0.69394803]\n",
      " [ 0.27159822]\n",
      " [ 0.07652831]\n",
      " [ 0.5917301 ]\n",
      " [-0.19863546]\n",
      " [ 0.5237256 ]\n",
      " [-0....-0.5254943 ]\n",
      " [ 0.60780084]\n",
      " [ 0.1084429 ]\n",
      " [ 0.39806497]\n",
      " [-0.97066325]\n",
      " [-0.24273288]\n",
      " [ 0.7051102 ]], device=cuda())\n",
      "out        = NDArray([[7.961647e-34]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00...00e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]\n",
      " [0.000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.83192635 0.35676605 0.7843149  0.6506798  0.9282743  0.38921213\n",
      "  0.7822052  0.99370533 0.02530116 0.58829...9  0.75803876 0.80389607 0.5464417  0.19898826 0.5708613\n",
      "  0.21306174 0.8488065  0.14158577 0.83602446]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.9450461 ]\n",
      "  [-1.7307779 ]\n",
      "  [ 0.30158305]\n",
      "  [ 0.85685384]\n",
      "  [ 1.2491144 ]\n",
      "  [-0.18212801]\n",
      "  [-0.18...49 ]\n",
      "  [ 1.4821141 ]\n",
      "  [ 0.04166644]\n",
      "  [-0.3048185 ]\n",
      "  [ 1.9308428 ]\n",
      "  [-0.88195866]\n",
      "  [-0.34915113]\n",
      "  [ 1.824623  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.9519999 ]\n",
      "  [ 1.9703281 ]\n",
      "  [-0.65850794]\n",
      "  [-0.820611  ]\n",
      "  [-0.17847742]\n",
      "  [-1.4359571 ]\n",
      "  [ 0.06...037]\n",
      "  [ 2.253457  ]\n",
      "  [ 1.3946226 ]\n",
      "  [ 1.1344994 ]\n",
      "  [ 1.484413  ]\n",
      "  [-1.0734617 ]\n",
      "  [ 0.95597667]\n",
      "  [ 0.635453  ]]])\n",
      "h0         = needle.Tensor([[[ 0.9519999 ]\n",
      "  [ 1.9703281 ]\n",
      "  [-0.65850794]\n",
      "  [-0.820611  ]\n",
      "  [-0.17847742]\n",
      "  [-1.4359571 ]\n",
      "  [ 0.06...037]\n",
      "  [ 2.253457  ]\n",
      "  [ 1.3946226 ]\n",
      "  [ 1.1344994 ]\n",
      "  [ 1.484413  ]\n",
      "  [-1.0734617 ]\n",
      "  [ 0.95597667]\n",
      "  [ 0.635453  ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5716e110>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 0.9519999 ]\n",
      "  [ 1.9703281 ]\n",
      "  [-0....37]\n",
      "  [ 2.253457  ]\n",
      "  [ 1.3946226 ]\n",
      "  [ 1.1344994 ]\n",
      "  [ 1.484413  ]\n",
      "  [-1.0734617 ]\n",
      "  [ 0.95597667]\n",
      "  [ 0.635453  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5716e110>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.9519999 ]\n",
      "  [ 1.9703281 ]\n",
      "  [-0.65850794]\n",
      "  [-0.820611  ]\n",
      "  [-0.17847742]\n",
      "  [-1.4359571 ]\n",
      "  [ 0.06...037]\n",
      "  [ 2.253457  ]\n",
      "  [ 1.3946226 ]\n",
      "  [ 1.1344994 ]\n",
      "  [ 1.484413  ]\n",
      "  [-1.0734617 ]\n",
      "  [ 0.95597667]\n",
      "  [ 0.635453  ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd5716e110>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "   0.023279   0.65811193 0.93097585 ...   0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "   0.9484248  0.79297316 0.0497785  0.74165225]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "   0.023279   0.65811193 0.93097585...37]\n",
      "  [ 2.253457  ]\n",
      "  [ 1.3946226 ]\n",
      "  [ 1.1344994 ]\n",
      "  [ 1.484413  ]\n",
      "  [-1.0734617 ]\n",
      "  [ 0.95597667]\n",
      "  [ 0.635453  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5716c110>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "   0.023279   0.65811193 0.93097585 ...   0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "   0.9484248  0.79297316 0.0497785  0.74165225]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.9519999 ]\n",
      " [ 1.9703281 ]\n",
      " [-0.65850794]\n",
      " [-0.820611  ]\n",
      " [-0.17847742]\n",
      " [-1.4359571 ]\n",
      " [ 0.06135043]\n",
      " [ 0.07284785]\n",
      " [ 0.92254806]\n",
      " [ 0.72394794]\n",
      " [-0.8277396 ]\n",
      " [-0.0211385 ]\n",
      " [-1.2737079 ]\n",
      " [-0.39166498]\n",
      " [ 0.17624222]])\n",
      "        h0         = (needle.Tensor([[ 0.9519999 ]\n",
      " [ 1.9703281 ]\n",
      " [-0.65850794]\n",
      " [-0.820611  ]\n",
      " [-0.17847742]\n",
      " [-1.4359571 ]\n",
      " [ 0.06135043...0.25642037]\n",
      " [ 2.253457  ]\n",
      " [ 1.3946226 ]\n",
      " [ 1.1344994 ]\n",
      " [ 1.484413  ]\n",
      " [-1.0734617 ]\n",
      " [ 0.95597667]\n",
      " [ 0.635453  ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0....25\n",
      "  0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]])\n",
      "        inputs     = [needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0...5\n",
      "  0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5716ccd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5716c110>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0...0.07284785]\n",
      " [ 0.92254806]\n",
      " [ 0.72394794]\n",
      " [-0.8277396 ]\n",
      " [-0.0211385 ]\n",
      " [-1.2737079 ]\n",
      " [-0.39166498]\n",
      " [ 0.17624222]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5716ccd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0....25\n",
      "  0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]\n",
      " [0.7993417]])\n",
      "        bias_ih    = needle.Tensor([[-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]\n",
      " [-0.382977]])\n",
      "        h          = needle.Tensor([[ 0.9519999 ]\n",
      " [ 1.9703281 ]\n",
      " [-0.65850794]\n",
      " [-0.820611  ]\n",
      " [-0.17847742]\n",
      " [-1.4359571 ]\n",
      " [ 0.06135043]\n",
      " [ 0.07284785]\n",
      " [ 0.92254806]\n",
      " [ 0.72394794]\n",
      " [-0.8277396 ]\n",
      " [-0.0211385 ]\n",
      " [-1.2737079 ]\n",
      " [-0.39166498]\n",
      " [ 0.17624222]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5716ccd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.40033305]\n",
      " [-0.23328358]\n",
      " [ 0.8217038 ]\n",
      " [-0.6321084 ]\n",
      " [-0.31275308]\n",
      " [ 0.06649244]\n",
      " [-0.70368993]... 0.55005455]\n",
      " [ 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]])\n",
      "        self       = needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0....25\n",
      "  0.59230596 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0...0.55005455]\n",
      " [ 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5716fa10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0...0.55005455]\n",
      " [ 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5716fa10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58696470>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd5716c8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd586943f0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd5716c8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0.085648...6 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]], device=cpu())\n",
      "        b          = NDArray([[ 0.40033305]\n",
      " [-0.23328358]\n",
      " [ 0.8217038 ]\n",
      " [-0.6321084 ]\n",
      " [-0.31275308]\n",
      " [ 0.06649244]\n",
      " [-0.70368993]\n",
      " [ 0.... 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5716fa10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0.085648...6 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]], device=cpu())\n",
      "        b          = NDArray([[ 0.40033305]\n",
      " [-0.23328358]\n",
      " [ 0.8217038 ]\n",
      " [-0.6321084 ]\n",
      " [-0.31275308]\n",
      " [ 0.06649244]\n",
      " [-0.70368993]\n",
      " [ 0.... 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0.085648...6 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]], device=cpu())\n",
      "other = NDArray([[ 0.40033305]\n",
      " [-0.23328358]\n",
      " [ 0.8217038 ]\n",
      " [-0.6321084 ]\n",
      " [-0.31275308]\n",
      " [ 0.06649244]\n",
      " [-0.70368993]\n",
      " [ 0.... 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5716e430>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5716ffb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5716cdf0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.40033305]\n",
      " [-0.23328358]\n",
      " [ 0.8217038 ]\n",
      " [-0.6321084 ]\n",
      " [-0.31275308]\n",
      " [ 0.06649244]\n",
      " [-0.70368993]\n",
      " [ 0.... 0.1469605 ]\n",
      " [ 0.28424156]\n",
      " [ 0.65413094]\n",
      " [-0.9089861 ]\n",
      " [ 0.11739075]\n",
      " [ 0.3205725 ]\n",
      " [-0.18584847]], device=cuda())\n",
      "out        = NDArray([[2.8037267e-34]\n",
      " [0.0000000e+00]\n",
      " [1.4187023e-34]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8254727  0.3911465  0.4696176  0.20605056 0.00268574 0.9372591\n",
      "  0.023279   0.65811193 0.93097585 0.085648...6 0.6232357  0.0157905  0.9852543  0.95023084 0.46079102\n",
      "  0.9484248  0.79297316 0.0497785  0.74165225]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.0574354 ]\n",
      "  [ 1.6596229 ]\n",
      "  [ 2.2407155 ]\n",
      "  [-0.15695502]\n",
      "  [-0.32856452]\n",
      "  [ 0.30959618]\n",
      "  [ 1.33...644]\n",
      "  [-0.1607935 ]\n",
      "  [-0.80683833]\n",
      "  [ 1.2666291 ]\n",
      "  [-0.3679231 ]\n",
      "  [-0.6442939 ]\n",
      "  [ 1.2972629 ]\n",
      "  [ 0.21548785]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 1.6623237 ]\n",
      "  [-0.9392097 ]\n",
      "  [-0.9646754 ]\n",
      "  [ 0.537005  ]\n",
      "  [-0.6815862 ]\n",
      "  [ 0.6374627 ]\n",
      "  [ 0.09...869]\n",
      "  [-1.5547516 ]\n",
      "  [-0.9221835 ]\n",
      "  [ 0.35694882]\n",
      "  [-0.22721991]\n",
      "  [ 0.9462005 ]\n",
      "  [ 0.7992209 ]\n",
      "  [-0.6331115 ]]])\n",
      "h0         = needle.Tensor([[[ 1.6623237 ]\n",
      "  [-0.9392097 ]\n",
      "  [-0.9646754 ]\n",
      "  [ 0.537005  ]\n",
      "  [-0.6815862 ]\n",
      "  [ 0.6374627 ]\n",
      "  [ 0.09...869]\n",
      "  [-1.5547516 ]\n",
      "  [-0.9221835 ]\n",
      "  [ 0.35694882]\n",
      "  [-0.22721991]\n",
      "  [ 0.9462005 ]\n",
      "  [ 0.7992209 ]\n",
      "  [-0.6331115 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584bb810>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....69]\n",
      "  [-1.5547516 ]\n",
      "  [-0.9221835 ]\n",
      "  [ 0.35694882]\n",
      "  [-0.22721991]\n",
      "  [ 0.9462005 ]\n",
      "  [ 0.7992209 ]\n",
      "  [-0.6331115 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584bb810>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 1.6623237 ]\n",
      "  [-0.9392097 ]\n",
      "  [-0.9646754 ]\n",
      "  [ 0.537005  ]\n",
      "  [-0.6815862 ]\n",
      "  [ 0.6374627 ]\n",
      "  [ 0.09...869]\n",
      "  [-1.5547516 ]\n",
      "  [-0.9221835 ]\n",
      "  [ 0.35694882]\n",
      "  [-0.22721991]\n",
      "  [ 0.9462005 ]\n",
      "  [ 0.7992209 ]\n",
      "  [-0.6331115 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584bb810>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.24639203 0.7241313  0.03982913 ... 0.49781874 0.8992742  0.5947346 ]\n",
      "  [0.24639203 0.7241313  0.039...2913 ... 0.49781874 0.8992742  0.5947346 ]\n",
      "  [0.24639203 0.7241313  0.03982913 ... 0.49781874 0.8992742  0.5947346 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.24639203 0.7241313  0.03982913 ... 0.49781874 0.8992742  0.5947346 ]\n",
      "  [0.24639203 0.7241313  0.03...69]\n",
      "  [-1.5547516 ]\n",
      "  [-0.9221835 ]\n",
      "  [ 0.35694882]\n",
      "  [-0.22721991]\n",
      "  [ 0.9462005 ]\n",
      "  [ 0.7992209 ]\n",
      "  [-0.6331115 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b9910>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.24639203 0.7241313  0.03982913 ... 0.49781874 0.8992742  0.5947346 ]\n",
      "  [0.24639203 0.7241313  0.039...2913 ... 0.49781874 0.8992742  0.5947346 ]\n",
      "  [0.24639203 0.7241313  0.03982913 ... 0.49781874 0.8992742  0.5947346 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 1.6623237 ]\n",
      " [-0.9392097 ]\n",
      " [-0.9646754 ]\n",
      " [ 0.537005  ]\n",
      " [-0.6815862 ]\n",
      " [ 0.6374627 ]\n",
      " [ 0.09048779]\n",
      " [-0.13634701]\n",
      " [ 1.7370296 ]\n",
      " [-1.1742797 ]\n",
      " [-0.6418118 ]\n",
      " [ 0.6755683 ]\n",
      " [ 1.0496712 ]\n",
      " [ 0.32559812]\n",
      " [ 1.3873272 ]])\n",
      "        h0         = (needle.Tensor([[ 1.6623237 ]\n",
      " [-0.9392097 ]\n",
      " [-0.9646754 ]\n",
      " [ 0.537005  ]\n",
      " [-0.6815862 ]\n",
      " [ 0.6374627 ]\n",
      " [ 0.09048779...0.13153869]\n",
      " [-1.5547516 ]\n",
      " [-0.9221835 ]\n",
      " [ 0.35694882]\n",
      " [-0.22721991]\n",
      " [ 0.9462005 ]\n",
      " [ 0.7992209 ]\n",
      " [-0.6331115 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0....016\n",
      "  0.17146124 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]])\n",
      "        inputs     = [needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0...0.17146124 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b8bd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b9910>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0...0.13634701]\n",
      " [ 1.7370296 ]\n",
      " [-1.1742797 ]\n",
      " [-0.6418118 ]\n",
      " [ 0.6755683 ]\n",
      " [ 1.0496712 ]\n",
      " [ 0.32559812]\n",
      " [ 1.3873272 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b8bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0....016\n",
      "  0.17146124 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]\n",
      " [-0.7880657]])\n",
      "        bias_ih    = needle.Tensor([[0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]\n",
      " [0.62405753]])\n",
      "        h          = needle.Tensor([[ 1.6623237 ]\n",
      " [-0.9392097 ]\n",
      " [-0.9646754 ]\n",
      " [ 0.537005  ]\n",
      " [-0.6815862 ]\n",
      " [ 0.6374627 ]\n",
      " [ 0.09048779]\n",
      " [-0.13634701]\n",
      " [ 1.7370296 ]\n",
      " [-1.1742797 ]\n",
      " [-0.6418118 ]\n",
      " [ 0.6755683 ]\n",
      " [ 1.0496712 ]\n",
      " [ 0.32559812]\n",
      " [ 1.3873272 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b8bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.6529188 ]\n",
      " [-0.36420006]\n",
      " [ 0.04599965]\n",
      " [ 0.0397501 ]\n",
      " [ 0.5505482 ]\n",
      " [-0.45973897]\n",
      " [ 0.22858584]... 0.8799528 ]\n",
      " [-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]])\n",
      "        self       = needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0....016\n",
      "  0.17146124 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0...0.8799528 ]\n",
      " [-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0...0.8799528 ]\n",
      " [-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9550>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58773df0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57ed9110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd587704f0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57ed9110>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0.566451...24 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.6529188 ]\n",
      " [-0.36420006]\n",
      " [ 0.04599965]\n",
      " [ 0.0397501 ]\n",
      " [ 0.5505482 ]\n",
      " [-0.45973897]\n",
      " [ 0.22858584]\n",
      " [-0....-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9550>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0.566451...24 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.6529188 ]\n",
      " [-0.36420006]\n",
      " [ 0.04599965]\n",
      " [ 0.0397501 ]\n",
      " [ 0.5505482 ]\n",
      " [-0.45973897]\n",
      " [ 0.22858584]\n",
      " [-0....-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0.566451...24 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]], device=cpu())\n",
      "other = NDArray([[ 0.6529188 ]\n",
      " [-0.36420006]\n",
      " [ 0.04599965]\n",
      " [ 0.0397501 ]\n",
      " [ 0.5505482 ]\n",
      " [-0.45973897]\n",
      " [ 0.22858584]\n",
      " [-0....-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b9d30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b82b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ed8670>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.6529188 ]\n",
      " [-0.36420006]\n",
      " [ 0.04599965]\n",
      " [ 0.0397501 ]\n",
      " [ 0.5505482 ]\n",
      " [-0.45973897]\n",
      " [ 0.22858584]\n",
      " [-0....-0.4999867 ]\n",
      " [-0.6543593 ]\n",
      " [ 0.05334973]\n",
      " [-0.5356244 ]\n",
      " [ 0.2786379 ]\n",
      " [-0.30979687]\n",
      " [-0.88023406]], device=cuda())\n",
      "out        = NDArray([[9.6139183e-35]\n",
      " [0.0000000e+00]\n",
      " [1.9848773e-34]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.24639203 0.7241313  0.03982913 0.00801681 0.11533827 0.1407276\n",
      "  0.56199646 0.69092965 0.49781656 0.566451...24 0.38124248 0.9402785  0.92176634 0.5878145  0.9674753\n",
      "  0.87788546 0.49781874 0.8992742  0.5947346 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.7392399   0.8493074  -1.0161203   0.6474175  -0.5825926\n",
      "    0.4275198   0.99732774 -1.1196202   0.04308277 -0.07428318\n",
      "    0.8385535   0.48953325]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048\n",
      "    0.8773783   1.2645419   0.41417518 -1.0997918   0.2008133\n",
      "   -0.2708222   2.4761906 ]]])\n",
      "h0         = needle.Tensor([[[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048\n",
      "    0.8773783   1.2645419   0.41417518 -1.0997918   0.2008133\n",
      "   -0.2708222   2.4761906 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5696a450>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048\n",
      "    0.8773783   1.2645419   0.41417518 -1.0997918   0.2008133\n",
      "   -0.2708222   2.4761906 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5696a450>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048\n",
      "    0.8773783   1.2645419   0.41417518 -1.0997918   0.2008133\n",
      "   -0.2708222   2.4761906 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd5696a450>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.57824326]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.57824326]]]), needle.Tensor([[[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048\n",
      "    0.8773783   1.2645419   0.41417518 -1.0997918   0.2008133\n",
      "   -0.2708222   2.4761906 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56968450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.57824326]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048  0.8773783\n",
      "   1.2645419   0.41417518 -1.0997918   0.2008133  -0.2708222   2.4761906 ]])\n",
      "        h0         = (needle.Tensor([[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048  0.8773783\n",
      "   1.2645419   0.41417518 -1.0997918   0.2008133  -0.2708222   2.4761906 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.57824326]])\n",
      "        inputs     = [needle.Tensor([[0.57824326]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5696a790>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56968450>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.57824326]]), needle.Tensor([[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048  0.8773783\n",
      "   1.2645419   0.41417518 -1.0997918   0.2008133  -0.2708222   2.4761906 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5696a790>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.57824326]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.06692812  0.17960638  0.24774265 -0.15128766  0.0039894   0.24117708\n",
      "  -0.11851303  0.08528453 -0.16051026  0.10866427  0.15347832  0.14596468]])\n",
      "        bias_ih    = needle.Tensor([[-0.26367122 -0.22644469  0.14164412  0.2554714   0.27559072 -0.01265982\n",
      "   0.06107715  0.26090622 -0.06114851 -0.13198625 -0.19920717 -0.28388163]])\n",
      "        h          = needle.Tensor([[ 2.2875888   1.1948873   0.36937842 -0.80151916 -0.29758048  0.8773783\n",
      "   1.2645419   0.41417518 -1.0997918   0.2008133  -0.2708222   2.4761906 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5696a790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]])\n",
      "        self       = needle.Tensor([[0.57824326]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.57824326]]), needle.Tensor([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56969a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.57824326]]), needle.Tensor([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56969a90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e7fdb0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56969ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd58921270>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56969ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.57824326]], device=cpu())\n",
      "        b          = NDArray([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56969a90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.57824326]], device=cpu())\n",
      "        b          = NDArray([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.57824326]], device=cpu())\n",
      "other = NDArray([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56968f30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5696a5b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5696adb0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.18155986  0.2863769   0.10732964 -0.03359526 -0.06507395 -0.0304108\n",
      "   0.04300794  0.28489077 -0.25282153  0.01103222  0.10361105  0.0670853 ]], device=cuda())\n",
      "out        = NDArray([[ 8.1775126e+35  4.5823861e-41  3.7788424e-35  0.0000000e+00\n",
      "  -2.5610232e-01 -1.6559549e-01  3.6013371e-43  0.0000000e+00\n",
      "   8.0091206e-35  0.0000000e+00  2.0963619e-34  0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.57824326]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.67891854  0.72455364 -0.8291505  -0.10255678 -1.0923628\n",
      "   -0.2033346  -0.7653846  -0.7625436   0.37690133  0.49421686\n",
      "   -0.37598434  0.64256525]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187\n",
      "   -0.9844096   1.5315286   0.9754208  -0.67095083 -0.4856012\n",
      "    0.02712796 -0.7918243 ]]])\n",
      "h0         = needle.Tensor([[[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187\n",
      "   -0.9844096   1.5315286   0.9754208  -0.67095083 -0.4856012\n",
      "    0.02712796 -0.7918243 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58700390>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-0.28...8  1.2224702  -0.51011187\n",
      "   -0.9844096   1.5315286   0.9754208  -0.67095083 -0.4856012\n",
      "    0.02712796 -0.7918243 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58700390>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187\n",
      "   -0.9844096   1.5315286   0.9754208  -0.67095083 -0.4856012\n",
      "    0.02712796 -0.7918243 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58700390>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.30...8  1.2224702  -0.51011187\n",
      "   -0.9844096   1.5315286   0.9754208  -0.67095083 -0.4856012\n",
      "    0.02712796 -0.7918243 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58702350>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]\n",
      "\n",
      " [[0.3076733]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187 -0.9844096\n",
      "   1.5315286   0.9754208  -0.67095083 -0.4856012   0.02712796 -0.7918243 ]])\n",
      "        h0         = (needle.Tensor([[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187 -0.9844096\n",
      "   1.5315286   0.9754208  -0.67095083 -0.4856012   0.02712796 -0.7918243 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.3076733]])\n",
      "        inputs     = [needle.Tensor([[0.3076733]]), needle.Tensor([[0.3076733]]), needle.Tensor([[0.3076733]]), needle.Tensor([[0.3076733]]), needle.Tensor([[0.3076733]]), needle.Tensor([[0.3076733]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58703290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58702350>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.3076733]]), needle.Tensor([[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187 -0.9844096\n",
      "   1.5315286   0.9754208  -0.67095083 -0.4856012   0.02712796 -0.7918243 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58703290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.3076733]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.0957647  -0.25430438  0.15010968 -0.11833237  0.27174604  0.23971868\n",
      "  -0.01332295  0.0563128   0.24832964 -0.17336863  0.07386482  0.15731502]])\n",
      "        bias_ih    = needle.Tensor([[ 0.14053851 -0.16354756  0.25607312  0.08151072  0.19802132 -0.17549162\n",
      "   0.04330075 -0.15632114  0.11665303 -0.06072076  0.22189605 -0.06392562]])\n",
      "        h          = needle.Tensor([[-0.28056055 -0.45799598 -0.37976828  1.2224702  -0.51011187 -0.9844096\n",
      "   1.5315286   0.9754208  -0.67095083 -0.4856012   0.02712796 -0.7918243 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58703290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]])\n",
      "        self       = needle.Tensor([[0.3076733]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.3076733]]), needle.Tensor([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebdd10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.3076733]]), needle.Tensor([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebdd10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56385af0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56ebcfd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd563869b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56ebcfd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.3076733]], device=cpu())\n",
      "        b          = NDArray([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebdd10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.3076733]], device=cpu())\n",
      "        b          = NDArray([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.3076733]], device=cpu())\n",
      "other = NDArray([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58702930>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58702230>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56ebc770>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.12140635 -0.20516548 -0.03524825  0.04895484 -0.23824579 -0.19620574\n",
      "   0.09301579  0.20946243 -0.08403449 -0.18801036 -0.20996389 -0.11818415]], device=cuda())\n",
      "out        = NDArray([[8.177513e+35 4.582386e-41 6.425311e-34 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.3076733]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.8643851  -0.9331707  -0.090949    1.507122    2.7597601\n",
      "    1.207559    0.6103917  -0.4320682  -0....79   0.3857189   0.7693372\n",
      "   -0.9053026  -0.42460012  0.77293336 -0.33706313  0.5264511\n",
      "    1.6478038  -0.8892285 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808\n",
      "   -0.321494    2.3211508   1.0456505  -1...3   0.24686062  0.20539066\n",
      "    3.1839638   0.75853294  1.2163732   1.7626548  -0.3553288\n",
      "   -1.1517738   0.49734795]]])\n",
      "h0         = needle.Tensor([[[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808\n",
      "   -0.321494    2.3211508   1.0456505  -1...3   0.24686062  0.20539066\n",
      "    3.1839638   0.75853294  1.2163732   1.7626548  -0.3553288\n",
      "   -1.1517738   0.49734795]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57cd0510>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808\n",
      "   -0.321494    2...   0.24686062  0.20539066\n",
      "    3.1839638   0.75853294  1.2163732   1.7626548  -0.3553288\n",
      "   -1.1517738   0.49734795]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57cd0510>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808\n",
      "   -0.321494    2.3211508   1.0456505  -1...3   0.24686062  0.20539066\n",
      "    3.1839638   0.75853294  1.2163732   1.7626548  -0.3553288\n",
      "   -1.1517738   0.49734795]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57cd0510>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.4398514]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.4398514]]]), needle.Tensor([[[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808\n",
      "   -0.32...   0.24686062  0.20539066\n",
      "    3.1839638   0.75853294  1.2163732   1.7626548  -0.3553288\n",
      "   -1.1517738   0.49734795]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57cd2ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.4398514]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808 -0.321494\n",
      "   2.3211508   1.0456505  -1.419857    0.79126376  0.17983271 -0.16752017]])\n",
      "        h0         = (needle.Tensor([[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808 -0.321494\n",
      "   2.3211508   1.0456505  -1.41...24713   0.24686062  0.20539066  3.1839638\n",
      "   0.75853294  1.2163732   1.7626548  -0.3553288  -1.1517738   0.49734795]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4398514]])\n",
      "        inputs     = [needle.Tensor([[0.4398514]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57cd1950>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57cd2ad0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4398514]]), needle.Tensor([[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808 -0.321494\n",
      "   2.3211508   1.0456505  -1.419857    0.79126376  0.17983271 -0.16752017]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57cd1950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4398514]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.097422   -0.05969274 -0.0676458   0.04877698  0.05445626 -0.04336742\n",
      "  -0.19623092 -0.163692   -0.20437852  0.1469765   0.19497952  0.1275883 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.20662734 -0.21883345 -0.1874977   0.23844188 -0.05054469  0.13041165\n",
      "  -0.08906527  0.24085158  0.15197393 -0.25897005 -0.02885008 -0.06784277]])\n",
      "        h          = needle.Tensor([[-0.25456762 -0.03375996  0.44707224  0.24014004  0.03124808 -0.321494\n",
      "   2.3211508   1.0456505  -1.419857    0.79126376  0.17983271 -0.16752017]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57cd1950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]])\n",
      "        self       = needle.Tensor([[0.4398514]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4398514]]), needle.Tensor([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57cd0c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4398514]]), needle.Tensor([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57cd0c90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57ebd370>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd57cd0710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57ebcc30>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd57cd0710>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4398514]], device=cpu())\n",
      "        b          = NDArray([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57cd0c90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4398514]], device=cpu())\n",
      "        b          = NDArray([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4398514]], device=cpu())\n",
      "other = NDArray([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57cd1230>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57cd17b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57cd2ff0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.07255149 -0.06821677  0.18458101  0.21382195  0.07401127 -0.03822267\n",
      "   0.19734848 -0.22896113  0.25127614  0.0693967   0.19676703  0.17983598]], device=cuda())\n",
      "out        = NDArray([[1.4213543e-34 0.0000000e+00 2.6392066e-01 1.5329088e-01 1.6494493e-01\n",
      "  1.8948351e-01 3.7364259e-01 2.5106734e-01 2.3265868e-01 1.1704107e-01\n",
      "  3.3024672e-01 2.0197022e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.4398514]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.0497452   1.6412479   2.4909751  -1.471876    0.30818895\n",
      "   -0.15456527  1.1080132  -0.61717993 -1...5   0.18997401  0.10890406\n",
      "   -0.59128815  0.54141814  2.0691268  -0.22591713 -0.6254304\n",
      "    0.9449948   0.13092934]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603\n",
      "    0.19728708  1.698427    0.6580133   0...5   0.6910103  -0.7259086\n",
      "    0.31951833  2.6618488  -0.80904907  0.3151029   0.29344872\n",
      "    0.05044505  0.23021317]]])\n",
      "h0         = needle.Tensor([[[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603\n",
      "    0.19728708  1.698427    0.6580133   0...5   0.6910103  -0.7259086\n",
      "    0.31951833  2.6618488  -0.80904907  0.3151029   0.29344872\n",
      "    0.05044505  0.23021317]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58504bd0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-0.98...   0.6910103  -0.7259086\n",
      "    0.31951833  2.6618488  -0.80904907  0.3151029   0.29344872\n",
      "    0.05044505  0.23021317]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58504bd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603\n",
      "    0.19728708  1.698427    0.6580133   0...5   0.6910103  -0.7259086\n",
      "    0.31951833  2.6618488  -0.80904907  0.3151029   0.29344872\n",
      "    0.05044505  0.23021317]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58504bd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " ...   0.6910103  -0.7259086\n",
      "    0.31951833  2.6618488  -0.80904907  0.3151029   0.29344872\n",
      "    0.05044505  0.23021317]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58506e10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]\n",
      "\n",
      " [[0.44591194]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603  0.19728708\n",
      "   1.698427    0.6580133   0.08656875  0.71119016 -0.1087369   0.43084887]])\n",
      "        h0         = (needle.Tensor([[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603  0.19728708\n",
      "   1.698427    0.6580133   0....3835   0.6910103  -0.7259086   0.31951833\n",
      "   2.6618488  -0.80904907  0.3151029   0.29344872  0.05044505  0.23021317]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.44591194]])\n",
      "        inputs     = [needle.Tensor([[0.44591194]]), needle.Tensor([[0.44591194]]), needle.Tensor([[0.44591194]]), needle.Tensor([[0.44591194]]), needle.Tensor([[0.44591194]]), needle.Tensor([[0.44591194]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58505090>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58506e10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44591194]]), needle.Tensor([[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603  0.19728708\n",
      "   1.698427    0.6580133   0.08656875  0.71119016 -0.1087369   0.43084887]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58505090>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.44591194]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.09887952 -0.01404184 -0.06160605 -0.13841744 -0.270307    0.03972253\n",
      "   0.05169624 -0.22815262 -0.24968503 -0.05293551  0.18230695  0.12160003]])\n",
      "        bias_ih    = needle.Tensor([[-0.1939255  -0.0344803  -0.19087936  0.13915694 -0.0073972   0.03139603\n",
      "  -0.2132653  -0.09783007  0.271842    0.15776744 -0.22917864 -0.26555327]])\n",
      "        h          = needle.Tensor([[-0.9805859   1.9225154  -0.06666151 -1.0519065   0.05936603  0.19728708\n",
      "   1.698427    0.6580133   0.08656875  0.71119016 -0.1087369   0.43084887]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58505090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]])\n",
      "        self       = needle.Tensor([[0.44591194]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44591194]]), needle.Tensor([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58348f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.44591194]]), needle.Tensor([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58348f50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585ffbf0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58349c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585ff270>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58349c50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44591194]], device=cpu())\n",
      "        b          = NDArray([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58348f50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44591194]], device=cpu())\n",
      "        b          = NDArray([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.44591194]], device=cpu())\n",
      "other = NDArray([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585053b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58505d30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58348ef0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.03586742  0.22849852 -0.13151112 -0.2871262   0.26616305 -0.01701751\n",
      "  -0.09156246  0.17497936 -0.197207   -0.04077728 -0.17081839 -0.2674396 ]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 1.5718986e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.44591194]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-6.5305817e-01  2.2158444e-01 -1.0863721e+00 -5.2262431e-01\n",
      "    2.5945187e-01  1.2255272e+00 -9.69348...747e-01  9.7251207e-01  9.7094959e-01 -4.5680702e-01\n",
      "   -1.3173645e+00  5.1426387e-01  2.9293174e-01 -4.0204680e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "   -4.71406609e-01 -1.64722413e-01 -9... -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "    6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]])\n",
      "h0         = needle.Tensor([[[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "   -4.71406609e-01 -1.64722413e-01 -9... -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "    6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56187590>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 1.98832643e+00  1.30895770e+00 -3....-7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "    6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56187590>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "   -4.71406609e-01 -1.64722413e-01 -9... -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "    6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56187590>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]...0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383...-7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "    6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56185510>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]...0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]\n",
      "  [0.37435383]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "  -4.71406609e-01 -1.64722413e-01 -9.1...01 -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "   6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]])\n",
      "        h0         = (needle.Tensor([[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "  -4.71406609e-01 -1.64722413e-01 -9.... -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "   6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]])\n",
      "        inputs     = [needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56184f10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56185510>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.3...1 -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "   6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56184f10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.21692026  0.21535653 -0.2713021   0.15664184 -0.2718537   0.13668951\n",
      "  -0.15590042 -0.14883858  0.1...13021   0.15664184 -0.2718537   0.13668951\n",
      "  -0.15590042 -0.14883858  0.17954352 -0.11116847  0.1835415   0.16961098]])\n",
      "        bias_ih    = needle.Tensor([[ 0.04767638 -0.21228382 -0.05012245 -0.18196988 -0.08297558 -0.08620368\n",
      "  -0.01144385  0.05632249 -0.1...012245 -0.18196988 -0.08297558 -0.08620368\n",
      "  -0.01144385  0.05632249 -0.14884123  0.03467846 -0.00166544  0.06016377]])\n",
      "        h          = needle.Tensor([[ 1.98832643e+00  1.30895770e+00 -3.62184554e-01  1.04071212e+00\n",
      "  -4.71406609e-01 -1.64722413e-01 -9.1...01 -7.81583548e-01 -5.71250141e-01 -2.36941409e+00\n",
      "   6.74637020e-01  1.22664797e+00  1.42971945e+00 -5.56315601e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56184f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.19544825  0.23163319  0.17078355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]])\n",
      "        self       = needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.3...78355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56187450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.3...78355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56187450>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58730330>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd561851d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584b3a70>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd561851d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]], device=cpu())\n",
      "        b          = NDArray([[ 0.19544825  0.23163319  0.17078355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56187450>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]], device=cpu())\n",
      "        b          = NDArray([[ 0.19544825  0.23163319  0.17078355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]], device=cpu())\n",
      "other = NDArray([[ 0.19544825  0.23163319  0.17078355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561852f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd561840f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56186530>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.19544825  0.23163319  0.17078355  0.04133213  0.06498376 -0.25504562\n",
      "  -0.22226961 -0.00968531 -0.08466925  0.17274028 -0.28387377  0.21838641]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 5.1652216e+13 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]\n",
      " [0.37435383]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.9193166   0.1367743   0.05907753 -2.2650535   1.410696\n",
      "    0.646521    0.04579311 -0.24680759  0.9...  -0.21570078  0.34927538\n",
      "   -0.3670721  -1.277713   -0.25062588 -1.4174368  -0.53532696\n",
      "   -1.1611105  -1.2504435 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906\n",
      "    0.65674776 -0.5066145  -0.8955068   1...   1.3176728   0.57967734\n",
      "   -0.793      -0.72293586 -0.18500625  0.19576778  0.20856294\n",
      "    0.22228792 -1.4494368 ]]])\n",
      "h0         = needle.Tensor([[[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906\n",
      "    0.65674776 -0.5066145  -0.8955068   1...   1.3176728   0.57967734\n",
      "   -0.793      -0.72293586 -0.18500625  0.19576778  0.20856294\n",
      "    0.22228792 -1.4494368 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56485450>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....  1.3176728   0.57967734\n",
      "   -0.793      -0.72293586 -0.18500625  0.19576778  0.20856294\n",
      "    0.22228792 -1.4494368 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56485450>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906\n",
      "    0.65674776 -0.5066145  -0.8955068   1...   1.3176728   0.57967734\n",
      "   -0.793      -0.72293586 -0.18500625  0.19576778  0.20856294\n",
      "    0.22228792 -1.4494368 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56485450>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]...0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475...  1.3176728   0.57967734\n",
      "   -0.793      -0.72293586 -0.18500625  0.19576778  0.20856294\n",
      "    0.22228792 -1.4494368 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56487dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]...0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]\n",
      "  [0.14061475]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906  0.65674776\n",
      "  -0.5066145  -0.8955068   1.1... 0.501583    1.3176728   0.57967734 -0.793\n",
      "  -0.72293586 -0.18500625  0.19576778  0.20856294  0.22228792 -1.4494368 ]])\n",
      "        h0         = (needle.Tensor([[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906  0.65674776\n",
      "  -0.5066145  -0.8955068   1.....501583    1.3176728   0.57967734 -0.793\n",
      "  -0.72293586 -0.18500625  0.19576778  0.20856294  0.22228792 -1.4494368 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]])\n",
      "        inputs     = [needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.1... [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56487710>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56487dd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.1...0.501583    1.3176728   0.57967734 -0.793\n",
      "  -0.72293586 -0.18500625  0.19576778  0.20856294  0.22228792 -1.4494368 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56487710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.20548663  0.09082296 -0.13749857 -0.2091878   0.08551461  0.18990546\n",
      "   0.08512044 -0.22302818  0.1...749857 -0.2091878   0.08551461  0.18990546\n",
      "   0.08512044 -0.22302818  0.17904258  0.11482859 -0.06216903 -0.00331309]])\n",
      "        bias_ih    = needle.Tensor([[ 0.13395277  0.24551427 -0.26430303 -0.0251573  -0.00027779 -0.05038761\n",
      "   0.07497388 -0.2501569  -0.0...430303 -0.0251573  -0.00027779 -0.05038761\n",
      "   0.07497388 -0.2501569  -0.00313634 -0.04279529 -0.18894309 -0.2247791 ]])\n",
      "        h          = needle.Tensor([[-0.41341746  0.21096697 -1.4782765  -0.38033912  0.97560906  0.65674776\n",
      "  -0.5066145  -0.8955068   1.1... 0.501583    1.3176728   0.57967734 -0.793\n",
      "  -0.72293586 -0.18500625  0.19576778  0.20856294  0.22228792 -1.4494368 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56487710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 1.2939289e-01 -2.7697125e-01  2.1956694e-01 -1.6519113e-01\n",
      "  -2.6825666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]])\n",
      "        self       = needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.1...5666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.1...5666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090e10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57aa4d70>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd58091f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57aa6bf0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd58091f10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]], device=cpu())\n",
      "        b          = NDArray([[ 1.2939289e-01 -2.7697125e-01  2.1956694e-01 -1.6519113e-01\n",
      "  -2.6825666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58090e10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]], device=cpu())\n",
      "        b          = NDArray([[ 1.2939289e-01 -2.7697125e-01  2.1956694e-01 -1.6519113e-01\n",
      "  -2.6825666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]], device=cpu())\n",
      "other = NDArray([[ 1.2939289e-01 -2.7697125e-01  2.1956694e-01 -1.6519113e-01\n",
      "  -2.6825666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56484d70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56485bb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58091cb0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 1.2939289e-01 -2.7697125e-01  2.1956694e-01 -1.6519113e-01\n",
      "  -2.6825666e-01  1.6816032e-01  1.1705023e-01 -4.4779480e-03\n",
      "  -9.4477847e-02 -2.3651123e-04  2.1165943e-01 -7.1089268e-03]], device=cuda())\n",
      "out        = NDArray([[5.1026300e-34 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]\n",
      " [0.14061475]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 9.99046922e-01  3.78260054e-02  1.70354736e+00  5.03059864e-01\n",
      "   -4.93830115e-01 -2.55525970e+00 -3... -1.43550023e-01  4.55326289e-01  3.57173979e-01\n",
      "    7.80850589e-01  8.02440107e-01 -7.11630464e-01 -4.55780715e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715\n",
      "   -0.10782964 -0.37293458  0.8394606   0....   0.10713123 -0.17333865\n",
      "   -0.8829801  -0.18179148 -0.164526   -0.11713386 -0.19181152\n",
      "    0.81610334 -0.42244673]]])\n",
      "h0         = needle.Tensor([[[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715\n",
      "   -0.10782964 -0.37293458  0.8394606   0....   0.10713123 -0.17333865\n",
      "   -0.8829801  -0.18179148 -0.164526   -0.11713386 -0.19181152\n",
      "    0.81610334 -0.42244673]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57fc9e50>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 0.8000694   0.76065814  1.4136983 ...  0.10713123 -0.17333865\n",
      "   -0.8829801  -0.18179148 -0.164526   -0.11713386 -0.19181152\n",
      "    0.81610334 -0.42244673]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57fc9e50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715\n",
      "   -0.10782964 -0.37293458  0.8394606   0....   0.10713123 -0.17333865\n",
      "   -0.8829801  -0.18179148 -0.164526   -0.11713386 -0.19181152\n",
      "    0.81610334 -0.42244673]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57fc9e50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]...0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766...  0.10713123 -0.17333865\n",
      "   -0.8829801  -0.18179148 -0.164526   -0.11713386 -0.19181152\n",
      "    0.81610334 -0.42244673]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57fc84d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]...0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]\n",
      "  [0.47200766]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715  -0.10782964\n",
      "  -0.37293458  0.8394606   0.0...1922463 -1.4062303  -1.0190654   1.0600564\n",
      "  -0.4698363  -0.44465914 -1.6765273  -0.4718379  -1.1309655   0.99790865]])\n",
      "        h0         = (needle.Tensor([[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715  -0.10782964\n",
      "  -0.37293458  0.8394606   0....67098   0.10713123 -0.17333865 -0.8829801\n",
      "  -0.18179148 -0.164526   -0.11713386 -0.19181152  0.81610334 -0.42244673]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]])\n",
      "        inputs     = [needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57fc8cd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57fc84d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.4...922463 -1.4062303  -1.0190654   1.0600564\n",
      "  -0.4698363  -0.44465914 -1.6765273  -0.4718379  -1.1309655   0.99790865]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57fc8cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.17204876  0.0436646   0.06394586 -0.25444233 -0.14901547  0.13819388\n",
      "   0.14412579  0.2700019   0.0...394586 -0.25444233 -0.14901547  0.13819388\n",
      "   0.14412579  0.2700019   0.05728969 -0.01738131  0.21730363 -0.06978683]])\n",
      "        bias_ih    = needle.Tensor([[-0.09715883  0.04810292  0.15199417  0.13235185  0.06840551 -0.22346964\n",
      "   0.06171611 -0.179555   -0.0...199417  0.13235185  0.06840551 -0.22346964\n",
      "   0.06171611 -0.179555   -0.09513471  0.00057477 -0.26212507  0.21733814]])\n",
      "        h          = needle.Tensor([[ 0.8000694   0.76065814  1.4136983  -0.1916018  -1.0267715  -0.10782964\n",
      "  -0.37293458  0.8394606   0.0...1922463 -1.4062303  -1.0190654   1.0600564\n",
      "  -0.4698363  -0.44465914 -1.6765273  -0.4718379  -1.1309655   0.99790865]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57fc8cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.25757346 -0.11586645  0.12788883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]])\n",
      "        self       = needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.4...88883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57fca790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.4...88883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57fca790>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58709c30>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd587d7410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5870a630>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd587d7410>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]], device=cpu())\n",
      "        b          = NDArray([[-0.25757346 -0.11586645  0.12788883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57fca790>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]], device=cpu())\n",
      "        b          = NDArray([[-0.25757346 -0.11586645  0.12788883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]], device=cpu())\n",
      "other = NDArray([[-0.25757346 -0.11586645  0.12788883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57fcb4b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57fcac30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587d4630>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.25757346 -0.11586645  0.12788883  0.21404439  0.24876928 -0.07064535\n",
      "   0.03634864  0.11574286 -0.15154876  0.07003278  0.20686877 -0.07126398]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 9.9407823e+13 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]\n",
      " [0.47200766]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.44516754e+00 -6.78973319e-03 -1.56049442e+00  1.07879423e-01\n",
      "   -7.12164164e-01 -6.72460258e-01 -7...  6.23161674e-01 -5.94616309e-02 -1.28307879e+00\n",
      "    1.78684103e+00 -8.08469951e-01 -1.66685000e-01  6.79313540e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 4.21250969e-01 -7.13725507e-01  1.33876288e+00 -1.35747898e+00\n",
      "   -1.25506866e+00  2.15844607e+00 -1... -1.23747118e-01  1.58724144e-01 -3.83711338e-01\n",
      "    1.86009943e-01  4.98015136e-01 -5.03199160e-01  1.27623275e-01]]])\n",
      "h0         = needle.Tensor([[[ 4.21250969e-01 -7.13725507e-01  1.33876288e+00 -1.35747898e+00\n",
      "   -1.25506866e+00  2.15844607e+00 -1... -1.23747118e-01  1.58724144e-01 -3.83711338e-01\n",
      "    1.86009943e-01  4.98015136e-01 -5.03199160e-01  1.27623275e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58349310>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....-1.23747118e-01  1.58724144e-01 -3.83711338e-01\n",
      "    1.86009943e-01  4.98015136e-01 -5.03199160e-01  1.27623275e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58349310>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 4.21250969e-01 -7.13725507e-01  1.33876288e+00 -1.35747898e+00\n",
      "   -1.25506866e+00  2.15844607e+00 -1... -1.23747118e-01  1.58724144e-01 -3.83711338e-01\n",
      "    1.86009943e-01  4.98015136e-01 -5.03199160e-01  1.27623275e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58349310>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]...0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612...-1.23747118e-01  1.58724144e-01 -3.83711338e-01\n",
      "    1.86009943e-01  4.98015136e-01 -5.03199160e-01  1.27623275e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5834a550>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]...0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]\n",
      "  [0.15100612]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.42125097 -0.7137255   1.3387629  -1.357479   -1.2550687   2.158446\n",
      "  -1.0849718  -1.5729271  -0.047...203183  0.44892487 -1.0512592  -0.07078581\n",
      "  -0.4657518   0.5016494  -0.3776373   0.57196224  0.35856584  0.6911925 ]])\n",
      "        h0         = (needle.Tensor([[ 0.42125097 -0.7137255   1.3387629  -1.357479   -1.2550687   2.158446\n",
      "  -1.0849718  -1.5729271  -0.04...0667  -4.2238197   0.6668836  -0.12374712\n",
      "   0.15872414 -0.38371134  0.18600994  0.49801514 -0.50319916  0.12762327]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]])\n",
      "        inputs     = [needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.1... [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58348210>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5834a550>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.1...03183  0.44892487 -1.0512592  -0.07078581\n",
      "  -0.4657518   0.5016494  -0.3776373   0.57196224  0.35856584  0.6911925 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58348210>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.1828734   0.08787414  0.2439748  -0.06294237 -0.20100425 -0.1558911\n",
      "  -0.2000167  -0.03532949 -0.14...439748  -0.06294237 -0.20100425 -0.1558911\n",
      "  -0.2000167  -0.03532949 -0.14862183 -0.08675259 -0.2112048   0.02823907]])\n",
      "        bias_ih    = needle.Tensor([[ 0.08424228 -0.24861953  0.12176019 -0.16004162  0.21895665 -0.04446451\n",
      "   0.2020781  -0.24968272  0.2...176019 -0.16004162  0.21895665 -0.04446451\n",
      "   0.2020781  -0.24968272  0.22308189 -0.09634408  0.21118572 -0.20887244]])\n",
      "        h          = needle.Tensor([[ 0.42125097 -0.7137255   1.3387629  -1.357479   -1.2550687   2.158446\n",
      "  -1.0849718  -1.5729271  -0.047...203183  0.44892487 -1.0512592  -0.07078581\n",
      "  -0.4657518   0.5016494  -0.3776373   0.57196224  0.35856584  0.6911925 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58348210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.28739884 -0.1471336   0.12810764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]])\n",
      "        self       = needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.1...10764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585dea10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.1...10764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585dea10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584b60f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd585dcc10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584b6bb0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd585dcc10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]], device=cpu())\n",
      "        b          = NDArray([[-0.28739884 -0.1471336   0.12810764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585dea10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]], device=cpu())\n",
      "        b          = NDArray([[-0.28739884 -0.1471336   0.12810764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]], device=cpu())\n",
      "other = NDArray([[-0.28739884 -0.1471336   0.12810764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58349430>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5834b030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585dda30>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.28739884 -0.1471336   0.12810764  0.02967143 -0.1783973  -0.22255462\n",
      "  -0.14327462 -0.009204   -0.21822974 -0.26683846 -0.06103371  0.11152601]], device=cuda())\n",
      "out        = NDArray([[9.03555671e-35 0.00000000e+00 2.32063891e-34 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0...000000e+00 1.20371762e-38 0.00000000e+00\n",
      "  1.24666238e-38 0.00000000e+00 1.20371762e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]\n",
      " [0.15100612]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.6676226  -1.0269749  -1.5112581   0.33446866 -1.5718296\n",
      "    0.26567537 -0.9487685   1.3183743   0.32010606  0.5051416\n",
      "    0.49171734  1.0898587 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425\n",
      "   -0.86632925  0.6183043   0.16629325  0.9296573   0.91212416\n",
      "    0.6917912   0.65102065]]])\n",
      "h0         = needle.Tensor([[[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425\n",
      "   -0.86632925  0.6183043   0.16629325  0.9296573   0.91212416\n",
      "    0.6917912   0.65102065]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57e89a90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425\n",
      "   -0.86632925  0.6183043   0.16629325  0.9296573   0.91212416\n",
      "    0.6917912   0.65102065]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e89a90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425\n",
      "   -0.86632925  0.6183043   0.16629325  0.9296573   0.91212416\n",
      "    0.6917912   0.65102065]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e89a90>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "   0.9426786  0.20810682 0.3392497  ...   0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "   0.8669709  0.7930699  0.6972242  0.6930648 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "   0.9426786  0.20810682 0.3392497 ...  -1.0386899   0.6557425\n",
      "   -0.86632925  0.6183043   0.16629325  0.9296573   0.91212416\n",
      "    0.6917912   0.65102065]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e899d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "   0.9426786  0.20810682 0.3392497  ...   0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "   0.8669709  0.7930699  0.6972242  0.6930648 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425  -0.86632925\n",
      "   0.6183043   0.16629325  0.9296573   0.91212416  0.6917912   0.65102065]])\n",
      "        h0         = (needle.Tensor([[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425  -0.86632925\n",
      "   0.6183043   0.16629325  0.9296573   0.91212416  0.6917912   0.65102065]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0....24\n",
      "  0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]])\n",
      "        inputs     = [needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0...4\n",
      "  0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e884d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e899d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0...1352  -1.0386899   0.6557425  -0.86632925\n",
      "   0.6183043   0.16629325  0.9296573   0.91212416  0.6917912   0.65102065]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e884d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0....24\n",
      "  0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.0034855  -0.04382002  0.20303497  0.12530959  0.15488756  0.11095682\n",
      "  -0.15816651 -0.28014144  0.18121186  0.28525686 -0.18742487 -0.13730395]])\n",
      "        bias_ih    = needle.Tensor([[-0.07905397 -0.0626204   0.01190293  0.17061013  0.19618335 -0.04063877\n",
      "  -0.02378216 -0.1061257   0.15525761  0.25917512  0.23231286 -0.09627433]])\n",
      "        h          = needle.Tensor([[-0.24783887 -0.33983144  0.2621352  -1.0386899   0.6557425  -0.86632925\n",
      "   0.6183043   0.16629325  0.9296573   0.91212416  0.6917912   0.65102065]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e884d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.02621403 -0.20791864 -0.00255087  0.20851436  0.23418063 -0.12850645\n",
      "   0.03972152  0.1065172   0.2...811614  0.13670799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]])\n",
      "        self       = needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0....24\n",
      "  0.9787091  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0...11614  0.13670799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e880d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0...11614  0.13670799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e880d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5716ccb0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57e8acd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5716df30>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57e8acd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0.038268...  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.02621403 -0.20791864 -0.00255087  0.20851436  0.23418063 -0.12850645\n",
      "   0.03972152  0.1065172   0.2051631...799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e880d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0.038268...  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.02621403 -0.20791864 -0.00255087  0.20851436  0.23418063 -0.12850645\n",
      "   0.03972152  0.1065172   0.2051631...799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0.038268...  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]], device=cpu())\n",
      "other = NDArray([[ 0.02621403 -0.20791864 -0.00255087  0.20851436  0.23418063 -0.12850645\n",
      "   0.03972152  0.1065172   0.2051631...799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57e891b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57e8bd30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57e89030>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.02621403 -0.20791864 -0.00255087  0.20851436  0.23418063 -0.12850645\n",
      "   0.03972152  0.1065172   0.2051631...799  0.05120164 -0.24534532\n",
      "  -0.0247055   0.0364694   0.00711787  0.22420007 -0.22602332 -0.20808446]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 2.7857049e-34 0.0000000e+00 1.1808202e-01\n",
      "  8.1913851e-02 5.6347340e-02 2.6339063e-02 2.0122123e-01 5.1345851e-02\n",
      "  2.5970688e-02 2.2139128e-02]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.98934066 0.65589947 0.01971657 0.8285738  0.7015047  0.9071358\n",
      "  0.9426786  0.20810682 0.3392497  0.038268...  0.10817547 0.18613103 0.8063271  0.07231937 0.43244353\n",
      "  0.8669709  0.7930699  0.6972242  0.6930648 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.42453396  0.5845636  -1.2369071   0.2647715  -1.1808362\n",
      "    0.30671293 -1.3490852   0.3019343  -0.50695556 -0.9852052\n",
      "    0.19965848  2.4108386 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409\n",
      "   -0.00650134  0.8996556  -0.6079941   1.0295954   1.6762394\n",
      "   -0.7302734   0.48128602]]])\n",
      "h0         = needle.Tensor([[[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409\n",
      "   -0.00650134  0.8996556  -0.6079941   1.0295954   1.6762394\n",
      "   -0.7302734   0.48128602]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd585079d0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[ 1.05...9   1.0291061  -0.4774409\n",
      "   -0.00650134  0.8996556  -0.6079941   1.0295954   1.6762394\n",
      "   -0.7302734   0.48128602]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585079d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409\n",
      "   -0.00650134  0.8996556  -0.6079941   1.0295954   1.6762394\n",
      "   -0.7302734   0.48128602]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd585079d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "   0.30417708 0.10532817 0.1433344 ...   0.67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "   0.51240194 0.5596371  0.6019605  0.11150056]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "   0.30417708 0.10532817 0.1433344...9   1.0291061  -0.4774409\n",
      "   -0.00650134  0.8996556  -0.6079941   1.0295954   1.6762394\n",
      "   -0.7302734   0.48128602]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585056d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "   0.30417708 0.10532817 0.1433344 ...   0.67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "   0.51240194 0.5596371  0.6019605  0.11150056]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409  -0.00650134\n",
      "   0.8996556  -0.6079941   1.0295954   1.6762394  -0.7302734   0.48128602]])\n",
      "        h0         = (needle.Tensor([[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409  -0.00650134\n",
      "   0.8996556  -0.6079941   1.0295954   1.6762394  -0.7302734   0.48128602]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0...26\n",
      "  0.67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]])\n",
      "        inputs     = [needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  ....67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585069d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585056d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  ...8349   1.0291061  -0.4774409  -0.00650134\n",
      "   0.8996556  -0.6079941   1.0295954   1.6762394  -0.7302734   0.48128602]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585069d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0...26\n",
      "  0.67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.2602774   0.16365293  0.06287807 -0.01495552  0.0178614   0.01868469\n",
      "   0.2874828   0.25100023 -0.2585249  -0.11160381  0.2585131   0.00318968]])\n",
      "        bias_ih    = needle.Tensor([[-0.02588114 -0.2747439   0.19223097  0.2759353  -0.10831222  0.09858382\n",
      "   0.00123012 -0.08975674  0.03984013  0.18112868 -0.10296834  0.06375992]])\n",
      "        h          = needle.Tensor([[ 1.058751   -0.58791053  0.3758349   1.0291061  -0.4774409  -0.00650134\n",
      "   0.8996556  -0.6079941   1.0295954   1.6762394  -0.7302734   0.48128602]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585069d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.2533511   0.20038766 -0.05763899  0.02708885  0.10777426 -0.02035445\n",
      "  -0.2732852   0.09321532  0.0...68024  -0.28409055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]])\n",
      "        self       = needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0...26\n",
      "  0.67527866 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  ...8024  -0.28409055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57339ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  ...8024  -0.28409055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57339ed0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56413a70>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57339910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56411af0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57339910>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0.53703...6 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]], device=cpu())\n",
      "        b          = NDArray([[ 0.2533511   0.20038766 -0.05763899  0.02708885  0.10777426 -0.02035445\n",
      "  -0.2732852   0.09321532  0.0436398...055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57339ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0.53703...6 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]], device=cpu())\n",
      "        b          = NDArray([[ 0.2533511   0.20038766 -0.05763899  0.02708885  0.10777426 -0.02035445\n",
      "  -0.2732852   0.09321532  0.0436398...055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0.53703...6 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]], device=cpu())\n",
      "other = NDArray([[ 0.2533511   0.20038766 -0.05763899  0.02708885  0.10777426 -0.02035445\n",
      "  -0.2732852   0.09321532  0.0436398...055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585074f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58504af0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57338d70>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.2533511   0.20038766 -0.05763899  0.02708885  0.10777426 -0.02035445\n",
      "  -0.2732852   0.09321532  0.0436398...055  0.24362397  0.12411895\n",
      "   0.03672653  0.23742175 -0.15166126  0.14778632  0.16562817 -0.11929686]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 2.9224516e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.7147819  0.5405403  0.6446     0.09703874 0.74518514 0.59971714\n",
      "  0.30417708 0.10532817 0.1433344  0.53703...6 0.01255571 0.0165239  0.2721536  0.0903869  0.70009035\n",
      "  0.51240194 0.5596371  0.6019605  0.11150056]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-2.3018832   0.18943575 -0.34705558 -1.7362674  -0.33850208\n",
      "    0.19525617 -3.447916    0.41938952 -0...39  -1.0663588  -0.3913598\n",
      "    1.0471506  -0.51650494  1.5709789   0.17968743  0.5074564\n",
      "   -0.24016395 -0.9839025 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295\n",
      "    1.2712735   2.257143    1.8325479   1....   -0.06953806 -0.8692122\n",
      "    1.3763105   0.7073787   1.1210015   0.56306565  0.74744946\n",
      "   -0.02602876  0.18566193]]])\n",
      "h0         = needle.Tensor([[[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295\n",
      "    1.2712735   2.257143    1.8325479   1....   -0.06953806 -0.8692122\n",
      "    1.3763105   0.7073787   1.1210015   0.56306565  0.74744946\n",
      "   -0.02602876  0.18566193]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57dd9e90>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), needle.Tensor([[[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295\n",
      "    1.2712735   2....  -0.06953806 -0.8692122\n",
      "    1.3763105   0.7073787   1.1210015   0.56306565  0.74744946\n",
      "   -0.02602876  0.18566193]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57dd9e90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295\n",
      "    1.2712735   2.257143    1.8325479   1....   -0.06953806 -0.8692122\n",
      "    1.3763105   0.7073787   1.1210015   0.56306565  0.74744946\n",
      "   -0.02602876  0.18566193]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57dd9e90>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "   0.42438975 0.74472785 0.51995444...\n",
      "   0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "   0.64395607 0.2593336  0.54324764 0.33215544]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "   0.42438975 0.74472785 0.5199544...  -0.06953806 -0.8692122\n",
      "    1.3763105   0.7073787   1.1210015   0.56306565  0.74744946\n",
      "   -0.02602876  0.18566193]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ddbbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "   0.42438975 0.74472785 0.51995444...\n",
      "   0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "   0.64395607 0.2593336  0.54324764 0.33215544]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295   1.2712735\n",
      "   2.257143    1.8325479   1.244458   -0.5761717  -1.4334464   0.44761488]])\n",
      "        h0         = (needle.Tensor([[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295   1.2712735\n",
      "   2.257143    1.8325479   1.2...2744   -0.06953806 -0.8692122   1.3763105\n",
      "   0.7073787   1.1210015   0.56306565  0.74744946 -0.02602876  0.18566193]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0...092\n",
      "  0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]])\n",
      "        inputs     = [needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 ...92\n",
      "  0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ddbdd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ddbbd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 ...77383  -0.8378841   1.7789295   1.2712735\n",
      "   2.257143    1.8325479   1.244458   -0.5761717  -1.4334464   0.44761488]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ddbdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0...092\n",
      "  0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.22511065 -0.25719348  0.22105289 -0.22699106 -0.07446326 -0.03427258\n",
      "  -0.0261372  -0.22710454  0.18045637  0.03886434  0.2868513   0.23807961]])\n",
      "        bias_ih    = needle.Tensor([[-0.11329621  0.22581047  0.16316849  0.03668013 -0.03691107  0.00769785\n",
      "   0.2775439  -0.11399931  0.23141181 -0.2700148   0.02343872 -0.09944099]])\n",
      "        h          = needle.Tensor([[-0.28410923  1.3409449   0.8277383  -0.8378841   1.7789295   1.2712735\n",
      "   2.257143    1.8325479   1.244458   -0.5761717  -1.4334464   0.44761488]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ddbdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.16627526 -0.20952198 -0.27516165  0.15684393  0.11781377 -0.10966164\n",
      "  -0.05681911 -0.2671208   0.0...308192  0.1294618  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]])\n",
      "        self       = needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0...092\n",
      "  0.4604354  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 ...08192  0.1294618  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ddbc10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 ...08192  0.1294618  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ddbc10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58010c70>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57dd8a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58010930>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57dd8a10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0.02600...4  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]], device=cpu())\n",
      "        b          = NDArray([[ 0.16627526 -0.20952198 -0.27516165  0.15684393  0.11781377 -0.10966164\n",
      "  -0.05681911 -0.2671208   0.0435345...18  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ddbc10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0.02600...4  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]], device=cpu())\n",
      "        b          = NDArray([[ 0.16627526 -0.20952198 -0.27516165  0.15684393  0.11781377 -0.10966164\n",
      "  -0.05681911 -0.2671208   0.0435345...18  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0.02600...4  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]], device=cpu())\n",
      "other = NDArray([[ 0.16627526 -0.20952198 -0.27516165  0.15684393  0.11781377 -0.10966164\n",
      "  -0.05681911 -0.2671208   0.0435345...18  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57dd8b70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57dd81b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57dd8bf0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.16627526 -0.20952198 -0.27516165  0.15684393  0.11781377 -0.10966164\n",
      "  -0.05681911 -0.2671208   0.0435345...18  -0.28501862  0.27425063\n",
      "   0.1494081  -0.28287724  0.00348878  0.24381739  0.08543253 -0.09395699]], device=cuda())\n",
      "out        = NDArray([[8.2777302e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.4093246e-34\n",
      "  0.0000000e+00 4.7179937e-27           nan           nan 1.4012985e-45\n",
      "  1.5334618e-34 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.69861025 0.82509017 0.24644798 0.8706897  0.98918957 0.86395603\n",
      "  0.42438975 0.74472785 0.51995444 0.02600...4  0.8363548  0.21293268 0.60550517 0.41596848 0.5619388\n",
      "  0.64395607 0.2593336  0.54324764 0.33215544]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.1630691   0.29181084 -0.04505282 -1.0518205   0.69431126\n",
      "   -0.60020083 -0.67643684 -0.38882333  1...2   1.1904353   0.41753292\n",
      "   -2.1187432   1.0674423  -1.6950564  -0.14680979 -1.8126904\n",
      "    0.97514784  1.1189001 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "   -3.1681809e-01  2.4843442e+00  6.91423...704e-01 -1.5732461e+00  1.2091538e+00  4.2182231e-01\n",
      "   -2.9031765e-01  1.5545062e+00 -8.0910045e-01  1.8951981e-01]]])\n",
      "h0         = needle.Tensor([[[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "   -3.1681809e-01  2.4843442e+00  6.91423...704e-01 -1.5732461e+00  1.2091538e+00  4.2182231e-01\n",
      "   -2.9031765e-01  1.5545062e+00 -8.0910045e-01  1.8951981e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd576dafd0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[[-8.14...04e-01 -1.5732461e+00  1.2091538e+00  4.2182231e-01\n",
      "   -2.9031765e-01  1.5545062e+00 -8.0910045e-01  1.8951981e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd576dafd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "   -3.1681809e-01  2.4843442e+00  6.91423...704e-01 -1.5732461e+00  1.2091538e+00  4.2182231e-01\n",
      "   -2.9031765e-01  1.5545062e+00 -8.0910045e-01  1.8951981e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd576dafd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "   0.66294956 0.85330343 0.5693994 ...   0.21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "   0.19641487 0.93964297 0.7398162  0.8638739 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "   0.66294956 0.85330343 0.5693994...04e-01 -1.5732461e+00  1.2091538e+00  4.2182231e-01\n",
      "   -2.9031765e-01  1.5545062e+00 -8.0910045e-01  1.8951981e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576d9010>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "   0.66294956 0.85330343 0.5693994 ...   0.21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "   0.19641487 0.93964297 0.7398162  0.8638739 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "  -3.1681809e-01  2.4843442e+00  6.9142359e-01  9.8013741e-01\n",
      "  -1.7824161e-01  3.8797668e-01  2.2686261e-01  7.2744370e-01]])\n",
      "        h0         = (needle.Tensor([[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "  -3.1681809e-01  2.4843442e+00  6.914235...099053  0.93361366  0.59959704 -1.5732461\n",
      "   1.2091538   0.4218223  -0.29031765  1.5545062  -0.80910045  0.18951981]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0...06\n",
      "  0.21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]])\n",
      "        inputs     = [needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  ....21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576da4d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576d9010>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  ...1809e-01  2.4843442e+00  6.9142359e-01  9.8013741e-01\n",
      "  -1.7824161e-01  3.8797668e-01  2.2686261e-01  7.2744370e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576da4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0...06\n",
      "  0.21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.21243668  0.0961203  -0.14446059 -0.04841837  0.09658271 -0.22450855\n",
      "   0.19277856 -0.26598796  0.14130878  0.10107946  0.11224079  0.00737128]])\n",
      "        bias_ih    = needle.Tensor([[ 0.1412996   0.24173737 -0.13373455  0.18722993 -0.27080286 -0.20690647\n",
      "  -0.17882824  0.17548421 -0.16003089 -0.02865934 -0.1625713   0.16718075]])\n",
      "        h          = needle.Tensor([[-8.1423228e-04 -7.9775405e-01 -9.7639108e-01  1.4397140e+00\n",
      "  -3.1681809e-01  2.4843442e+00  6.9142359e-01  9.8013741e-01\n",
      "  -1.7824161e-01  3.8797668e-01  2.2686261e-01  7.2744370e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576da4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.17091021  0.16900405 -0.26251218 -0.10590278 -0.0015485  -0.13936566\n",
      "   0.23512876 -0.24654499  0.2...0313311 -0.19489533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]])\n",
      "        self       = needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0...06\n",
      "  0.21826471 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  ...313311 -0.19489533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5735d690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  ...313311 -0.19489533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5735d690>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ab7870>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5735e890>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ab4df0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5735e890>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0.97887...1 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.17091021  0.16900405 -0.26251218 -0.10590278 -0.0015485  -0.13936566\n",
      "   0.23512876 -0.24654499  0.2773502...9533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5735d690>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0.97887...1 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.17091021  0.16900405 -0.26251218 -0.10590278 -0.0015485  -0.13936566\n",
      "   0.23512876 -0.24654499  0.2773502...9533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0.97887...1 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]], device=cpu())\n",
      "other = NDArray([[ 0.17091021  0.16900405 -0.26251218 -0.10590278 -0.0015485  -0.13936566\n",
      "   0.23512876 -0.24654499  0.2773502...9533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd576d9830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd576db030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5735e0f0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.17091021  0.16900405 -0.26251218 -0.10590278 -0.0015485  -0.13936566\n",
      "   0.23512876 -0.24654499  0.2773502...9533 -0.19052415 -0.1248817\n",
      "   0.1413311   0.18753457  0.25752437  0.26224232  0.12467822 -0.26065585]], device=cuda())\n",
      "out        = NDArray([[1.4441251e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.88897055 0.918008   0.29186314 0.04414826 0.9741621  0.61545205\n",
      "  0.66294956 0.85330343 0.5693994  0.97887...1 0.2605036  0.8528393  0.1340973  0.67684174 0.53890747\n",
      "  0.19641487 0.93964297 0.7398162  0.8638739 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.52267262e-01  1.26286650e+00 -6.89056516e-01  1.56121911e-04\n",
      "   -1.71042442e-01  6.30377054e-01  1... -3.35843474e-01 -1.06884658e+00 -1.41735101e+00\n",
      "    7.78726280e-01  7.23865509e-01  5.54309487e-01 -1.54861486e+00]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "    1.33206293e-01 -6.42969310e-01 -3... -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "   -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]])\n",
      "h0         = needle.Tensor([[[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "    1.33206293e-01 -6.42969310e-01 -3... -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "   -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58080c10>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[ 3.52276176e-01 -3.46777588e-01  2....-1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "   -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58080c10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "    1.33206293e-01 -6.42969310e-01 -3... -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "   -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58080c10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "   0.7308294  0.30786046 0.38129938 ...   0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "   0.00534553 0.75581133 0.8804441  0.69925934]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "   0.7308294  0.30786046 0.38129938...-1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "   -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58080b10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "   0.7308294  0.30786046 0.38129938 ...   0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "   0.00534553 0.75581133 0.8804441  0.69925934]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "   1.33206293e-01 -6.42969310e-01 -3.9...00 -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "  -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]])\n",
      "        h0         = (needle.Tensor([[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "   1.33206293e-01 -6.42969310e-01 -3.... -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "  -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0....47\n",
      "  0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]])\n",
      "        inputs     = [needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0...7\n",
      "  0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58081cd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58080b10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0...0 -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "  -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58081cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0....47\n",
      "  0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.20847479 -0.08408034  0.2625944  -0.0610798   0.11471784 -0.03490096\n",
      "  -0.25810486  0.07229659  0.2...25944  -0.0610798   0.11471784 -0.03490096\n",
      "  -0.25810486  0.07229659  0.20652509  0.05728641 -0.2210933   0.18993339]])\n",
      "        bias_ih    = needle.Tensor([[-0.00112614 -0.19787134  0.24158382  0.21805221 -0.05580367  0.18825158\n",
      "  -0.09260857  0.25751096  0.2...158382  0.21805221 -0.05580367  0.18825158\n",
      "  -0.09260857  0.25751096  0.20134524  0.06573042  0.1468594  -0.09175855]])\n",
      "        h          = needle.Tensor([[ 3.52276176e-01 -3.46777588e-01  2.17456746e+00 -1.08275700e+00\n",
      "   1.33206293e-01 -6.42969310e-01 -3.9...00 -1.50212073e+00 -8.15941155e-01 -6.10273071e-02\n",
      "  -4.73145425e-01 -9.08604622e-01  8.01013038e-02 -7.97987655e-02]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58081cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.06664282 -0.2550448   0.076276    0.1475997   0.17835519 -0.04191424\n",
      "   0.21739548  0.06704348  0.0...245675 -0.14334643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]])\n",
      "        self       = needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0....47\n",
      "  0.3193449  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0...45675 -0.14334643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58081d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0...45675 -0.14334643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58081d50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5833da70>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd580820d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5833ddf0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd580820d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0.354020...  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]], device=cpu())\n",
      "        b          = NDArray([[ 0.06664282 -0.2550448   0.076276    0.1475997   0.17835519 -0.04191424\n",
      "   0.21739548  0.06704348  0.0099231...643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58081d50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0.354020...  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]], device=cpu())\n",
      "        b          = NDArray([[ 0.06664282 -0.2550448   0.076276    0.1475997   0.17835519 -0.04191424\n",
      "   0.21739548  0.06704348  0.0099231...643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0.354020...  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]], device=cpu())\n",
      "other = NDArray([[ 0.06664282 -0.2550448   0.076276    0.1475997   0.17835519 -0.04191424\n",
      "   0.21739548  0.06704348  0.0099231...643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58081130>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580813f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580819f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.06664282 -0.2550448   0.076276    0.1475997   0.17835519 -0.04191424\n",
      "   0.21739548  0.06704348  0.0099231...643 -0.22130679  0.21855503\n",
      "   0.24457175 -0.2295812  -0.04375178  0.04631761  0.01371938  0.02197182]], device=cuda())\n",
      "out        = NDArray([[2.0456658e-33 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.35880363 0.26810935 0.22586012 0.22969125 0.29623878 0.9138155\n",
      "  0.7308294  0.30786046 0.38129938 0.354020...  0.5501019  0.846376   0.8786662  0.24909547 0.34687415\n",
      "  0.00534553 0.75581133 0.8804441  0.69925934]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.55254984  0.5233728  -0.4050596   0.99803984  1.3077797\n",
      "    2.2168832  -0.08995695 -0.6975696   0....1   0.7911274  -0.07474909\n",
      "   -1.9750885  -0.2135864   1.8193091  -0.23330441  0.3449521\n",
      "    0.6077561  -2.1469498 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "    5.0383426e-02  2.6471749e-01  1.99090...240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "    1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]])\n",
      "h0         = needle.Tensor([[[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "    5.0383426e-02  2.6471749e-01  1.99090...240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "    1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd578dfbd0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....40e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "    1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dfbd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "    5.0383426e-02  2.6471749e-01  1.99090...240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "    1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dfbd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.07034062 0.32083088 0.42818937 ... 0.9195694  0.5474804  0.8917765 ]\n",
      "  [0.07034062 0.32083088 0.428...8937 ... 0.9195694  0.5474804  0.8917765 ]\n",
      "  [0.07034062 0.32083088 0.42818937 ... 0.9195694  0.5474804  0.8917765 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.07034062 0.32083088 0.42818937 ... 0.9195694  0.5474804  0.8917765 ]\n",
      "  [0.07034062 0.32083088 0.42...40e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "    1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578dd210>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.07034062 0.32083088 0.42818937 ... 0.9195694  0.5474804  0.8917765 ]\n",
      "  [0.07034062 0.32083088 0.428...8937 ... 0.9195694  0.5474804  0.8917765 ]\n",
      "  [0.07034062 0.32083088 0.42818937 ... 0.9195694  0.5474804  0.8917765 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "   5.0383426e-02  2.6471749e-01  1.9909035...19240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "   1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]])\n",
      "        h0         = (needle.Tensor([[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "   5.0383426e-02  2.6471749e-01  1.990903...240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "   1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0....0162\n",
      "  0.7995879  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]])\n",
      "        inputs     = [needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0... 0.7995879  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dea10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578dd210>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0...9240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "   1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dea10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0....0162\n",
      "  0.7995879  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.01369879 -0.16811293  0.18070123 -0.1040685   0.26531297  0.02575231\n",
      "   0.14406013 -0.02394205 -0.0...070123 -0.1040685   0.26531297  0.02575231\n",
      "   0.14406013 -0.02394205 -0.06233384 -0.04079039  0.23867989  0.11998668]])\n",
      "        bias_ih    = needle.Tensor([[ 0.27925307  0.18086267  0.16820782 -0.23595358 -0.1390117  -0.04193301\n",
      "  -0.21971056 -0.20652732  0.2...820782 -0.23595358 -0.1390117  -0.04193301\n",
      "  -0.21971056 -0.20652732  0.21861768  0.1763142  -0.26089072  0.2055122 ]])\n",
      "        h          = needle.Tensor([[-1.5540051e-01 -6.3494956e-01 -5.8578813e-01 -3.0347642e-01\n",
      "   5.0383426e-02  2.6471749e-01  1.9909035...19240e-01  1.3645090e+00  4.5972067e-01  2.6511585e-02\n",
      "   1.0446699e+00 -2.1287310e+00 -2.0820814e-01  2.9968485e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dea10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.14219388  0.10322851 -0.2707133   0.16797024  0.10846433 -0.12451114\n",
      "   0.2163102   0.03733569  0.0...875462  0.2379952  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]])\n",
      "        self       = needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0....0162\n",
      "  0.7995879  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0...75462  0.2379952  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576d9810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0...75462  0.2379952  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576d9810>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd55a45270>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd576dab10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd55a473f0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd576dab10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0.671210...79  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]], device=cpu())\n",
      "        b          = NDArray([[-0.14219388  0.10322851 -0.2707133   0.16797024  0.10846433 -0.12451114\n",
      "   0.2163102   0.03733569  0.0962175...52  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576d9810>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0.671210...79  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]], device=cpu())\n",
      "        b          = NDArray([[-0.14219388  0.10322851 -0.2707133   0.16797024  0.10846433 -0.12451114\n",
      "   0.2163102   0.03733569  0.0962175...52  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0.671210...79  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]], device=cpu())\n",
      "other = NDArray([[-0.14219388  0.10322851 -0.2707133   0.16797024  0.10846433 -0.12451114\n",
      "   0.2163102   0.03733569  0.0962175...52  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd578de570>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd578de9b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585059f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.14219388  0.10322851 -0.2707133   0.16797024  0.10846433 -0.12451114\n",
      "   0.2163102   0.03733569  0.0962175...52  -0.19375734 -0.23695128\n",
      "  -0.1281561   0.19266164  0.11067945 -0.1581408   0.25507313  0.18129152]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 4.1999680e+14 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.07034062 0.32083088 0.42818937 0.1978291  0.25121748 0.6244534\n",
      "  0.7911615  0.81980675 0.28473693 0.671210...79  0.77949697 0.30899578 0.01474408 0.27257302 0.627663\n",
      "  0.805702   0.9195694  0.5474804  0.8917765 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.5830957  -0.6611049   0.6755933   0.20439366  0.7847199\n",
      "    0.02559225  1.3345976   3.2667668  -1....  -1.0701238  -0.25921437\n",
      "    0.25636792 -0.49940747 -0.2520237  -0.33227286 -0.52468985\n",
      "   -1.3527961  -1.4533347 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-7.16661036e-01 -5.61465740e-01  7.73448110e-01 -2.48417780e-01\n",
      "   -1.81878567e-01 -8.82629395e-01 -2... -5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "   -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]])\n",
      "h0         = needle.Tensor([[[-7.16661036e-01 -5.61465740e-01  7.73448110e-01 -2.48417780e-01\n",
      "   -1.81878567e-01 -8.82629395e-01 -2... -5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "   -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56b90390>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[[-7.16661036e-01 -5.61465740e-01  7....-5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "   -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56b90390>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-7.16661036e-01 -5.61465740e-01  7.73448110e-01 -2.48417780e-01\n",
      "   -1.81878567e-01 -8.82629395e-01 -2... -5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "   -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56b90390>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "   0.9250008  0.69071645 0.7780601  ...   0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "   0.50355196 0.1798371  0.7702547  0.19270636]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "   0.9250008  0.69071645 0.7780601 ...-5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "   -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b908d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "   0.9250008  0.69071645 0.7780601  ...   0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "   0.50355196 0.1798371  0.7702547  0.19270636]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-7.1666104e-01 -5.6146574e-01  7.7344811e-01 -2.4841778e-01\n",
      "  -1.8187857e-01 -8.8262939e-01 -2.5638014...18014e-01  1.6753711e-02 -5.7820362e-01 -9.5612633e-01\n",
      "  -9.5183796e-01  3.6910239e-01  8.3905303e-01  3.7808681e-01]])\n",
      "        h0         = (needle.Tensor([[-7.1666104e-01 -5.6146574e-01  7.7344811e-01 -2.4841778e-01\n",
      "  -1.8187857e-01 -8.8262939e-01 -2.563801...1 -5.51431417e-01  2.26664543e+00 -5.33770382e-01\n",
      "  -1.62272418e+00 -9.44152892e-01 -5.06852090e-01  5.52482545e-01]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0....48\n",
      "  0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]])\n",
      "        inputs     = [needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0...8\n",
      "  0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b93a90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b908d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0...8014e-01  1.6753711e-02 -5.7820362e-01 -9.5612633e-01\n",
      "  -9.5183796e-01  3.6910239e-01  8.3905303e-01  3.7808681e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b93a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0....48\n",
      "  0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.12227803 -0.28325686  0.11163935 -0.19007114  0.01094824 -0.12332585\n",
      "   0.18603277  0.1680088  -0.2...163935 -0.19007114  0.01094824 -0.12332585\n",
      "   0.18603277  0.1680088  -0.21157265 -0.13496032 -0.13674021  0.27000618]])\n",
      "        bias_ih    = needle.Tensor([[-0.17682892 -0.15391278  0.13992485  0.08300832  0.14224836  0.27971053\n",
      "   0.22191274  0.06827381  0.1...992485  0.08300832  0.14224836  0.27971053\n",
      "   0.22191274  0.06827381  0.17064515  0.24282724 -0.28635484 -0.02559441]])\n",
      "        h          = needle.Tensor([[-7.1666104e-01 -5.6146574e-01  7.7344811e-01 -2.4841778e-01\n",
      "  -1.8187857e-01 -8.8262939e-01 -2.5638014...18014e-01  1.6753711e-02 -5.7820362e-01 -9.5612633e-01\n",
      "  -9.5183796e-01  3.6910239e-01  8.3905303e-01  3.7808681e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b93a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.13678339  0.24123186  0.09837541  0.18248424 -0.1450828   0.2680652\n",
      "   0.09324694 -0.16936487 -0.21...435572 -0.0542616   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]])\n",
      "        self       = needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0....48\n",
      "  0.91563946 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0...35572 -0.0542616   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0...35572 -0.0542616   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668690>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd58502d70>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5866a010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd585024b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5866a010>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0.091645...6 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]], device=cpu())\n",
      "        b          = NDArray([[-0.13678339  0.24123186  0.09837541  0.18248424 -0.1450828   0.2680652\n",
      "   0.09324694 -0.16936487 -0.21596508...16   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668690>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0.091645...6 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]], device=cpu())\n",
      "        b          = NDArray([[-0.13678339  0.24123186  0.09837541  0.18248424 -0.1450828   0.2680652\n",
      "   0.09324694 -0.16936487 -0.21596508...16   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0.091645...6 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]], device=cpu())\n",
      "other = NDArray([[-0.13678339  0.24123186  0.09837541  0.18248424 -0.1450828   0.2680652\n",
      "   0.09324694 -0.16936487 -0.21596508...16   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b938b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56b92570>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd586694b0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.13678339  0.24123186  0.09837541  0.18248424 -0.1450828   0.2680652\n",
      "   0.09324694 -0.16936487 -0.21596508...16   0.04465765 -0.15932573\n",
      "  -0.07387854  0.16151157 -0.05297877  0.12045851 -0.12141265  0.04425243]], device=cuda())\n",
      "out        = NDArray([[8.83280231e-35 0.00000000e+00 2.32063891e-34 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0...000000e+00 1.20371762e-38 0.00000000e+00\n",
      "  1.24666238e-38 0.00000000e+00 1.20371762e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.53188205 0.83270556 0.6766659  0.55615956 0.35674295 0.6849894\n",
      "  0.9250008  0.69071645 0.7780601  0.091645...6 0.3023605  0.39589876 0.45400247 0.2842776  0.72679853\n",
      "  0.50355196 0.1798371  0.7702547  0.19270636]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.48034620e+00 -4.13851410e-01 -4.11460936e-01 -1.15806496e+00\n",
      "   -1.10227752e+00  2.94116959e-02 -2... -2.08884418e-01 -3.19594324e-01 -6.80702507e-01\n",
      "    2.02350393e-01  7.56171420e-02 -6.35739625e-01 -9.95795369e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391\n",
      "    0.5027986   0.0283315   0.9816221  -0....07  0.3536418   0.5691848\n",
      "    0.20380752 -1.1677669  -0.48416343  1.0277611  -0.08584145\n",
      "    0.10190717 -0.00453746]]])\n",
      "h0         = needle.Tensor([[[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391\n",
      "    0.5027986   0.0283315   0.9816221  -0....07  0.3536418   0.5691848\n",
      "    0.20380752 -1.1677669  -0.48416343  1.0277611  -0.08584145\n",
      "    0.10190717 -0.00453746]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5855ba50>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....7  0.3536418   0.5691848\n",
      "    0.20380752 -1.1677669  -0.48416343  1.0277611  -0.08584145\n",
      "    0.10190717 -0.00453746]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5855ba50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391\n",
      "    0.5027986   0.0283315   0.9816221  -0....07  0.3536418   0.5691848\n",
      "    0.20380752 -1.1677669  -0.48416343  1.0277611  -0.08584145\n",
      "    0.10190717 -0.00453746]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd5855ba50>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.15987334 0.4285059  0.5229447  ... 0.36194453 0.0509349  0.6219123 ]\n",
      "  [0.15987334 0.4285059  0.522...447  ... 0.36194453 0.0509349  0.6219123 ]\n",
      "  [0.15987334 0.4285059  0.5229447  ... 0.36194453 0.0509349  0.6219123 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.15987334 0.4285059  0.5229447  ... 0.36194453 0.0509349  0.6219123 ]\n",
      "  [0.15987334 0.4285059  0.52...7  0.3536418   0.5691848\n",
      "    0.20380752 -1.1677669  -0.48416343  1.0277611  -0.08584145\n",
      "    0.10190717 -0.00453746]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5855a550>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.15987334 0.4285059  0.5229447  ... 0.36194453 0.0509349  0.6219123 ]\n",
      "  [0.15987334 0.4285059  0.522...447  ... 0.36194453 0.0509349  0.6219123 ]\n",
      "  [0.15987334 0.4285059  0.5229447  ... 0.36194453 0.0509349  0.6219123 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391   0.5027986\n",
      "   0.0283315   0.9816221  -0.15...09417  -0.18126938 -0.43582317  0.11858623\n",
      "  -0.4020102   1.0078624   0.0580977  -1.5280368  -0.49061152  0.74248874]])\n",
      "        h0         = (needle.Tensor([[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391   0.5027986\n",
      "   0.0283315   0.9816221  -0.1...15907  0.3536418   0.5691848   0.20380752\n",
      "  -1.1677669  -0.48416343  1.0277611  -0.08584145  0.10190717 -0.00453746]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0...54\n",
      "  0.66468984 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]])\n",
      "        inputs     = [needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  ....66468984 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5855a710>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5855a550>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  ...9417  -0.18126938 -0.43582317  0.11858623\n",
      "  -0.4020102   1.0078624   0.0580977  -1.5280368  -0.49061152  0.74248874]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5855a710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0...54\n",
      "  0.66468984 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.1108616   0.24697858  0.13880086  0.11233941 -0.08485599  0.2246955\n",
      "   0.23105383 -0.07783489  0.08...3880086  0.11233941 -0.08485599  0.2246955\n",
      "   0.23105383 -0.07783489  0.08847392 -0.20154326 -0.25638703  0.0090929 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.07521304  0.04972348  0.23389369 -0.21894327  0.1095975  -0.08394668\n",
      "  -0.14085266  0.14974698  0.0...389369 -0.21894327  0.1095975  -0.08394668\n",
      "  -0.14085266  0.14974698  0.07139033  0.2481044   0.1824516  -0.17682663]])\n",
      "        h          = needle.Tensor([[ 0.1080828   1.2808703  -0.09525321  0.19274774  1.3478391   0.5027986\n",
      "   0.0283315   0.9816221  -0.15...09417  -0.18126938 -0.43582317  0.11858623\n",
      "  -0.4020102   1.0078624   0.0580977  -1.5280368  -0.49061152  0.74248874]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5855a710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.1284684  -0.26412514  0.24478292 -0.00937107 -0.00180513  0.05268446\n",
      "   0.18154973  0.09322324  0.0...626845 -0.1278909  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]])\n",
      "        self       = needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0...54\n",
      "  0.66468984 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  ...26845 -0.1278909  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edbad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  ...26845 -0.1278909  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edbad0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd58375eb0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd57ed8d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd583754b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd57ed8d50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0.78836...4 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]], device=cpu())\n",
      "        b          = NDArray([[-0.1284684  -0.26412514  0.24478292 -0.00937107 -0.00180513  0.05268446\n",
      "   0.18154973  0.09322324  0.0154343...09  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edbad0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0.78836...4 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]], device=cpu())\n",
      "        b          = NDArray([[-0.1284684  -0.26412514  0.24478292 -0.00937107 -0.00180513  0.05268446\n",
      "   0.18154973  0.09322324  0.0154343...09  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0.78836...4 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]], device=cpu())\n",
      "other = NDArray([[-0.1284684  -0.26412514  0.24478292 -0.00937107 -0.00180513  0.05268446\n",
      "   0.18154973  0.09322324  0.0154343...09  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58559bf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5855aef0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ed8d70>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.1284684  -0.26412514  0.24478292 -0.00937107 -0.00180513  0.05268446\n",
      "   0.18154973  0.09322324  0.0154343...09  -0.16469464 -0.22302775\n",
      "   0.05681637 -0.2210415   0.12054962 -0.27667788 -0.27585593 -0.08067182]], device=cuda())\n",
      "out        = NDArray([[9.0355567e-35 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.15987334 0.4285059  0.5229447  0.96532404 0.4281719  0.38313785\n",
      "  0.64997625 0.41682187 0.7338232  0.78836...4 0.78834933 0.05984578 0.87192863 0.8166322  0.96602273\n",
      "  0.66429293 0.36194453 0.0509349  0.6219123 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[1.379104]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[0.43295085]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd585a3450>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585a3450>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd585a3450>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.02370715]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.02370715]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585a3990>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.02370715]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.02370715]])\n",
      "        inputs     = [needle.Tensor([[0.02370715]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a2910>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585a3990>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.02370715]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a2910>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.02370715]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.17811918]])\n",
      "        bias_ih    = needle.Tensor([[-0.23375034]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a2910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.2571988]])\n",
      "        self       = needle.Tensor([[0.02370715]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.02370715]]), needle.Tensor([[0.2571988]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585a27d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.02370715]]), needle.Tensor([[0.2571988]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585a27d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57ddac30>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd585a2ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57ddae30>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd585a2ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.02370715]], device=cpu())\n",
      "        b          = NDArray([[0.2571988]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585a27d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.02370715]], device=cpu())\n",
      "        b          = NDArray([[0.2571988]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.02370715]], device=cpu())\n",
      "other = NDArray([[0.2571988]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585a3bf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585a3d30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585a0570>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.2571988]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.02370715]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.5197194]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[0.42171535]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5830ca90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5830ca90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5830ca90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5830fc90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]\n",
      "\n",
      " [[0.8282277]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8282277]])\n",
      "        inputs     = [needle.Tensor([[0.8282277]]), needle.Tensor([[0.8282277]]), needle.Tensor([[0.8282277]]), needle.Tensor([[0.8282277]]), needle.Tensor([[0.8282277]]), needle.Tensor([[0.8282277]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5830e0d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5830fc90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8282277]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5830e0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8282277]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.44618577]])\n",
      "        bias_ih    = needle.Tensor([[-0.84494483]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5830e0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.7171582]])\n",
      "        self       = needle.Tensor([[0.8282277]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8282277]]), needle.Tensor([[0.7171582]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58920750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8282277]]), needle.Tensor([[0.7171582]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58920750>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd584daab0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58923bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd584d8430>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58923bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8282277]], device=cpu())\n",
      "        b          = NDArray([[0.7171582]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58920750>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8282277]], device=cpu())\n",
      "        b          = NDArray([[0.7171582]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8282277]], device=cpu())\n",
      "other = NDArray([[0.7171582]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5830cbf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5830ed30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58922b30>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.7171582]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8282277]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.05663809]]\n",
      "\n",
      " [[-0.07429428]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.88844866]]\n",
      "\n",
      " [[ 0.46090484]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd580b4550>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b4550>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b4550>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.74324805]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.74324805]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b46d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.74324805]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.74324805]])\n",
      "        inputs     = [needle.Tensor([[0.74324805]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4f10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b46d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.74324805]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4f10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.74324805]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.30684352]])\n",
      "        bias_ih    = needle.Tensor([[-0.96685934]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.8541316]])\n",
      "        self       = needle.Tensor([[0.74324805]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.74324805]]), needle.Tensor([[0.8541316]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580b6610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.74324805]]), needle.Tensor([[0.8541316]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580b6610>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd56410770>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd580b67d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd56412570>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd580b67d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.74324805]], device=cpu())\n",
      "        b          = NDArray([[0.8541316]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580b6610>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.74324805]], device=cpu())\n",
      "        b          = NDArray([[0.8541316]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.74324805]], device=cpu())\n",
      "other = NDArray([[0.8541316]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580b64f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580b6e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580b7af0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.8541316]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.74324805]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.13495569]]\n",
      "\n",
      " [[ 0.95578456]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-1.686656  ]]\n",
      "\n",
      " [[-0.15562958]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5735d550>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5735d550>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5735d550>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " ...876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5735e710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]\n",
      "\n",
      " [[0.17876157]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.17876157]])\n",
      "        inputs     = [needle.Tensor([[0.17876157]]), needle.Tensor([[0.17876157]]), needle.Tensor([[0.17876157]]), needle.Tensor([[0.17876157]]), needle.Tensor([[0.17876157]]), needle.Tensor([[0.17876157]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5735d3d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5735e710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.17876157]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5735d3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.17876157]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.9042695]])\n",
      "        bias_ih    = needle.Tensor([[0.8443897]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5735d3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.70521545]])\n",
      "        self       = needle.Tensor([[0.17876157]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.17876157]]), needle.Tensor([[-0.70521545]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.17876157]]), needle.Tensor([[-0.70521545]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3c90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd572b34b0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd584c19d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd572b09f0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd584c19d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.17876157]], device=cpu())\n",
      "        b          = NDArray([[-0.70521545]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3c90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.17876157]], device=cpu())\n",
      "        b          = NDArray([[-0.70521545]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.17876157]], device=cpu())\n",
      "other = NDArray([[-0.70521545]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5735dfb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5735f8b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c3af0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.70521545]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.17876157]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.42567787]\n",
      "  [ 0.84833556]\n",
      "  [-0.22024822]\n",
      "  [ 0.24896564]\n",
      "  [-0.02289018]\n",
      "  [ 1.9679941 ]\n",
      "  [-0.17...63 ]\n",
      "  [ 0.69226426]\n",
      "  [ 0.3771038 ]\n",
      "  [ 1.0266871 ]\n",
      "  [ 0.47549325]\n",
      "  [ 1.7188913 ]\n",
      "  [-0.451869  ]\n",
      "  [ 1.0383848 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.2547251 ]\n",
      "  [ 0.09576435]\n",
      "  [ 0.8201193 ]\n",
      "  [ 2.438934  ]\n",
      "  [ 0.67370754]\n",
      "  [-2.3088415 ]\n",
      "  [ 1.09...08 ]\n",
      "  [-0.24051526]\n",
      "  [ 0.15346928]\n",
      "  [-0.0182328 ]\n",
      "  [-0.89159995]\n",
      "  [-1.103372  ]\n",
      "  [ 0.73332024]\n",
      "  [ 0.89714503]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58668410>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58668410>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58668410>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]...0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804...804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5866a8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]...0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]\n",
      "  [0.88860804]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]])\n",
      "        inputs     = [needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd586689d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5866a8d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.8...88860804]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd586689d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]\n",
      " [0.43243194]])\n",
      "        bias_ih    = needle.Tensor([[0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]\n",
      " [0.58834875]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd586689d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.3463714]])\n",
      "        self       = needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.8...4]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]]), needle.Tensor([[0.3463714]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5866b050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.8...4]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]]), needle.Tensor([[0.3463714]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5866b050>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd580f0db0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58669350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd580f2c70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58669350>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]], device=cpu())\n",
      "        b          = NDArray([[0.3463714]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5866b050>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]], device=cpu())\n",
      "        b          = NDArray([[0.3463714]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]], device=cpu())\n",
      "other = NDArray([[0.3463714]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5866aeb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5866a870>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5866a4b0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.3463714]], device=cuda())\n",
      "out        = NDArray([[9.1213804e-35]\n",
      " [0.0000000e+00]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799...]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]\n",
      " [4.9799255e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]\n",
      " [0.88860804]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.67023736]\n",
      "  [ 0.02981238]\n",
      "  [ 0.22589806]\n",
      "  [ 1.6291778 ]\n",
      "  [-2.7689745 ]\n",
      "  [-0.7944007 ]\n",
      "  [ 1.08...2  ]\n",
      "  [ 1.0144441 ]\n",
      "  [ 2.0932536 ]\n",
      "  [ 1.146921  ]\n",
      "  [ 0.87349993]\n",
      "  [ 0.8716831 ]\n",
      "  [-1.0050793 ]\n",
      "  [-1.3532224 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 9.0013683e-01]\n",
      "  [ 6.4351612e-01]\n",
      "  [ 1.1077926e+00]\n",
      "  [-2.9515246e-01]\n",
      "  [-4.5776588e-01]\n",
      "  [ 1.769...1]\n",
      "  [-1.0635532e+00]\n",
      "  [ 1.1376607e+00]\n",
      "  [-7.8706242e-02]\n",
      "  [ 2.7751393e-04]\n",
      "  [-1.4086198e+00]\n",
      "  [-1.1296664e+00]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5833d210>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5833d210>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5833d210>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]...0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544...544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5833c710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]...0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]\n",
      "  [0.23985544]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]])\n",
      "        inputs     = [needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.2... [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833d490>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5833c710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.2...23985544]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833d490>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]\n",
      " [0.8862723]])\n",
      "        bias_ih    = needle.Tensor([[-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]\n",
      " [-0.32377154]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833d490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.1774069]])\n",
      "        self       = needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.2...4]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]]), needle.Tensor([[0.1774069]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.2...4]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]]), needle.Tensor([[0.1774069]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7e10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57f4fbf0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57ab7cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57f4de70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57ab7cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]], device=cpu())\n",
      "        b          = NDArray([[0.1774069]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7e10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]], device=cpu())\n",
      "        b          = NDArray([[0.1774069]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]], device=cpu())\n",
      "other = NDArray([[0.1774069]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5833d430>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5833ebf0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ab61b0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.1774069]], device=cuda())\n",
      "out        = NDArray([[7.4486494e-34]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000...]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]\n",
      " [0.23985544]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.27229238]\n",
      "  [-0.93124145]\n",
      "  [-0.28360516]\n",
      "  [ 0.20831743]\n",
      "  [-1.3060758 ]\n",
      "  [ 0.4117608 ]\n",
      "  [-0.28...33 ]\n",
      "  [-1.1400472 ]\n",
      "  [ 1.0002046 ]\n",
      "  [-0.39295718]\n",
      "  [ 1.3804692 ]\n",
      "  [ 1.106856  ]\n",
      "  [-1.188505  ]\n",
      "  [ 0.64570653]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.9148897 ]\n",
      "  [-0.29454616]\n",
      "  [ 1.1418997 ]\n",
      "  [ 2.5392375 ]\n",
      "  [ 0.54529786]\n",
      "  [ 0.20933539]\n",
      "  [ 0.89...86 ]\n",
      "  [-1.0558138 ]\n",
      "  [-1.0065007 ]\n",
      "  [ 0.40434042]\n",
      "  [-0.8659364 ]\n",
      "  [ 0.34794754]\n",
      "  [-1.3135542 ]\n",
      "  [ 1.1783546 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56f2f350>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56f2f350>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56f2f350>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]...0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114...114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56f2ca50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]...0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]\n",
      "  [0.44317114]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]])\n",
      "        inputs     = [needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2d1d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56f2ca50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.4...44317114]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2d1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]\n",
      " [0.31548762]])\n",
      "        bias_ih    = needle.Tensor([[-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]\n",
      " [-0.34737396]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56f2d1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.84043145]])\n",
      "        self       = needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.4...\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]]), needle.Tensor([[-0.84043145]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f2f410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.4...\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]]), needle.Tensor([[-0.84043145]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f2f410>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585a28b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd56f2ded0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585a2430>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd56f2ded0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]], device=cpu())\n",
      "        b          = NDArray([[-0.84043145]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f2f410>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]], device=cpu())\n",
      "        b          = NDArray([[-0.84043145]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]], device=cpu())\n",
      "other = NDArray([[-0.84043145]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56f2f7f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56f2eb30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58012330>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.84043145]], device=cuda())\n",
      "out        = NDArray([[3.5257803e-34]\n",
      " [0.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000...]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]\n",
      " [0.44317114]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.8520729 ]\n",
      "  [ 0.70629513]\n",
      "  [-0.42606845]\n",
      "  [-1.2488853 ]\n",
      "  [-0.5272731 ]\n",
      "  [-1.445876  ]\n",
      "  [ 0.42...024]\n",
      "  [ 0.77037394]\n",
      "  [-0.5269266 ]\n",
      "  [-0.53582937]\n",
      "  [ 0.2805709 ]\n",
      "  [-1.2991362 ]\n",
      "  [ 0.68530786]\n",
      "  [-0.00919892]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.9226815 ]\n",
      "  [-0.35537603]\n",
      "  [-0.8302709 ]\n",
      "  [ 1.3252664 ]\n",
      "  [ 0.78384495]\n",
      "  [-0.7824229 ]\n",
      "  [ 0.63...753]\n",
      "  [-0.342754  ]\n",
      "  [ 0.263305  ]\n",
      "  [-0.9200006 ]\n",
      "  [ 0.93380207]\n",
      "  [-1.9751158 ]\n",
      "  [-1.7605407 ]\n",
      "  [ 1.1929549 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56304390>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56304390>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56304390>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "...  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]...1244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd563071d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "...  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]\n",
      "  [0.651244]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]])\n",
      "        inputs     = [needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651...44]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56305b50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd563071d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651...0.651244]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56305b50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]\n",
      " [0.12618887]])\n",
      "        bias_ih    = needle.Tensor([[0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]\n",
      " [0.867574]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56305b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.35412836]])\n",
      "        self       = needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]]), needle.Tensor([[0.35412836]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e44ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]]), needle.Tensor([[0.35412836]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e44ed0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f128b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57e44690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f10770>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57e44690>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]], device=cpu())\n",
      "        b          = NDArray([[0.35412836]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e44ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]], device=cpu())\n",
      "        b          = NDArray([[0.35412836]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]], device=cpu())\n",
      "other = NDArray([[0.35412836]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56304c70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56307e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57e454b0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.35412836]], device=cuda())\n",
      "out        = NDArray([[2.1564653e-33]\n",
      " [0.0000000e+00]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100...]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]\n",
      " [1.5100612e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]\n",
      " [0.651244]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.00926839]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.05372703]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58503010>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58503010>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58503010>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "   0.29849997 0.55472875 0.9012379  0...   0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "   0.11021991 0.15288416 0.24180156 0.38017473]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "   0.29849997 0.55472875 0.9012379  ...27371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "   0.11021991 0.15288416 0.24180156 0.38017473]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58502250>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "   0.29849997 0.55472875 0.9012379  0...   0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "   0.11021991 0.15288416 0.24180156 0.38017473]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8...64\n",
      "  0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]])\n",
      "        inputs     = [needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0....4\n",
      "  0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585039d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58502250>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0....5  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585039d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8...64\n",
      "  0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.61944807]])\n",
      "        bias_ih    = needle.Tensor([[0.78135693]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585039d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.94973916]\n",
      " [-0.937229  ]\n",
      " [ 0.472288  ]\n",
      " [-0.9318174 ]\n",
      " [-0.9521243 ]\n",
      " [ 0.88266146]\n",
      " [-0.9577957 ]... 0.03665316]\n",
      " [ 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]])\n",
      "        self       = needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8...64\n",
      "  0.3527371  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0....0.03665316]\n",
      " [ 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585014d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0....0.03665316]\n",
      " [ 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585014d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5806dbf0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58502e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56b918f0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58502e90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8328314...  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]], device=cpu())\n",
      "        b          = NDArray([[-0.94973916]\n",
      " [-0.937229  ]\n",
      " [ 0.472288  ]\n",
      " [-0.9318174 ]\n",
      " [-0.9521243 ]\n",
      " [ 0.88266146]\n",
      " [-0.9577957 ]\n",
      " [ 0.... 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585014d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8328314...  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]], device=cpu())\n",
      "        b          = NDArray([[-0.94973916]\n",
      " [-0.937229  ]\n",
      " [ 0.472288  ]\n",
      " [-0.9318174 ]\n",
      " [-0.9521243 ]\n",
      " [ 0.88266146]\n",
      " [-0.9577957 ]\n",
      " [ 0.... 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8328314...  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]], device=cpu())\n",
      "other = NDArray([[-0.94973916]\n",
      " [-0.937229  ]\n",
      " [ 0.472288  ]\n",
      " [-0.9318174 ]\n",
      " [-0.9521243 ]\n",
      " [ 0.88266146]\n",
      " [-0.9577957 ]\n",
      " [ 0.... 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585027b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58500630>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58503ef0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.94973916]\n",
      " [-0.937229  ]\n",
      " [ 0.472288  ]\n",
      " [-0.9318174 ]\n",
      " [-0.9521243 ]\n",
      " [ 0.88266146]\n",
      " [-0.9577957 ]\n",
      " [ 0.... 0.03808784]\n",
      " [-0.5365541 ]\n",
      " [ 0.2781527 ]\n",
      " [ 0.09484553]\n",
      " [-0.38132924]\n",
      " [ 0.34964228]\n",
      " [-0.15402138]], device=cuda())\n",
      "out        = NDArray([[2.2035902e-33]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.58814794 0.30614504 0.39651564 0.9490691  0.5141257  0.274107\n",
      "  0.29849997 0.55472875 0.9012379  0.8328314...  0.5623905  0.71004707 0.74897355 0.18924157 0.54464227\n",
      "  0.11021991 0.15288416 0.24180156 0.38017473]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[1.7535622]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.3412936]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58114650>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58114650>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58114650>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "   0.00108453 0.9844572  0.9098228 ...   0.2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "   0.52513134 0.29266387 0.15574603 0.5562786 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "   0.00108453 0.9844572  0.9098228...18438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "   0.52513134 0.29266387 0.15574603 0.5562786 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58114550>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "   0.00108453 0.9844572  0.9098228 ...   0.2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "   0.52513134 0.29266387 0.15574603 0.5562786 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0...43\n",
      "  0.2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]])\n",
      "        inputs     = [needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  ....2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58114e10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58114550>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  ...25 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58114e10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0...43\n",
      "  0.2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.98331]])\n",
      "        bias_ih    = needle.Tensor([[0.33933842]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58114e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.8236711 ]\n",
      " [ 0.5979314 ]\n",
      " [-0.37792766]\n",
      " [ 0.13833272]\n",
      " [-0.20783573]\n",
      " [ 0.9992019 ]\n",
      " [ 0.3681786 ]...-0.95605814]\n",
      " [-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]])\n",
      "        self       = needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0...43\n",
      "  0.2918438  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  ...0.95605814]\n",
      " [-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c0450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  ...0.95605814]\n",
      " [-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c0450>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d5eb0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd584c0ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d6c30>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd584c0ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0.32549...  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.8236711 ]\n",
      " [ 0.5979314 ]\n",
      " [-0.37792766]\n",
      " [ 0.13833272]\n",
      " [-0.20783573]\n",
      " [ 0.9992019 ]\n",
      " [ 0.3681786 ]\n",
      " [-0....-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c0450>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0.32549...  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.8236711 ]\n",
      " [ 0.5979314 ]\n",
      " [-0.37792766]\n",
      " [ 0.13833272]\n",
      " [-0.20783573]\n",
      " [ 0.9992019 ]\n",
      " [ 0.3681786 ]\n",
      " [-0....-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0.32549...  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]], device=cpu())\n",
      "other = NDArray([[ 0.8236711 ]\n",
      " [ 0.5979314 ]\n",
      " [-0.37792766]\n",
      " [ 0.13833272]\n",
      " [-0.20783573]\n",
      " [ 0.9992019 ]\n",
      " [ 0.3681786 ]\n",
      " [-0....-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58116f70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58116830>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c24f0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.8236711 ]\n",
      " [ 0.5979314 ]\n",
      " [-0.37792766]\n",
      " [ 0.13833272]\n",
      " [-0.20783573]\n",
      " [ 0.9992019 ]\n",
      " [ 0.3681786 ]\n",
      " [-0....-0.3786751 ]\n",
      " [-0.30482227]\n",
      " [-0.84537977]\n",
      " [ 0.6291379 ]\n",
      " [ 0.49297678]\n",
      " [ 0.07090795]\n",
      " [-0.9245052 ]], device=cuda())\n",
      "out        = NDArray([[5.799981e-34]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.22292781 0.67108595 0.8612172  0.9134578  0.44777134 0.23922509\n",
      "  0.00108453 0.9844572  0.9098228  0.32549...  0.57173425 0.03061181 0.763115   0.0583796  0.48428488\n",
      "  0.52513134 0.29266387 0.15574603 0.5562786 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.29846665]]\n",
      "\n",
      " [[ 0.23900263]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.8156109 ]]\n",
      "\n",
      " [[-0.69834167]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd587d6750>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd587d6750>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd587d6750>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "   0.13126563 0.17528139 0.6238178  0...   0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "   0.60212344 0.21988079 0.3561337  0.14000617]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "   0.13126563 0.17528139 0.6238178  ...95423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "   0.60212344 0.21988079 0.3561337  0.14000617]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd587d4850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "   0.13126563 0.17528139 0.6238178  0...   0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "   0.60212344 0.21988079 0.3561337  0.14000617]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2...49\n",
      "  0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]])\n",
      "        inputs     = [needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0....9\n",
      "  0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587d5a90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd587d4850>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0....6  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587d5a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2...49\n",
      "  0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.4433651]])\n",
      "        bias_ih    = needle.Tensor([[-0.26403522]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587d5a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.8132234 ]\n",
      " [-0.7175082 ]\n",
      " [ 0.10035443]\n",
      " [-0.9034336 ]\n",
      " [-0.24100202]\n",
      " [-0.88719225]\n",
      " [-0.4037097 ]... 0.72388256]\n",
      " [-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]])\n",
      "        self       = needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2...49\n",
      "  0.9395423  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0....0.72388256]\n",
      " [-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585026d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0....0.72388256]\n",
      " [-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585026d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578dc4f0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58503150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578de3f0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58503150>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2621532...  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]], device=cpu())\n",
      "        b          = NDArray([[-0.8132234 ]\n",
      " [-0.7175082 ]\n",
      " [ 0.10035443]\n",
      " [-0.9034336 ]\n",
      " [-0.24100202]\n",
      " [-0.88719225]\n",
      " [-0.4037097 ]\n",
      " [-0....-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585026d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2621532...  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]], device=cpu())\n",
      "        b          = NDArray([[-0.8132234 ]\n",
      " [-0.7175082 ]\n",
      " [ 0.10035443]\n",
      " [-0.9034336 ]\n",
      " [-0.24100202]\n",
      " [-0.88719225]\n",
      " [-0.4037097 ]\n",
      " [-0....-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2621532...  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]], device=cpu())\n",
      "other = NDArray([[-0.8132234 ]\n",
      " [-0.7175082 ]\n",
      " [ 0.10035443]\n",
      " [-0.9034336 ]\n",
      " [-0.24100202]\n",
      " [-0.88719225]\n",
      " [-0.4037097 ]\n",
      " [-0....-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587d53b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd587d42b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58502ab0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.8132234 ]\n",
      " [-0.7175082 ]\n",
      " [ 0.10035443]\n",
      " [-0.9034336 ]\n",
      " [-0.24100202]\n",
      " [-0.88719225]\n",
      " [-0.4037097 ]\n",
      " [-0....-0.48824233]\n",
      " [ 0.5805054 ]\n",
      " [-0.02151144]\n",
      " [-0.73866796]\n",
      " [ 0.5380405 ]\n",
      " [ 0.87139416]\n",
      " [-0.3679998 ]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.31345287 0.8656335  0.560548   0.74968654 0.78205746 0.271266\n",
      "  0.13126563 0.17528139 0.6238178  0.2621532...  0.6787366  0.8401066  0.7124535  0.3619834  0.27726662\n",
      "  0.60212344 0.21988079 0.3561337  0.14000617]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[0.02845582]]\n",
      "\n",
      " [[0.5045329 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.48414055]]\n",
      "\n",
      " [[-0.01255007]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd584b2c10>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b2c10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b2c10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "   0.6227759  0.6300682  0.43935245...   0.10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "   0.53637815 0.75125813 0.6345502  0.68115914]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "   0.6227759  0.6300682  0.4393524...661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "   0.53637815 0.75125813 0.6345502  0.68115914]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b0a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "   0.6227759  0.6300682  0.43935245...   0.10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "   0.53637815 0.75125813 0.6345502  0.68115914]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0...61\n",
      "  0.10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]])\n",
      "        inputs     = [needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 ....10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3710>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b0a90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 ...54 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0...61\n",
      "  0.10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.88730204]])\n",
      "        bias_ih    = needle.Tensor([[0.1110214]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.07380974]\n",
      " [ 0.1217798 ]\n",
      " [ 0.42367935]\n",
      " [ 0.25556827]\n",
      " [ 0.24223816]\n",
      " [ 0.41502059]\n",
      " [-0.93090576]...-0.21504223]\n",
      " [ 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]])\n",
      "        self       = needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0...61\n",
      "  0.10661846 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 ...0.21504223]\n",
      " [ 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6dbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 ...0.21504223]\n",
      " [ 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6dbd0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57cd2f70>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56d6c210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57cd0970>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56d6c210>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0.71140...6 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]], device=cpu())\n",
      "        b          = NDArray([[ 0.07380974]\n",
      " [ 0.1217798 ]\n",
      " [ 0.42367935]\n",
      " [ 0.25556827]\n",
      " [ 0.24223816]\n",
      " [ 0.41502059]\n",
      " [-0.93090576]\n",
      " [ 0.... 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56d6dbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0.71140...6 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]], device=cpu())\n",
      "        b          = NDArray([[ 0.07380974]\n",
      " [ 0.1217798 ]\n",
      " [ 0.42367935]\n",
      " [ 0.25556827]\n",
      " [ 0.24223816]\n",
      " [ 0.41502059]\n",
      " [-0.93090576]\n",
      " [ 0.... 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0.71140...6 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]], device=cpu())\n",
      "other = NDArray([[ 0.07380974]\n",
      " [ 0.1217798 ]\n",
      " [ 0.42367935]\n",
      " [ 0.25556827]\n",
      " [ 0.24223816]\n",
      " [ 0.41502059]\n",
      " [-0.93090576]\n",
      " [ 0.... 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b0df0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b28b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56d6e6f0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.07380974]\n",
      " [ 0.1217798 ]\n",
      " [ 0.42367935]\n",
      " [ 0.25556827]\n",
      " [ 0.24223816]\n",
      " [ 0.41502059]\n",
      " [-0.93090576]\n",
      " [ 0.... 0.00145793]\n",
      " [ 0.00422347]\n",
      " [ 0.26187527]\n",
      " [-0.7409927 ]\n",
      " [-0.47250247]\n",
      " [ 0.61988246]\n",
      " [ 0.7547264 ]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.02948165 0.41161364 0.7966624  0.0836091  0.79095817 0.21934497\n",
      "  0.6227759  0.6300682  0.43935245 0.71140...6 0.41985554 0.67965156 0.52501255 0.49457535 0.03050692\n",
      "  0.53637815 0.75125813 0.6345502  0.68115914]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.6020799e+00]\n",
      "  [ 2.6210803e-01]\n",
      "  [-6.0057878e-02]\n",
      "  [ 9.2466760e-01]\n",
      "  [ 1.6076957e+00]\n",
      "  [ 2.719...1]\n",
      "  [-1.6613182e-01]\n",
      "  [-1.9776160e-01]\n",
      "  [ 4.9596584e-01]\n",
      "  [-1.6511266e-01]\n",
      "  [-1.0608132e-05]\n",
      "  [ 1.3849849e+00]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.17875203]\n",
      "  [ 0.9836089 ]\n",
      "  [ 0.62139463]\n",
      "  [-2.3524985 ]\n",
      "  [-0.21128722]\n",
      "  [ 0.07239576]\n",
      "  [ 0.25...3  ]\n",
      "  [-1.2642568 ]\n",
      "  [ 0.5997684 ]\n",
      "  [ 0.3384024 ]\n",
      "  [ 0.25342825]\n",
      "  [-0.5479216 ]\n",
      "  [ 0.43067655]\n",
      "  [-0.97174454]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56c58e10>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56c58e10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56c58e10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "   0.4854229  0.38596976 0.59705293...\n",
      "   0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "   0.1602381  0.17665848 0.5266229  0.08911254]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "   0.4854229  0.38596976 0.5970529...142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "   0.1602381  0.17665848 0.5266229  0.08911254]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56c5b4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "   0.4854229  0.38596976 0.59705293...\n",
      "   0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "   0.1602381  0.17665848 0.5266229  0.08911254]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0...766\n",
      "  0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]])\n",
      "        inputs     = [needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 ...66\n",
      "  0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56c5bdd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56c5b4d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 ...08911254]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56c5bdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0...766\n",
      "  0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]\n",
      " [-0.7867151]])\n",
      "        bias_ih    = needle.Tensor([[0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]\n",
      " [0.26544523]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56c5bdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.41099727]\n",
      " [-0.655944  ]\n",
      " [-0.84848803]\n",
      " [ 0.85213065]\n",
      " [ 0.7344477 ]\n",
      " [ 0.96950984]\n",
      " [-0.46628803]...-0.5469141 ]\n",
      " [ 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]])\n",
      "        self       = needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0...766\n",
      "  0.8142036  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 ...0.5469141 ]\n",
      " [ 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 ...0.5469141 ]\n",
      " [ 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58110>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd566c7370>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56c5a910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd566c5370>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56c5a910>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0.49999...6  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]], device=cpu())\n",
      "        b          = NDArray([[-0.41099727]\n",
      " [-0.655944  ]\n",
      " [-0.84848803]\n",
      " [ 0.85213065]\n",
      " [ 0.7344477 ]\n",
      " [ 0.96950984]\n",
      " [-0.46628803]\n",
      " [ 0.... 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58110>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0.49999...6  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]], device=cpu())\n",
      "        b          = NDArray([[-0.41099727]\n",
      " [-0.655944  ]\n",
      " [-0.84848803]\n",
      " [ 0.85213065]\n",
      " [ 0.7344477 ]\n",
      " [ 0.96950984]\n",
      " [-0.46628803]\n",
      " [ 0.... 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0.49999...6  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]], device=cpu())\n",
      "other = NDArray([[-0.41099727]\n",
      " [-0.655944  ]\n",
      " [-0.84848803]\n",
      " [ 0.85213065]\n",
      " [ 0.7344477 ]\n",
      " [ 0.96950984]\n",
      " [-0.46628803]\n",
      " [ 0.... 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56c58270>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56c5a1f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56c5a230>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.41099727]\n",
      " [-0.655944  ]\n",
      " [-0.84848803]\n",
      " [ 0.85213065]\n",
      " [ 0.7344477 ]\n",
      " [ 0.96950984]\n",
      " [-0.46628803]\n",
      " [ 0.... 0.22759819]\n",
      " [ 0.5861114 ]\n",
      " [-0.24721074]\n",
      " [-0.3725646 ]\n",
      " [-0.08003563]\n",
      " [-0.07881838]\n",
      " [ 0.74551296]], device=cuda())\n",
      "out        = NDArray([[8.4053201e-35]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.7950043  0.18473397 0.7304968  0.39043212 0.8331955  0.21490163\n",
      "  0.4854229  0.38596976 0.59705293 0.49999...6  0.40840757 0.72760713 0.7380408  0.70138067 0.5300615\n",
      "  0.1602381  0.17665848 0.5266229  0.08911254]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.5363432 ]\n",
      "  [-0.6184526 ]\n",
      "  [ 0.06586046]\n",
      "  [-1.1822573 ]\n",
      "  [-1.5732782 ]\n",
      "  [-0.36026487]\n",
      "  [-2.34...69 ]\n",
      "  [-1.5856907 ]\n",
      "  [-0.17687477]\n",
      "  [ 1.2202578 ]\n",
      "  [ 0.88270235]\n",
      "  [-0.55901587]\n",
      "  [ 0.40385476]\n",
      "  [ 0.6042619 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.5136485 ]\n",
      "  [-0.30331105]\n",
      "  [-0.41283455]\n",
      "  [ 0.2142307 ]\n",
      "  [-0.8935382 ]\n",
      "  [ 0.6062218 ]\n",
      "  [-0.33...54 ]\n",
      "  [ 0.36704096]\n",
      "  [ 0.6270946 ]\n",
      "  [ 2.0803506 ]\n",
      "  [ 0.3573811 ]\n",
      "  [-1.5681677 ]\n",
      "  [-0.4965402 ]\n",
      "  [-0.98711646]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57ebc250>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ebc250>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ebc250>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.694...76   ... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.69.... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ebc210>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.694...76   ... 0.04127935 0.34402126 0.82318234]\n",
      "  [0.13526796 0.92154616 0.694476   ... 0.04127935 0.34402126 0.82318234]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0...711\n",
      "  0.19784677 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]])\n",
      "        inputs     = [needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 ...0.19784677 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ebf450>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ebc210>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 ...82318234]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ebf450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0...711\n",
      "  0.19784677 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]\n",
      " [-0.8544328]])\n",
      "        bias_ih    = needle.Tensor([[-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]\n",
      " [-0.8710009]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ebf450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.91862667]\n",
      " [-0.64970577]\n",
      " [ 0.7461667 ]\n",
      " [-0.23406953]\n",
      " [-0.44624287]\n",
      " [-0.5927171 ]\n",
      " [-0.1407299 ]...-0.03522754]\n",
      " [-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]])\n",
      "        self       = needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0...711\n",
      "  0.19784677 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 ...0.03522754]\n",
      " [-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576dbf90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 ...0.03522754]\n",
      " [-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576dbf90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56055b30>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd576d8a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56055870>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd576d8a50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0.87621...77 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]], device=cpu())\n",
      "        b          = NDArray([[-0.91862667]\n",
      " [-0.64970577]\n",
      " [ 0.7461667 ]\n",
      " [-0.23406953]\n",
      " [-0.44624287]\n",
      " [-0.5927171 ]\n",
      " [-0.1407299 ]\n",
      " [ 0....-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576dbf90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0.87621...77 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]], device=cpu())\n",
      "        b          = NDArray([[-0.91862667]\n",
      " [-0.64970577]\n",
      " [ 0.7461667 ]\n",
      " [-0.23406953]\n",
      " [-0.44624287]\n",
      " [-0.5927171 ]\n",
      " [-0.1407299 ]\n",
      " [ 0....-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0.87621...77 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]], device=cpu())\n",
      "other = NDArray([[-0.91862667]\n",
      " [-0.64970577]\n",
      " [ 0.7461667 ]\n",
      " [-0.23406953]\n",
      " [-0.44624287]\n",
      " [-0.5927171 ]\n",
      " [-0.1407299 ]\n",
      " [ 0....-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ebf5b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57ebe3f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd576d88b0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.91862667]\n",
      " [-0.64970577]\n",
      " [ 0.7461667 ]\n",
      " [-0.23406953]\n",
      " [-0.44624287]\n",
      " [-0.5927171 ]\n",
      " [-0.1407299 ]\n",
      " [ 0....-0.01463723]\n",
      " [-0.63446134]\n",
      " [-0.72876966]\n",
      " [ 0.86101115]\n",
      " [-0.77554506]\n",
      " [-0.04547966]\n",
      " [-0.38251382]], device=cuda())\n",
      "out        = NDArray([[6.2226699e-35]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.13526796 0.92154616 0.694476   0.02414769 0.12546688 0.53881526\n",
      "  0.5891669  0.6926949  0.91346824 0.87621...77 0.2391328  0.30215356 0.8872467  0.32599798 0.7315338\n",
      "  0.61924464 0.04127935 0.34402126 0.82318234]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.36436954]\n",
      "  [-1.1789567 ]\n",
      "  [ 0.19322212]\n",
      "  [-0.46834832]\n",
      "  [-1.3256651 ]\n",
      "  [ 0.6374412 ]\n",
      "  [-0.10...59 ]\n",
      "  [-0.6096962 ]\n",
      "  [ 0.36532003]\n",
      "  [ 0.657253  ]\n",
      "  [-0.23781289]\n",
      "  [-0.5856332 ]\n",
      "  [-1.9687681 ]\n",
      "  [-2.281027  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.7328626 ]\n",
      "  [-0.70381486]\n",
      "  [-0.26984787]\n",
      "  [ 0.02469073]\n",
      "  [-0.37803024]\n",
      "  [-1.1180894 ]\n",
      "  [-1.20...86 ]\n",
      "  [-2.471721  ]\n",
      "  [-1.1077486 ]\n",
      "  [-0.11187652]\n",
      "  [-1.2510221 ]\n",
      "  [ 0.5895619 ]\n",
      "  [ 1.1342744 ]\n",
      "  [-0.11754585]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58800990>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58800990>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58800990>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "   0.9794635  0.09746005 0.33560354...\n",
      "   0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "   0.11195529 0.28526783 0.46507692 0.5791958 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "   0.9794635  0.09746005 0.3356035...213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "   0.11195529 0.28526783 0.46507692 0.5791958 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58802d50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "   0.9794635  0.09746005 0.33560354...\n",
      "   0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "   0.11195529 0.28526783 0.46507692 0.5791958 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0...313\n",
      "  0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]])\n",
      "        inputs     = [needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 ...13\n",
      "  0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58802290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58802d50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 ...5791958 ]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58802290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0...313\n",
      "  0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]\n",
      " [-0.90400445]])\n",
      "        bias_ih    = needle.Tensor([[0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]\n",
      " [0.6301379]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58802290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.72130394]\n",
      " [-0.46233016]\n",
      " [ 0.5332844 ]\n",
      " [-0.8019258 ]\n",
      " [-0.7086199 ]\n",
      " [-0.9031143 ]\n",
      " [-0.07179141]...-0.02941591]\n",
      " [ 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]])\n",
      "        self       = needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0...313\n",
      "  0.7213261  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 ...0.02941591]\n",
      " [ 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58802210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 ...0.02941591]\n",
      " [ 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58802210>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57cd1930>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56247bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57cd0830>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56247bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0.81277...1  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.72130394]\n",
      " [-0.46233016]\n",
      " [ 0.5332844 ]\n",
      " [-0.8019258 ]\n",
      " [-0.7086199 ]\n",
      " [-0.9031143 ]\n",
      " [-0.07179141]\n",
      " [-0.... 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58802210>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0.81277...1  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.72130394]\n",
      " [-0.46233016]\n",
      " [ 0.5332844 ]\n",
      " [-0.8019258 ]\n",
      " [-0.7086199 ]\n",
      " [-0.9031143 ]\n",
      " [-0.07179141]\n",
      " [-0.... 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0.81277...1  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]], device=cpu())\n",
      "other = NDArray([[ 0.72130394]\n",
      " [-0.46233016]\n",
      " [ 0.5332844 ]\n",
      " [-0.8019258 ]\n",
      " [-0.7086199 ]\n",
      " [-0.9031143 ]\n",
      " [-0.07179141]\n",
      " [-0.... 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58803030>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58800230>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd562457f0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.72130394]\n",
      " [-0.46233016]\n",
      " [ 0.5332844 ]\n",
      " [-0.8019258 ]\n",
      " [-0.7086199 ]\n",
      " [-0.9031143 ]\n",
      " [-0.07179141]\n",
      " [-0.... 0.7419015 ]\n",
      " [-0.86151975]\n",
      " [-0.49395657]\n",
      " [-0.45067668]\n",
      " [ 0.46122944]\n",
      " [ 0.48384058]\n",
      " [-0.41400486]], device=cuda())\n",
      "out        = NDArray([[3.1272048e-34]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.6869372  0.26778832 0.59187704 0.26212928 0.40555376 0.69884557\n",
      "  0.9794635  0.09746005 0.33560354 0.81277...1  0.412264   0.8684407  0.8977509  0.92596704 0.5044165\n",
      "  0.11195529 0.28526783 0.46507692 0.5791958 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.8278402 ]\n",
      "  [ 0.22186972]\n",
      "  [ 1.0596155 ]\n",
      "  [-0.8424963 ]\n",
      "  [ 0.59488815]\n",
      "  [-0.17260411]\n",
      "  [ 1.65...075]\n",
      "  [ 0.1539863 ]\n",
      "  [-0.87671906]\n",
      "  [-1.1737481 ]\n",
      "  [-0.48722395]\n",
      "  [-0.5260903 ]\n",
      "  [ 0.8904151 ]\n",
      "  [-0.6799145 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.54947996]\n",
      "  [-0.6535306 ]\n",
      "  [-0.00896846]\n",
      "  [ 1.1467607 ]\n",
      "  [-0.0848456 ]\n",
      "  [ 0.6804559 ]\n",
      "  [ 0.74...74 ]\n",
      "  [-0.34525594]\n",
      "  [-1.5410144 ]\n",
      "  [ 1.6337364 ]\n",
      "  [-0.69398934]\n",
      "  [-0.8762969 ]\n",
      "  [ 0.91202843]\n",
      "  [ 0.38946867]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57e458d0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e458d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e458d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.185...5312 ... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.18.... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e44c10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.185...5312 ... 0.25514165 0.07634109 0.32837588]\n",
      "  [0.4200595  0.41904432 0.18565312 ... 0.25514165 0.07634109 0.32837588]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8...302\n",
      "  0.41758847 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]])\n",
      "        inputs     = [needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0....0.41758847 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e45dd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e44c10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0....32837588]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e45dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8...302\n",
      "  0.41758847 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]\n",
      " [0.50876784]])\n",
      "        bias_ih    = needle.Tensor([[0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]\n",
      " [0.33271253]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e45dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.28915024]\n",
      " [-0.23860705]\n",
      " [-0.47353297]\n",
      " [-0.9239407 ]\n",
      " [-0.3050394 ]\n",
      " [-0.62232614]\n",
      " [-0.46476465]...-0.68895876]\n",
      " [ 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]])\n",
      "        self       = needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8...302\n",
      "  0.41758847 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0....0.68895876]\n",
      " [ 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57641750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0....0.68895876]\n",
      " [ 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57641750>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584a75b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57641f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584a7a30>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57641f50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8137764...47 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]], device=cpu())\n",
      "        b          = NDArray([[-0.28915024]\n",
      " [-0.23860705]\n",
      " [-0.47353297]\n",
      " [-0.9239407 ]\n",
      " [-0.3050394 ]\n",
      " [-0.62232614]\n",
      " [-0.46476465]\n",
      " [-0.... 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57641750>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8137764...47 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]], device=cpu())\n",
      "        b          = NDArray([[-0.28915024]\n",
      " [-0.23860705]\n",
      " [-0.47353297]\n",
      " [-0.9239407 ]\n",
      " [-0.3050394 ]\n",
      " [-0.62232614]\n",
      " [-0.46476465]\n",
      " [-0.... 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8137764...47 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]], device=cpu())\n",
      "other = NDArray([[-0.28915024]\n",
      " [-0.23860705]\n",
      " [-0.47353297]\n",
      " [-0.9239407 ]\n",
      " [-0.3050394 ]\n",
      " [-0.62232614]\n",
      " [-0.46476465]\n",
      " [-0.... 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57640eb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57e46e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57642030>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.28915024]\n",
      " [-0.23860705]\n",
      " [-0.47353297]\n",
      " [-0.9239407 ]\n",
      " [-0.3050394 ]\n",
      " [-0.62232614]\n",
      " [-0.46476465]\n",
      " [-0.... 0.07298601]\n",
      " [ 0.8444451 ]\n",
      " [ 0.8416319 ]\n",
      " [ 0.51596594]\n",
      " [ 0.01325536]\n",
      " [-0.65744984]\n",
      " [ 0.8641647 ]], device=cuda())\n",
      "out        = NDArray([[6.2410577e-35]\n",
      " [0.0000000e+00]\n",
      " [8.4704800e-34]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.4200595  0.41904432 0.18565312 0.10099479 0.4414977  0.507631\n",
      "  0.51027006 0.5951221  0.06918228 0.8137764...47 0.10405653 0.47245112 0.9280579  0.5738749  0.4646773\n",
      "  0.9654965  0.25514165 0.07634109 0.32837588]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.93051356 -0.59341514  1.8381847  -1.4600158   1.6330066\n",
      "   -0.14971203  1.1780516   2.3654501  -2.7882044   0.8526033\n",
      "   -0.23321949 -0.01617239]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 1.4623482  -0.5290212  -0.00498432  0.22776671  0.9090755\n",
      "   -1.5286193   0.23248301  2.410371    2.0119514  -0.6452563\n",
      "   -0.49360308  0.25135362]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56ebd850>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56ebd850>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56ebd850>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.44557956]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.44557956]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56ebe950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.44557956]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.44557956]])\n",
      "        inputs     = [needle.Tensor([[0.44557956]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ebdd90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56ebe950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44557956]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ebdd90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.44557956]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.19887431 -0.04854107  0.2797253   0.18608767 -0.27088386 -0.28484616\n",
      "  -0.2484011   0.09048563 -0.24897559  0.24471372  0.10703525  0.27737713]])\n",
      "        bias_ih    = needle.Tensor([[ 0.18456784  0.19357881  0.24130487  0.21919286 -0.19397657 -0.21918184\n",
      "  -0.1302975   0.23923409 -0.15415116 -0.20155352 -0.17169161 -0.09918511]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ebdd90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]])\n",
      "        self       = needle.Tensor([[0.44557956]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.44557956]]), needle.Tensor([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebea90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.44557956]]), needle.Tensor([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebea90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e8a830>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56ebf790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e89170>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56ebf790>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44557956]], device=cpu())\n",
      "        b          = NDArray([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ebea90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.44557956]], device=cpu())\n",
      "        b          = NDArray([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.44557956]], device=cpu())\n",
      "other = NDArray([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56ebc930>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56ebdd30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56ebf2b0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.11085385  0.19276577 -0.10921836  0.13277104  0.20111859  0.13021576\n",
      "  -0.25947312 -0.17261693 -0.17553717 -0.18888444 -0.07099894 -0.13194343]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 1.1103407e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.44557956]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.3034314   0.6666714  -0.26239574 -0.3889904  -0.19810048\n",
      "   -0.70913297 -0.29829413  0.7783879   0.9886607   0.1638201\n",
      "   -0.7374084   1.7870169 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.13561189 -0.5821096  -1.0743849   0.6584102   1.1716117\n",
      "    0.3887992   0.67454416 -0.5969425   0.8244402  -0.02828806\n",
      "    1.5574701   1.5543431 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd55a45e50>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a45e50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a45e50>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a47ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]\n",
      "\n",
      " [[0.7805864]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7805864]])\n",
      "        inputs     = [needle.Tensor([[0.7805864]]), needle.Tensor([[0.7805864]]), needle.Tensor([[0.7805864]]), needle.Tensor([[0.7805864]]), needle.Tensor([[0.7805864]]), needle.Tensor([[0.7805864]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a477d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a47ad0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7805864]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a477d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7805864]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.2665298   0.02717477 -0.15921487 -0.27447876 -0.04726453 -0.21513739\n",
      "   0.25991982  0.17858338 -0.15193433 -0.22482705 -0.2179578   0.11368749]])\n",
      "        bias_ih    = needle.Tensor([[-0.20575112 -0.2717812   0.18972176 -0.02603546 -0.12019297 -0.11494942\n",
      "  -0.16879466  0.00543749 -0.23930788 -0.06102967  0.01295254  0.27537876]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a477d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]])\n",
      "        self       = needle.Tensor([[0.7805864]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7805864]]), needle.Tensor([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58647c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7805864]]), needle.Tensor([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58647c50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578dccf0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58647a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578dcf30>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58647a10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7805864]], device=cpu())\n",
      "        b          = NDArray([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58647c50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7805864]], device=cpu())\n",
      "        b          = NDArray([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7805864]], device=cpu())\n",
      "other = NDArray([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a44230>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55a45bf0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58647e30>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.04256809  0.04971099 -0.06986418  0.24281114 -0.14338328 -0.12681377\n",
      "   0.08624908 -0.06645791 -0.19518524 -0.17175609 -0.27295336 -0.20977092]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 1.5896651e-34 0.0000000e+00 1.1159112e+00\n",
      "  6.9922596e-02 1.2303424e+00 2.0100413e-01 1.2520426e-03 1.0286338e+00\n",
      "  3.1799585e-01 1.3031569e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.7805864]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.15934676  0.1469349   2.0389493  -0.23578072  0.38921684\n",
      "    0.03398596  0.64816964 -0.04130866  0...   0.11436541 -0.27422073\n",
      "   -1.2807788  -0.21853864 -1.4056137   0.68259     0.68437606\n",
      "    0.5987835  -0.537194  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-3.6008844e-01  1.4452466e-01  1.0734417e+00 -1.6334558e-01\n",
      "    2.1442208e-01  5.5764598e-01 -1.87358...640e-01  4.5708618e-01 -2.1441695e-03 -2.1589929e-01\n",
      "    2.9102448e-01  1.3402721e+00 -6.4190888e-01  5.4836756e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57ddb750>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ddb750>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ddb750>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.03557428]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.03557428]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ddafd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.03557428]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.03557428]])\n",
      "        inputs     = [needle.Tensor([[0.03557428]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dd8050>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ddafd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.03557428]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dd8050>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.03557428]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.21028422 -0.24016535  0.23521554 -0.03299806 -0.22143832  0.23240954\n",
      "   0.15332648 -0.02869955  0.11042395  0.2667343  -0.13462691  0.14038262]])\n",
      "        bias_ih    = needle.Tensor([[-0.21440406 -0.0431439  -0.24578187 -0.09006266  0.26233572  0.00743133\n",
      "  -0.27452213 -0.03938797  0.00124216 -0.19759145 -0.13621835  0.26078415]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dd8050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]])\n",
      "        self       = needle.Tensor([[0.03557428]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.03557428]]), needle.Tensor([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c8f450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.03557428]]), needle.Tensor([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c8f450>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d6cb0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56c8fc50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d70b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56c8fc50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.03557428]], device=cpu())\n",
      "        b          = NDArray([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c8f450>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.03557428]], device=cpu())\n",
      "        b          = NDArray([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.03557428]], device=cpu())\n",
      "other = NDArray([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56c8f3b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57dd89f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56c8c570>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.08962193  0.26162446  0.09323755  0.14972588  0.12214592  0.24426538\n",
      "   0.25303894 -0.05817392 -0.2129397  -0.0305199   0.0845007  -0.12164876]], device=cuda())\n",
      "out        = NDArray([[ 7.9518032e-35  0.0000000e+00  1.2287307e-01 -1.2148678e-02\n",
      "  -5.3199190e-01  5.2386767e-01 -4.8083586e-01  3.0880648e-01\n",
      "   7.0646554e-01 -3.3597583e-01 -2.3057726e-01  9.2163086e-03]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.03557428]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.5456809  -0.29332832  0.37925872 -0.60990274  0.769712\n",
      "   -0.9592122   0.13420747  0.19944133 -0.3...17 -0.9063928  -0.07830022\n",
      "   -0.268468   -0.7456562   1.8449415  -1.1049021  -1.8617011\n",
      "    0.40207914  0.25841516]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-1.5624832  -0.36850736 -0.62296784  2.2615683   0.24867019\n",
      "   -0.58263355 -0.06331509 -0.9159551  -0...8  -0.9464402   0.5621027\n",
      "   -1.5917147  -1.4127887  -0.29194057  1.2543122  -0.14740239\n",
      "   -0.45263606  1.1770948 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd585fdfd0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585fdfd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd585fdfd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fd890>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]\n",
      "\n",
      " [[0.1785486]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.1785486]])\n",
      "        inputs     = [needle.Tensor([[0.1785486]]), needle.Tensor([[0.1785486]]), needle.Tensor([[0.1785486]]), needle.Tensor([[0.1785486]]), needle.Tensor([[0.1785486]]), needle.Tensor([[0.1785486]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fecd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fd890>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.1785486]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fecd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.1785486]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.28424448 -0.2471242   0.12602276  0.1647883   0.2623114   0.23383808\n",
      "  -0.26431766 -0.05798507 -0.18259376  0.15418717  0.23252738 -0.14763393]])\n",
      "        bias_ih    = needle.Tensor([[-0.1823265  -0.21147516  0.00898194 -0.27832076 -0.0036236  -0.05426328\n",
      "   0.09810969 -0.16418481  0.23889363  0.10423258 -0.02812314 -0.09766974]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fecd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]])\n",
      "        self       = needle.Tensor([[0.1785486]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.1785486]]), needle.Tensor([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b4950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.1785486]]), needle.Tensor([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b4950>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5716dfb0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd584b4210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5716e430>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd584b4210>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.1785486]], device=cpu())\n",
      "        b          = NDArray([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b4950>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.1785486]], device=cpu())\n",
      "        b          = NDArray([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.1785486]], device=cpu())\n",
      "other = NDArray([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58090eb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585fc6f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b45f0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.11377662 -0.01250368  0.14160278  0.01587814 -0.18703467  0.01707539\n",
      "  -0.03821132  0.20410651 -0.23941758 -0.01349145 -0.03674236  0.20536926]], device=cuda())\n",
      "out        = NDArray([[ 8.2580682e-35  0.0000000e+00  8.3022374e-34  0.0000000e+00\n",
      "  -6.3135624e-02 -1.0901111e-01  2.0917404e-01  1.4104122e-01\n",
      "  -1.2427318e-01 -5.3052312e-01  2.4112046e-01  2.3687500e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.1785486]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.48819867 -1.2398105  -1.0820074   1.74105    -0.97409356\n",
      "    0.99481183 -0.55854946  0.6983993   0...017  0.49419415  1.1340891\n",
      "    0.01652545 -0.34142488 -0.14084591  0.02065296  0.8997581\n",
      "   -1.52303    -0.06614714]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 2.7694026e-01 -3.2617936e-01 -4.3678364e-01 -3.2150346e-01\n",
      "   -5.1087059e-02  7.3316604e-01  2.88039...582e-01  1.1432272e+00  4.1128448e-01 -1.2648112e-01\n",
      "    2.8943086e-01 -1.3051378e+00 -1.0431932e+00  1.0444161e+00]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5705e290>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705e290>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705e290>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]...0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345...345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5705cad0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]...0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]\n",
      "  [0.40541345]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]])\n",
      "        inputs     = [needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705c0d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5705cad0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.4... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705c0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.09928055 -0.07937777  0.06085002  0.14981571  0.07433936 -0.25936466\n",
      "  -0.16325025  0.18414679 -0.2...085002  0.14981571  0.07433936 -0.25936466\n",
      "  -0.16325025  0.18414679 -0.24888337 -0.2635352  -0.1229952   0.04169241]])\n",
      "        bias_ih    = needle.Tensor([[-0.23686649 -0.18593639 -0.11187357  0.07522464  0.17745438  0.24102074\n",
      "   0.1873211  -0.04281549  0.2...187357  0.07522464  0.17745438  0.24102074\n",
      "   0.1873211  -0.04281549  0.28662735  0.16145694  0.20852476  0.1807684 ]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705c0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.1865544   0.1676943   0.07189929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]])\n",
      "        self       = needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.4...89929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ed50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.4...89929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ed50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd585dcfb0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd5705cb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd585de870>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd5705cb50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]], device=cpu())\n",
      "        b          = NDArray([[-0.1865544   0.1676943   0.07189929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ed50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]], device=cpu())\n",
      "        b          = NDArray([[-0.1865544   0.1676943   0.07189929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]], device=cpu())\n",
      "other = NDArray([[-0.1865544   0.1676943   0.07189929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5705c130>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5705f230>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5705d9b0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.1865544   0.1676943   0.07189929 -0.02079698 -0.09076737 -0.21319807\n",
      "   0.25358582 -0.24479765 -0.08816084 -0.08260089  0.08785555 -0.08897303]], device=cuda())\n",
      "out        = NDArray([[9.6153877e-35 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]\n",
      " [0.40541345]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 3.081713   -0.51123774 -1.5927002  -0.5521083  -0.03948095\n",
      "    2.6256897  -0.09113427 -1.2485952   1...8  1.1531643   0.36045554\n",
      "    2.20757     0.28670314 -0.44530132  0.57386297  0.50830895\n",
      "   -0.44296813  1.1347048 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.18406673 -1.2957606   0.18728267 -0.46477538  0.8637722\n",
      "   -0.17195943 -0.5493289  -1.6463174  -1....6   0.341977    0.27669135\n",
      "   -0.02779353  1.3731592   1.7727915  -0.4266293  -1.6165029\n",
      "   -1.6068665  -0.74631554]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5623c1d0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5623c1d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5623c1d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8...554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0....[0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5623ce90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8...554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]\n",
      "  [0.8689554]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]])\n",
      "        inputs     = [needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]...689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623fb90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5623ce90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623fb90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.08989456  0.15851155 -0.16254216 -0.10877022 -0.26085126 -0.24840873\n",
      "  -0.18920699 -0.04976231  0.0...254216 -0.10877022 -0.26085126 -0.24840873\n",
      "  -0.18920699 -0.04976231  0.04637519 -0.04304704  0.0057272   0.2167669 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.22488129 -0.04090735 -0.11635523  0.23264873 -0.12652163  0.01031986\n",
      "  -0.28697732  0.04006097  0.2...635523  0.23264873 -0.12652163  0.01031986\n",
      "  -0.28697732  0.04006097  0.2007632  -0.0408444  -0.18678898 -0.24352065]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623fb90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.1334435   0.27785265  0.06277707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]])\n",
      "        self       = needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]...77707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4a9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]...77707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4a9d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584a5ab0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd56b4afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584a6330>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd56b4afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]], device=cpu())\n",
      "        b          = NDArray([[ 0.1334435   0.27785265  0.06277707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4a9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]], device=cpu())\n",
      "        b          = NDArray([[ 0.1334435   0.27785265  0.06277707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]], device=cpu())\n",
      "other = NDArray([[ 0.1334435   0.27785265  0.06277707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5623eff0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5623f430>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b490f0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.1334435   0.27785265  0.06277707 -0.13228692  0.18968982 -0.15315175\n",
      "   0.2647614   0.19420078  0.01728442  0.01238626 -0.07040899  0.08590311]], device=cuda())\n",
      "out        = NDArray([[2.04566580e-33 0.00000000e+00 2.32063891e-34 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0...000000e+00 1.20371762e-38 0.00000000e+00\n",
      "  1.24666238e-38 0.00000000e+00 1.20371762e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]\n",
      " [0.8689554]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 1.13617063e+00  9.58115399e-01 -2.82250553e-01 -1.15383542e+00\n",
      "   -1.59258857e-01 -3.08414668e-01 -6... -1.35976374e+00  4.92420375e-01  2.69409478e-01\n",
      "    3.03431451e-01 -2.68530369e+00 -9.79508400e-01 -2.36269876e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 1.2985284  -0.5679861   1.1411948  -0.6248696  -0.19202085\n",
      "   -0.26346353 -0.63567764  0.3400917  -0...06  -0.0851777  -0.2767805\n",
      "    1.3474753   0.7394181   2.6768835  -0.6828987  -0.5005428\n",
      "   -1.5767318   3.1823897 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd580b7190>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b7190>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b7190>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b4e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]\n",
      "  [0.9483312]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]])\n",
      "        inputs     = [needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b7290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b4e50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b7290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.05975813  0.18051645  0.12092805 -0.01553491 -0.04120231  0.16376147\n",
      "  -0.09292005 -0.23992524  0.2...092805 -0.01553491 -0.04120231  0.16376147\n",
      "  -0.09292005 -0.23992524  0.21471786 -0.18537547 -0.15872476 -0.18437693]])\n",
      "        bias_ih    = needle.Tensor([[ 0.26188403  0.23471653 -0.13686454  0.02825645 -0.19500938 -0.2605682\n",
      "   0.16979063  0.16951999  0.01...3686454  0.02825645 -0.19500938 -0.2605682\n",
      "   0.16979063  0.16951999  0.01822922 -0.11559413 -0.27497005  0.0820685 ]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b7290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.19672433 -0.13964468  0.15100017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]])\n",
      "        self       = needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]...00017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5876c550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]...00017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5876c550>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5825b130>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd5876fd50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5825a470>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd5876fd50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]], device=cpu())\n",
      "        b          = NDArray([[ 0.19672433 -0.13964468  0.15100017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5876c550>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]], device=cpu())\n",
      "        b          = NDArray([[ 0.19672433 -0.13964468  0.15100017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]], device=cpu())\n",
      "other = NDArray([[ 0.19672433 -0.13964468  0.15100017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580b76b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580b6d70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5876edf0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.19672433 -0.13964468  0.15100017  0.09327501  0.00996366  0.12495053\n",
      "  -0.11450498  0.15890193  0.17723376 -0.24189761 -0.03406778  0.17075995]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 5.1617588e+13 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]\n",
      " [0.9483312]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-4.01226729e-01 -2.84854919e-01  3.47999841e-01  2.49358565e-01\n",
      "   -2.80724347e-01  8.53318453e-01 -9... -9.35506165e-01 -4.69647497e-01  7.81560659e-01\n",
      "    1.43954802e+00  6.93909347e-01 -1.08592939e+00 -4.48946089e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-6.00667708e-02 -8.40017293e-03  8.30531716e-01  1.30188361e-01\n",
      "    1.37310350e+00 -4.89358068e-01  6...  7.66703188e-01  6.18271064e-04 -4.74136412e-01\n",
      "    1.46910286e+00  1.92138866e-01  1.01960635e+00 -1.39523312e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57ab4890>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ab4890>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57ab4890>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8...082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0....[0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ab5310>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8...082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]\n",
      "  [0.8532082]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]])\n",
      "        inputs     = [needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]...532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ab6490>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ab5310>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ab6490>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.25711465 -0.21877185  0.23614043  0.27709335 -0.15132867  0.20324501\n",
      "  -0.23398253  0.14020458 -0.1...614043  0.27709335 -0.15132867  0.20324501\n",
      "  -0.23398253  0.14020458 -0.19096377 -0.08558604 -0.06122719 -0.20699021]])\n",
      "        bias_ih    = needle.Tensor([[-0.20552507 -0.06545818 -0.2776617  -0.06514047 -0.09003231 -0.01192495\n",
      "   0.03032091  0.15966138  0.1...76617  -0.06514047 -0.09003231 -0.01192495\n",
      "   0.03032091  0.15966138  0.1486558   0.02626482 -0.19372582 -0.02413553]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ab6490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.12931527  0.04026848 -0.0674746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]])\n",
      "        self       = needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]...4746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edb850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]...4746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edb850>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584b87f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd57edb090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584bb9b0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd57edb090>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]], device=cpu())\n",
      "        b          = NDArray([[-0.12931527  0.04026848 -0.0674746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57edb850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]], device=cpu())\n",
      "        b          = NDArray([[-0.12931527  0.04026848 -0.0674746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]], device=cpu())\n",
      "other = NDArray([[-0.12931527  0.04026848 -0.0674746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ab41b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57ab69f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57edaaf0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.12931527  0.04026848 -0.0674746  -0.28723073 -0.13704811 -0.18743613\n",
      "  -0.11501576  0.2813127  -0.19029126 -0.18378735 -0.24564695 -0.03620511]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 5.9549936e+14 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]\n",
      " [0.8532082]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.97279835 -0.6422341   0.738405    1.1645491  -1.0582404\n",
      "   -0.05856239 -0.52050155 -0.52386135  0.90706134 -0.1867868\n",
      "    0.29006085 -0.13468204]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.2033217  -1.11334     0.0434734  -0.8960802   0.8826433\n",
      "    0.68413055  1.1925328   0.08707939  0.8714843  -0.6505444\n",
      "   -0.06406744  0.5525916 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd573a7550>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd573a7550>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd573a7550>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "   0.8534875  0.27343372 0.89299095...\n",
      "   0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "   0.59198844 0.39987034 0.0489424  0.34567708]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "   0.8534875  0.27343372 0.8929909...655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "   0.59198844 0.39987034 0.0489424  0.34567708]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd573a5710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "   0.8534875  0.27343372 0.89299095...\n",
      "   0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "   0.59198844 0.39987034 0.0489424  0.34567708]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0...388\n",
      "  0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]])\n",
      "        inputs     = [needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 ...88\n",
      "  0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7810>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd573a5710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 ...163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7810>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0...388\n",
      "  0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.20925507  0.13945198  0.16837895 -0.1117204   0.11168161  0.00179675\n",
      "  -0.00679138 -0.25721836  0.12453923  0.12798637  0.2373997   0.27327472]])\n",
      "        bias_ih    = needle.Tensor([[ 0.01324725  0.22197819 -0.03417119  0.23830521  0.20088688  0.00729531\n",
      "   0.14517623 -0.08509403  0.2506479   0.02478305  0.25986165  0.19700101]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.00941828  0.09962618 -0.05218518 -0.03821445  0.17353708 -0.19902681\n",
      "  -0.08583729 -0.17761165  0.1...502485  0.1800932  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]])\n",
      "        self       = needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0...388\n",
      "  0.8655301  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 ...02485  0.1800932  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd573a6290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 ...02485  0.1800932  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd573a6290>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58506d30>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd573a5190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58506f70>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd573a5190>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0.49088...1  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]], device=cpu())\n",
      "        b          = NDArray([[-0.00941828  0.09962618 -0.05218518 -0.03821445  0.17353708 -0.19902681\n",
      "  -0.08583729 -0.17761165  0.1072796...32  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd573a6290>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0.49088...1  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]], device=cpu())\n",
      "        b          = NDArray([[-0.00941828  0.09962618 -0.05218518 -0.03821445  0.17353708 -0.19902681\n",
      "  -0.08583729 -0.17761165  0.1072796...32  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0.49088...1  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]], device=cpu())\n",
      "other = NDArray([[-0.00941828  0.09962618 -0.05218518 -0.03821445  0.17353708 -0.19902681\n",
      "  -0.08583729 -0.17761165  0.1072796...32  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573a42f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd573a6b70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573a4630>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.00941828  0.09962618 -0.05218518 -0.03821445  0.17353708 -0.19902681\n",
      "  -0.08583729 -0.17761165  0.1072796...32  -0.25248596 -0.10945548\n",
      "  -0.2845761  -0.18660331 -0.19290522 -0.22866319 -0.049052   -0.13770315]], device=cuda())\n",
      "out        = NDArray([[ 8.1775126e+35  4.5823861e-41  2.3016561e-33  0.0000000e+00\n",
      "  -5.3670597e-01  1.3800359e-01  9.0950727e-02 -6.9130152e-01\n",
      "  -2.0243567e-01  5.0584596e-01 -8.0800116e-02 -1.2393463e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.2735934  0.2971892  0.9960643  0.7109207  0.21266446 0.22332571\n",
      "  0.8534875  0.27343372 0.89299095 0.49088...1  0.64580923 0.8579637  0.8493403  0.48619163 0.6575906\n",
      "  0.59198844 0.39987034 0.0489424  0.34567708]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.23237595  0.61328465 -1.0959932  -0.14832093  0.02343689\n",
      "    1.0009646   0.31580877  1.9141004  -0.0086342  -1.6514521\n",
      "   -0.7976779  -0.917436  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.9268477  -0.15813646  0.5059544  -0.16744576 -0.3004821\n",
      "   -0.43840617 -1.0361124   1.1044836   1.5117539   0.16750139\n",
      "   -1.2933897   0.5385373 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56e7fb90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56e7fb90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56e7fb90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "   0.3181528  0.6829963  0.8015556 ...\n",
      "   0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "   0.3584367  0.30584115 0.9293163  0.46769142]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "   0.3181528  0.6829963  0.8015556...0820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "   0.3584367  0.30584115 0.9293163  0.46769142]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56e7ff50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "   0.3181528  0.6829963  0.8015556 ...\n",
      "   0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "   0.3584367  0.30584115 0.9293163  0.46769142]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0...245\n",
      "  0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]])\n",
      "        inputs     = [needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  ...0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56e7cdd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56e7ff50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  ...11  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56e7cdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0...245\n",
      "  0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.029796   -0.13319626  0.26176417  0.18589103  0.17437857  0.20297712\n",
      "   0.03032744 -0.26746604 -0.1834467  -0.06921323 -0.27928817  0.03099591]])\n",
      "        bias_ih    = needle.Tensor([[ 0.08423316 -0.01748779  0.19504902 -0.21528411 -0.19360991  0.12869284\n",
      "   0.2760114   0.2138446  -0.09726094  0.16387275  0.16381064 -0.21235101]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56e7cdd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.13012408  0.01459748 -0.15775792  0.15004829 -0.21869004  0.24640054\n",
      "   0.25638068  0.24283993  0.1...468706  0.14479294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]])\n",
      "        self       = needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0...245\n",
      "  0.40820163 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  ...68706  0.14479294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4ae50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  ...68706  0.14479294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4ae50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b7cf0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56b49d10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b4130>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56b49d10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0.49581...63 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]], device=cpu())\n",
      "        b          = NDArray([[-0.13012408  0.01459748 -0.15775792  0.15004829 -0.21869004  0.24640054\n",
      "   0.25638068  0.24283993  0.1464085...294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b4ae50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0.49581...63 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]], device=cpu())\n",
      "        b          = NDArray([[-0.13012408  0.01459748 -0.15775792  0.15004829 -0.21869004  0.24640054\n",
      "   0.25638068  0.24283993  0.1464085...294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0.49581...63 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]], device=cpu())\n",
      "other = NDArray([[-0.13012408  0.01459748 -0.15775792  0.15004829 -0.21869004  0.24640054\n",
      "   0.25638068  0.24283993  0.1464085...294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56e7ce30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56e7c970>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b48330>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.13012408  0.01459748 -0.15775792  0.15004829 -0.21869004  0.24640054\n",
      "   0.25638068  0.24283993  0.1464085...294  0.1925005  -0.19073233\n",
      "  -0.08019081 -0.2794979  -0.26415116  0.28310513  0.26316047  0.10939953]], device=cuda())\n",
      "out        = NDArray([[2.3889366e-33 0.0000000e+00 1.0585474e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.41471222 0.78288084 0.05136976 0.4768351  0.45698068 0.41341218\n",
      "  0.3181528  0.6829963  0.8015556  0.49581...63 0.6728993  0.8812985  0.36918873 0.8958011  0.3116625\n",
      "  0.3584367  0.30584115 0.9293163  0.46769142]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.0989684  -1.6020147  -0.74101996  0.04966041  0.12584929\n",
      "    0.30227005 -1.5326673  -0.82197595 -0...7  0.2660692  -0.36908937\n",
      "   -1.2383153  -0.42982846 -0.3705653  -1.1275562  -0.33583704\n",
      "    1.0080931   0.91784334]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.13896015 -0.28584448  0.89785796  0.3357448  -0.37243938\n",
      "    0.41248205 -1.0431846  -3.6673179   0...7 -1.2778555  -0.17391036\n",
      "    0.46767837 -1.7192675   0.21435522  1.7337345  -0.87017745\n",
      "    1.4711069   0.61992514]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57aa70d0>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57aa70d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57aa70d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "   0.6403927  0.6644536  0.56600326...\n",
      "   0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "   0.13861829 0.85532844 0.14899053 0.27625313]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "   0.6403927  0.6644536  0.5660032...1727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "   0.13861829 0.85532844 0.14899053 0.27625313]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57aa7610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "   0.6403927  0.6644536  0.56600326...\n",
      "   0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "   0.13861829 0.85532844 0.14899053 0.27625313]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0...631\n",
      "  0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]])\n",
      "        inputs     = [needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 ...31\n",
      "  0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57aa5350>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57aa7610>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 ...465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57aa5350>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0...631\n",
      "  0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.2656987   0.00727955 -0.17423469  0.12839705  0.18931824  0.17984146\n",
      "   0.0620895   0.02697828 -0.04371054  0.20931453  0.2748742   0.0544295 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.16602042  0.07180467  0.05076322 -0.08390875  0.13653708  0.21560317\n",
      "  -0.08614582  0.14854962 -0.13360587 -0.17145537  0.15639636 -0.13064572]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57aa5350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 1.51402652e-01 -2.30573788e-01 -1.09971166e-02  2.31659055e-01\n",
      "   1.74851626e-01 -1.25536025e-02 -2.6...01 -2.38354251e-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]])\n",
      "        self       = needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0...631\n",
      "  0.01727226 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 ...1 -2.38354251e-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa58d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 ...1 -2.38354251e-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa58d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58258b30>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd585ac950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5825beb0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd585ac950>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0.63937...26 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]], device=cpu())\n",
      "        b          = NDArray([[ 1.51402652e-01 -2.30573788e-01 -1.09971166e-02  2.31659055e-01\n",
      "   1.74851626e-01 -1.25536025e-02 -2.6174366...-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa58d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0.63937...26 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]], device=cpu())\n",
      "        b          = NDArray([[ 1.51402652e-01 -2.30573788e-01 -1.09971166e-02  2.31659055e-01\n",
      "   1.74851626e-01 -1.25536025e-02 -2.6174366...-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0.63937...26 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]], device=cpu())\n",
      "other = NDArray([[ 1.51402652e-01 -2.30573788e-01 -1.09971166e-02  2.31659055e-01\n",
      "   1.74851626e-01 -1.25536025e-02 -2.6174366...-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57aa6830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57aa6e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585ac6f0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 1.51402652e-01 -2.30573788e-01 -1.09971166e-02  2.31659055e-01\n",
      "   1.74851626e-01 -1.25536025e-02 -2.6174366...-01  1.60610974e-01 -2.35976696e-01\n",
      "  -2.45134547e-01 -2.53127784e-01 -7.90226460e-03  8.32531750e-02]], device=cuda())\n",
      "out        = NDArray([[4.4546e-41 0.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00\n",
      "  1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.7634778  0.86402327 0.29580343 0.7819376  0.37329862 0.43841577\n",
      "  0.6403927  0.6644536  0.56600326 0.63937...26 0.39931116 0.40269476 0.35794243 0.18759465 0.8318829\n",
      "  0.13861829 0.85532844 0.14899053 0.27625313]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.063263    1.0088557   1.2212087  -0.42487663 -0.28533933\n",
      "    0.6458061  -0.7916713  -1.3279136   0...85  -0.7266996  -1.213034\n",
      "    2.2300367  -1.3096622  -0.6452144   1.909223   -0.20155789\n",
      "    0.9575817  -0.70671535]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 2.1565828   1.4408962   0.6726587   1.4667532  -0.8753728\n",
      "   -0.2754906  -0.48930934  0.4765697  -0....3 -0.20415616  0.49704668\n",
      "   -0.627365    0.7156733  -1.0744215   0.42901567 -0.11834219\n",
      "   -0.02727306 -0.5290212 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5608a650>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5608a650>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5608a650>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "   0.35344648 0.6900886  0.43187776...   0.80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "   0.6418473  0.97454786 0.25709066 0.0474166 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "   0.35344648 0.6900886  0.4318777...062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "   0.6418473  0.97454786 0.25709066 0.0474166 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5608b1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "   0.35344648 0.6900886  0.43187776...   0.80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "   0.6418473  0.97454786 0.25709066 0.0474166 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0...77\n",
      "  0.80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]])\n",
      "        inputs     = [needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 ....80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089410>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5608b1d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 ...15 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089410>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0...77\n",
      "  0.80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.17407122 -0.16328298  0.1685001  -0.13875417  0.22575611  0.05066559\n",
      "   0.07533136  0.2472902  -0.02575561  0.17540619  0.17987779  0.2237947 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.08987674 -0.14863    -0.06065156 -0.08631967  0.1440793   0.0112882\n",
      "   0.24329102  0.04307011 -0.2371792   0.15712407 -0.2043496  -0.14420354]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.64585942e-01  6.60321414e-02 -1.71029866e-02 -3.48117352e-02\n",
      "  -2.61094034e-01 -2.75237143e-01  1.1...01 -1.40040621e-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]])\n",
      "        self       = needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0...77\n",
      "  0.80062646 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 ...1 -1.40040621e-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58772850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 ...1 -1.40040621e-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58772850>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56f11eb0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd58036e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5735e2f0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd58036e50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0.63007...6 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]], device=cpu())\n",
      "        b          = NDArray([[-2.64585942e-01  6.60321414e-02 -1.71029866e-02 -3.48117352e-02\n",
      "  -2.61094034e-01 -2.75237143e-01  1.1731258...-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58772850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0.63007...6 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]], device=cpu())\n",
      "        b          = NDArray([[-2.64585942e-01  6.60321414e-02 -1.71029866e-02 -3.48117352e-02\n",
      "  -2.61094034e-01 -2.75237143e-01  1.1731258...-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0.63007...6 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]], device=cpu())\n",
      "other = NDArray([[-2.64585942e-01  6.60321414e-02 -1.71029866e-02 -3.48117352e-02\n",
      "  -2.61094034e-01 -2.75237143e-01  1.1731258...-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58770530>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56089070>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58037bf0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-2.64585942e-01  6.60321414e-02 -1.71029866e-02 -3.48117352e-02\n",
      "  -2.61094034e-01 -2.75237143e-01  1.1731258...-01 -2.71721929e-01 -2.55478472e-01\n",
      "   9.76102352e-02  1.02628917e-01  5.89603484e-02  1.96341574e-01]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 1.5896651e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.10410908 0.21564794 0.04714888 0.72742575 0.15924215 0.41944572\n",
      "  0.35344648 0.6900886  0.43187776 0.63007...6 0.9738031  0.5150815  0.1621683  0.69327915 0.60915077\n",
      "  0.6418473  0.97454786 0.25709066 0.0474166 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-8.1280313e-02  1.8274432e-01  5.3402764e-01  3.9369944e-02\n",
      "    7.7159858e-01 -1.1949098e+00  2.40361...968e-01 -1.3256644e-01  2.5111964e-01  1.4399612e+00\n",
      "   -1.3114643e+00 -2.6754332e-01 -1.4823310e-01  1.2199858e+00]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.22665991  1.0270519  -1.6070608  -0.7319661  -0.1735938\n",
      "    0.35784784 -0.4607296   1.609324    1....91  -0.7128102  -0.13340093\n",
      "    0.07952744 -0.64953166 -0.26082087 -0.34428418 -2.526462\n",
      "   -2.6977131   0.43531173]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd584a7210>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a7210>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a7210>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "   0.5674448  0.16988643 0.52970874...   0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "   0.2850487  0.8065831  0.1834831  0.8760506 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "   0.5674448  0.16988643 0.5297087...841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "   0.2850487  0.8065831  0.1834831  0.8760506 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a4c50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "   0.5674448  0.16988643 0.52970874...   0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "   0.2850487  0.8065831  0.1834831  0.8760506 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0...75\n",
      "  0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]])\n",
      "        inputs     = [needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 ...5\n",
      "  0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a59d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a4c50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a59d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0...75\n",
      "  0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.28552443  0.12349853  0.14968625 -0.03080189 -0.01552519 -0.14601639\n",
      "  -0.1357518   0.2479918   0.0...968625 -0.03080189 -0.01552519 -0.14601639\n",
      "  -0.1357518   0.2479918   0.02053031  0.24376488  0.24658865  0.20721173]])\n",
      "        bias_ih    = needle.Tensor([[-0.20372711  0.18749413  0.04566535  0.26226556  0.11538979 -0.06687586\n",
      "   0.05412209 -0.18582511  0.2...566535  0.26226556  0.11538979 -0.06687586\n",
      "   0.05412209 -0.18582511  0.25517148 -0.06690054  0.1760174  -0.10565236]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a59d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-8.23050737e-03  1.85497135e-01  2.71834612e-01 -1.94052905e-01\n",
      "   2.49169230e-01 -8.80305022e-02 -2.4...02  3.70212793e-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]])\n",
      "        self       = needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0...75\n",
      "  0.55841863 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 ...2  3.70212793e-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584a56d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 ...2  3.70212793e-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584a56d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5735e6b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd584a6750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5735f630>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd584a6750>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0.58766...3 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]], device=cpu())\n",
      "        b          = NDArray([[-8.23050737e-03  1.85497135e-01  2.71834612e-01 -1.94052905e-01\n",
      "   2.49169230e-01 -8.80305022e-02 -2.4956856...-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584a56d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0.58766...3 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]], device=cpu())\n",
      "        b          = NDArray([[-8.23050737e-03  1.85497135e-01  2.71834612e-01 -1.94052905e-01\n",
      "   2.49169230e-01 -8.80305022e-02 -2.4956856...-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0.58766...3 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]], device=cpu())\n",
      "other = NDArray([[-8.23050737e-03  1.85497135e-01  2.71834612e-01 -1.94052905e-01\n",
      "   2.49169230e-01 -8.80305022e-02 -2.4956856...-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584a5630>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584a7f30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584bba70>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-8.23050737e-03  1.85497135e-01  2.71834612e-01 -1.94052905e-01\n",
      "   2.49169230e-01 -8.80305022e-02 -2.4956856...-02 -2.76089102e-01 -2.25117326e-01\n",
      "   2.32255578e-01 -1.98275760e-01 -1.79769069e-01  1.62344217e-01]], device=cuda())\n",
      "out        = NDArray([[1.35559005e-19 2.59572266e-06 1.72469996e-07 5.31856381e+22\n",
      "  1.50084232e-19 1.03828907e-05 1.06809212e-05 5...280665e-11 9.10653497e-12 1.68972430e-04\n",
      "  1.70894724e-04 1.30280656e-11 1.35688019e-19 4.39120522e-05]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.40565667 0.49171227 0.89939153 0.674164   0.14615087 0.04538301\n",
      "  0.5674448  0.16988643 0.52970874 0.58766...3 0.05109909 0.22352393 0.99768674 0.3648955  0.71734464\n",
      "  0.2850487  0.8065831  0.1834831  0.8760506 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.0411656  -0.10294421  0.02323925  0.9009672   0.14726128\n",
      "    0.38914502  0.49183148  0.536852   -0...38  0.36945263  0.07645454\n",
      "    0.74169576 -0.43820104 -2.4298577   0.2583633  -0.7113952\n",
      "   -1.1055423   0.807849  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.9089125   0.42067912 -1.4763339  -1.1740168   0.3633761\n",
      "    0.5070126  -1.2418844   0.7849431  -0....75  0.9186106   0.9937686\n",
      "    0.7327151   0.4239474  -0.89516526 -0.05240672 -0.17395602\n",
      "    0.7194566  -0.3489981 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5833ec90>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5833ec90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5833ec90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.484...8327 ... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.48.... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5833f010>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.484...8327 ... 0.72419316 0.55043954 0.54696697]\n",
      "  [0.8892988  0.19747463 0.48408327 ... 0.72419316 0.55043954 0.54696697]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0...962\n",
      "  0.69505703 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]])\n",
      "        inputs     = [needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  ...0.69505703 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833c690>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5833f010>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833c690>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0...962\n",
      "  0.69505703 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.00513545 -0.12659544  0.13747698 -0.23411518 -0.17386812 -0.1717987\n",
      "   0.15262908 -0.16374514 -0.23...3747698 -0.23411518 -0.17386812 -0.1717987\n",
      "   0.15262908 -0.16374514 -0.23363137 -0.13382562 -0.2739171  -0.14884725]])\n",
      "        bias_ih    = needle.Tensor([[-0.22140339 -0.20294845  0.10520589 -0.01125321 -0.05759749 -0.11966754\n",
      "   0.05886364  0.12552312 -0.2...520589 -0.01125321 -0.05759749 -0.11966754\n",
      "   0.05886364  0.12552312 -0.25780547  0.05282211  0.03315496 -0.03507078]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5833c690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.17732534 -0.03806248 -0.06140035  0.27041036  0.14066875  0.0311439\n",
      "   0.16086015 -0.11576062  0.23...842017 -0.17399532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]])\n",
      "        self       = needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0...962\n",
      "  0.69505703 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  ...42017 -0.17399532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889ffd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  ...42017 -0.17399532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889ffd0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd57dd91b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5889c710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd57dda570>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5889c710>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0.76655...03 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]], device=cpu())\n",
      "        b          = NDArray([[-0.17732534 -0.03806248 -0.06140035  0.27041036  0.14066875  0.0311439\n",
      "   0.16086015 -0.11576062  0.23879689...532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889ffd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0.76655...03 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]], device=cpu())\n",
      "        b          = NDArray([[-0.17732534 -0.03806248 -0.06140035  0.27041036  0.14066875  0.0311439\n",
      "   0.16086015 -0.11576062  0.23879689...532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0.76655...03 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]], device=cpu())\n",
      "other = NDArray([[-0.17732534 -0.03806248 -0.06140035  0.27041036  0.14066875  0.0311439\n",
      "   0.16086015 -0.11576062  0.23879689...532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5833e670>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5833ebb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561849f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.17732534 -0.03806248 -0.06140035  0.27041036  0.14066875  0.0311439\n",
      "   0.16086015 -0.11576062  0.23879689...532 -0.19453493 -0.05455337\n",
      "   0.23939818  0.11601245  0.19164836  0.20014766 -0.06213316  0.07908016]], device=cuda())\n",
      "out        = NDArray([[1.35558992e-19 1.62232482e-07 1.73752094e-04 5.20050555e+22\n",
      "  1.50084232e-19 4.15371505e-05 6.56395969e-07 5...296217e-11 1.35688019e-19 4.21843573e-08\n",
      "  1.07811982e-08 1.30289555e-11 1.35688019e-19 1.07207132e-08]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.8892988  0.19747463 0.48408327 0.6866353  0.5003645  0.73638695\n",
      "  0.70346355 0.739025   0.5768624  0.76655...03 0.9212132  0.8908848  0.51180613 0.38607174 0.5935576\n",
      "  0.5356052  0.72419316 0.55043954 0.54696697]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 9.55141842e-01 -8.75577748e-01  5.83077788e-01 -7.61657476e-01\n",
      "   -1.20011151e+00 -4.76980925e-01 -9... -1.05142641e+00  9.08979893e-01  5.17566800e-01\n",
      "    1.31425619e+00  5.84762335e-01 -6.33905590e-01 -3.19953859e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 2.4105674e-03 -1.0222628e+00  2.6881242e-01 -1.3244623e-02\n",
      "    1.6903455e+00 -5.1111406e-01 -1.25849...113e-01 -4.4830972e-01  2.5549648e+00 -1.1994240e+00\n",
      "    1.7475673e+00  3.8417187e-01 -1.3600128e+00 -7.4062115e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56fad050>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56fad050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56fad050>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "   0.08262206 0.1991176  0.60881245...   0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "   0.9811255  0.9944539  0.09422781 0.53214514]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "   0.08262206 0.1991176  0.6088124...275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "   0.9811255  0.9944539  0.09422781 0.53214514]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56fae890>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "   0.08262206 0.1991176  0.60881245...   0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "   0.9811255  0.9944539  0.09422781 0.53214514]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0...44\n",
      "  0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]])\n",
      "        inputs     = [needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 ...4\n",
      "  0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56fad150>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56fae890>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56fad150>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0...44\n",
      "  0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.231727   -0.18911386 -0.09352778  0.1775445  -0.08397821  0.0179435\n",
      "  -0.03254136  0.22826296  0.13...9352778  0.1775445  -0.08397821  0.0179435\n",
      "  -0.03254136  0.22826296  0.1323258   0.15470076 -0.08513376  0.11368936]])\n",
      "        bias_ih    = needle.Tensor([[-0.19867861 -0.25293115  0.2870627   0.04154488 -0.24256176  0.11808807\n",
      "  -0.12726517 -0.25749242 -0.2...70627   0.04154488 -0.24256176  0.11808807\n",
      "  -0.12726517 -0.25749242 -0.2087134  -0.03313664  0.04385263  0.06208944]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56fad150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.17856455  0.23345351 -0.04778883 -0.11852325  0.24639732 -0.1866403\n",
      "   0.03497079  0.24051267 -0.26...986421 -0.10884942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]])\n",
      "        self       = needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0...44\n",
      "  0.48275796 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 ...86421 -0.10884942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56facb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 ...86421 -0.10884942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56facb50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd57a53330>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd56faed50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd57a51ff0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd56faed50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0.56129...6 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]], device=cpu())\n",
      "        b          = NDArray([[ 0.17856455  0.23345351 -0.04778883 -0.11852325  0.24639732 -0.1866403\n",
      "   0.03497079  0.24051267 -0.2617378 ...942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56facb50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0.56129...6 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]], device=cpu())\n",
      "        b          = NDArray([[ 0.17856455  0.23345351 -0.04778883 -0.11852325  0.24639732 -0.1866403\n",
      "   0.03497079  0.24051267 -0.2617378 ...942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0.56129...6 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]], device=cpu())\n",
      "other = NDArray([[ 0.17856455  0.23345351 -0.04778883 -0.11852325  0.24639732 -0.1866403\n",
      "   0.03497079  0.24051267 -0.2617378 ...942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56fad930>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56fac5f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56faee30>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.17856455  0.23345351 -0.04778883 -0.11852325  0.24639732 -0.1866403\n",
      "   0.03497079  0.24051267 -0.2617378 ...942  0.23031831  0.18182266\n",
      "  -0.25573653  0.05655012 -0.12384093  0.102525    0.2048766   0.22662163]], device=cuda())\n",
      "out        = NDArray([[8.83280231e-35 0.00000000e+00 2.32063891e-34 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0...000000e+00 1.20371762e-38 0.00000000e+00\n",
      "  1.24666238e-38 0.00000000e+00 1.20371762e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.27931082 0.09277241 0.43405938 0.44539443 0.0030992  0.31167918\n",
      "  0.08262206 0.1991176  0.60881245 0.56129...6 0.1099445  0.180871   0.8069931  0.64490086 0.01386251\n",
      "  0.9811255  0.9944539  0.09422781 0.53214514]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-5.85732698e-01  6.46381617e-01  1.22066140e+00  1.11173069e+00\n",
      "   -5.45193493e-01 -8.93237233e-01 -6... -1.24482071e+00 -1.24177039e+00  5.34085155e-01\n",
      "   -1.22718394e+00  5.76861918e-01  2.31532753e-01  2.47476697e+00]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.63801801e+00 -4.28563565e-01  7.15819061e-01  1.04613729e-01\n",
      "    1.09301519e+00  6.31318510e-01  3... -3.55584085e-01 -2.41119266e-01  1.48845291e+00\n",
      "    2.48230368e-01  9.02382582e-02 -1.35742366e+00  5.45893013e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56411210>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0..... 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56411210>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56411210>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.719...7284 ... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.71.... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56411590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.719...7284 ... 0.01347695 0.69768375 0.8631984 ]\n",
      "  [0.42512298 0.5227543  0.71917284 ... 0.01347695 0.69768375 0.8631984 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0....474\n",
      "  0.45416498 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]])\n",
      "        inputs     = [needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0...0.45416498 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412710>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56411590>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0....474\n",
      "  0.45416498 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.02494046  0.09409451 -0.13131572 -0.04928392  0.27999812 -0.15736076\n",
      "  -0.14699051 -0.14455709  0.0...131572 -0.04928392  0.27999812 -0.15736076\n",
      "  -0.14699051 -0.14455709  0.05895525 -0.02381757  0.25747555 -0.23027703]])\n",
      "        bias_ih    = needle.Tensor([[-0.19035667  0.02465039  0.1405122  -0.23533839  0.2722857   0.2841491\n",
      "  -0.16122165 -0.28208697  0.13...405122  -0.23533839  0.2722857   0.2841491\n",
      "  -0.16122165 -0.28208697  0.1308575   0.00119671  0.19367352  0.20846027]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56412710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.64962345e-01  2.72953689e-01  2.46466339e-01 -2.09873065e-01\n",
      "  -2.72579372e-01 -8.66143703e-02  3.9...02 -2.04587325e-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]])\n",
      "        self       = needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0....474\n",
      "  0.45416498 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0...2 -2.04587325e-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830cad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0...2 -2.04587325e-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830cad0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd584a60f0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5830cbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd584a4270>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5830cbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0.347614...98 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]], device=cpu())\n",
      "        b          = NDArray([[-2.64962345e-01  2.72953689e-01  2.46466339e-01 -2.09873065e-01\n",
      "  -2.72579372e-01 -8.66143703e-02  3.9768904...-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830cad0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0.347614...98 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]], device=cpu())\n",
      "        b          = NDArray([[-2.64962345e-01  2.72953689e-01  2.46466339e-01 -2.09873065e-01\n",
      "  -2.72579372e-01 -8.66143703e-02  3.9768904...-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0.347614...98 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]], device=cpu())\n",
      "other = NDArray([[-2.64962345e-01  2.72953689e-01  2.46466339e-01 -2.09873065e-01\n",
      "  -2.72579372e-01 -8.66143703e-02  3.9768904...-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5830dbb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56411730>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5830f470>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-2.64962345e-01  2.72953689e-01  2.46466339e-01 -2.09873065e-01\n",
      "  -2.72579372e-01 -8.66143703e-02  3.9768904...-01  1.92521960e-01 -9.44881886e-02\n",
      "   3.49095166e-02  2.10412979e-01  1.48359030e-01 -1.02748066e-01]], device=cuda())\n",
      "out        = NDArray([[9.3307068e-35 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.42512298 0.5227543  0.71917284 0.8062005  0.914909   0.4536567\n",
      "  0.01101372 0.32550827 0.6159125  0.347614...98 0.6132816  0.7765661  0.97906244 0.30535087 0.6693337\n",
      "  0.05388917 0.01347695 0.69768375 0.8631984 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.6366559]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.8841032]]])\n",
      "h0         = needle.Tensor([[[-0.8841032]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd585a1a50>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[621.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[621.]]), needle.Tensor([[[-0.8841032]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585a1a50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.8841032]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd585a1a50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[621.]])\n",
      "        x_emb      = needle.Tensor([[[0.6132043]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6132043]]]), needle.Tensor([[[-0.8841032]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585a0b90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6132043]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.8841032]])\n",
      "        h0         = (needle.Tensor([[-0.8841032]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6132043]])\n",
      "        inputs     = [needle.Tensor([[0.6132043]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a1c90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585a0b90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6132043]]), needle.Tensor([[-0.8841032]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a1c90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6132043]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.29222322]])\n",
      "        bias_ih    = needle.Tensor([[0.2534834]])\n",
      "        h          = needle.Tensor([[-0.8841032]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585a1c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.52484083]])\n",
      "        self       = needle.Tensor([[0.6132043]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6132043]]), needle.Tensor([[-0.52484083]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5834a950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6132043]]), needle.Tensor([[-0.52484083]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5834a950>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd582c26f0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5834b190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd582c19b0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5834b190>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6132043]], device=cpu())\n",
      "        b          = NDArray([[-0.52484083]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5834a950>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6132043]], device=cpu())\n",
      "        b          = NDArray([[-0.52484083]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6132043]], device=cpu())\n",
      "other = NDArray([[-0.52484083]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585a0930>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585a1b70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58349830>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.52484083]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.6132043]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.5717356]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[0.31406358]]])\n",
      "h0         = needle.Tensor([[[0.31406358]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd55dc6290>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[967.],\n",
      "       [279.],\n",
      "       [427.],\n",
      "       [  6.],\n",
      "       [487.],\n",
      "       [927.],\n",
      "       [355.],\n",
      "       [428.],\n",
      "       [805.],\n",
      "       [591.],\n",
      "       [190.],\n",
      "       [949.],\n",
      "       [726.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[967.]\n",
      " [279.]\n",
      " [427.]\n",
      " [  6.]\n",
      " [487.]\n",
      " [927.]\n",
      " [355.]\n",
      " [428.]\n",
      " [805.]\n",
      " [591.]\n",
      " [190.]\n",
      " [949.]\n",
      " [726.]]), needle.Tensor([[[0.31406358]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55dc6290>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[0.31406358]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd55dc6290>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[967.]\n",
      " [279.]\n",
      " [427.]\n",
      " [  6.]\n",
      " [487.]\n",
      " [927.]\n",
      " [355.]\n",
      " [428.]\n",
      " [805.]\n",
      " [591.]\n",
      " [190.]\n",
      " [949.]\n",
      " [726.]])\n",
      "        x_emb      = needle.Tensor([[[0.7191703 ]]\n",
      "\n",
      " [[0.0262253 ]]\n",
      "\n",
      " [[0.69735575]]\n",
      "\n",
      " [[0.12426338]]\n",
      "\n",
      " [[0.07905525]]\n",
      "\n",
      " [[0.71466345]]\n",
      "\n",
      " [[0.73380035]]\n",
      "\n",
      " [[0.35903457]]\n",
      "\n",
      " [[0.94926137]]\n",
      "\n",
      " [[0.03535902]]\n",
      "\n",
      " [[0.76355577]]\n",
      "\n",
      " [[0.14190163]]\n",
      "\n",
      " [[0.39056003]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7191703 ]]\n",
      "\n",
      " [[0.0262253 ]]\n",
      "\n",
      " [[0.69735575]]\n",
      "\n",
      " [[0.12426338]]\n",
      "\n",
      " [[0.07905525]]\n",
      "\n",
      " [[0.71466345]]\n",
      "\n",
      " ... [[0.94926137]]\n",
      "\n",
      " [[0.03535902]]\n",
      "\n",
      " [[0.76355577]]\n",
      "\n",
      " [[0.14190163]]\n",
      "\n",
      " [[0.39056003]]]), needle.Tensor([[[0.31406358]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55dc5fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7191703 ]]\n",
      "\n",
      " [[0.0262253 ]]\n",
      "\n",
      " [[0.69735575]]\n",
      "\n",
      " [[0.12426338]]\n",
      "\n",
      " [[0.07905525]]\n",
      "\n",
      " [[0.71466345]]\n",
      "\n",
      " [[0.73380035]]\n",
      "\n",
      " [[0.35903457]]\n",
      "\n",
      " [[0.94926137]]\n",
      "\n",
      " [[0.03535902]]\n",
      "\n",
      " [[0.76355577]]\n",
      "\n",
      " [[0.14190163]]\n",
      "\n",
      " [[0.39056003]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.31406358]])\n",
      "        h0         = (needle.Tensor([[0.31406358]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7191703]])\n",
      "        inputs     = [needle.Tensor([[0.7191703]]), needle.Tensor([[0.0262253]]), needle.Tensor([[0.69735575]]), needle.Tensor([[0.12426338]]), needle.Tensor([[0.07905525]]), needle.Tensor([[0.71466345]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55dc7310>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55dc5fd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7191703]]), needle.Tensor([[0.31406358]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55dc7310>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7191703]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.04787868]])\n",
      "        bias_ih    = needle.Tensor([[-0.6755509]])\n",
      "        h          = needle.Tensor([[0.31406358]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55dc7310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.9232812]])\n",
      "        self       = needle.Tensor([[0.7191703]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7191703]]), needle.Tensor([[0.9232812]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58696890>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7191703]]), needle.Tensor([[0.9232812]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58696890>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd561c3db0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58694dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd561c2570>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd58694dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7191703]], device=cpu())\n",
      "        b          = NDArray([[0.9232812]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58696890>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7191703]], device=cpu())\n",
      "        b          = NDArray([[0.9232812]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7191703]], device=cpu())\n",
      "other = NDArray([[0.9232812]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55dc4cb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55dc77f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58697030>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.9232812]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.7191703]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.8706128]]\n",
      "\n",
      " [[-0.7482341]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[1.2044305]]\n",
      "\n",
      " [[1.4358549]]])\n",
      "h0         = needle.Tensor([[[1.2044305]]\n",
      "\n",
      " [[1.4358549]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd572b3350>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[388.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[388.]]), needle.Tensor([[[1.2044305]]\n",
      "\n",
      " [[1.4358549]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd572b3350>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[1.2044305]]\n",
      "\n",
      " [[1.4358549]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd572b3350>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[388.]])\n",
      "        x_emb      = needle.Tensor([[[0.25403953]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.25403953]]]), needle.Tensor([[[1.2044305]]\n",
      "\n",
      " [[1.4358549]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd572b3590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.25403953]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[1.2044305]])\n",
      "        h0         = (needle.Tensor([[1.2044305]]), needle.Tensor([[1.4358549]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.25403953]])\n",
      "        inputs     = [needle.Tensor([[0.25403953]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b3f50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd572b3590>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.25403953]]), needle.Tensor([[1.2044305]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b3f50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.25403953]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.18922627]])\n",
      "        bias_ih    = needle.Tensor([[0.00794256]])\n",
      "        h          = needle.Tensor([[1.2044305]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b3f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.19938314]])\n",
      "        self       = needle.Tensor([[0.25403953]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.25403953]]), needle.Tensor([[-0.19938314]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.25403953]]), needle.Tensor([[-0.19938314]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668ed0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd58772830>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5866b690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd58770eb0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5866b690>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.25403953]], device=cpu())\n",
      "        b          = NDArray([[-0.19938314]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58668ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.25403953]], device=cpu())\n",
      "        b          = NDArray([[-0.19938314]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.25403953]], device=cpu())\n",
      "other = NDArray([[-0.19938314]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd572b1ef0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd572b0eb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58669670>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.19938314]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.25403953]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.0129285]]\n",
      "\n",
      " [[-2.0597281]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 1.2300818 ]]\n",
      "\n",
      " [[-0.66841525]]])\n",
      "h0         = needle.Tensor([[[ 1.2300818 ]]\n",
      "\n",
      " [[-0.66841525]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd576d8790>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[206.],\n",
      "       [430.],\n",
      "       [709.],\n",
      "       [723.],\n",
      "       [700.],\n",
      "       [785.],\n",
      "       [678.],\n",
      "       [730.],\n",
      "       [718.],\n",
      "       [177.],\n",
      "       [269.],\n",
      "       [106.],\n",
      "       [176.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[206.]\n",
      " [430.]\n",
      " [709.]\n",
      " [723.]\n",
      " [700.]\n",
      " [785.]\n",
      " [678.]\n",
      " [730.]\n",
      " [718.]\n",
      " [177.]\n",
      " [269.]\n",
      " [106.]\n",
      " [176.]]), needle.Tensor([[[ 1.2300818 ]]\n",
      "\n",
      " [[-0.66841525]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd576d8790>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 1.2300818 ]]\n",
      "\n",
      " [[-0.66841525]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd576d8790>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[206.]\n",
      " [430.]\n",
      " [709.]\n",
      " [723.]\n",
      " [700.]\n",
      " [785.]\n",
      " [678.]\n",
      " [730.]\n",
      " [718.]\n",
      " [177.]\n",
      " [269.]\n",
      " [106.]\n",
      " [176.]])\n",
      "        x_emb      = needle.Tensor([[[0.5643545 ]]\n",
      "\n",
      " [[0.7555314 ]]\n",
      "\n",
      " [[0.57402384]]\n",
      "\n",
      " [[0.2415693 ]]\n",
      "\n",
      " [[0.72738534]]\n",
      "\n",
      " [[0.45416495]]\n",
      "\n",
      " [[0.8929539 ]]\n",
      "\n",
      " [[0.6704713 ]]\n",
      "\n",
      " [[0.0868278 ]]\n",
      "\n",
      " [[0.67171067]]\n",
      "\n",
      " [[0.57901716]]\n",
      "\n",
      " [[0.9649554 ]]\n",
      "\n",
      " [[0.52476525]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5643545 ]]\n",
      "\n",
      " [[0.7555314 ]]\n",
      "\n",
      " [[0.57402384]]\n",
      "\n",
      " [[0.2415693 ]]\n",
      "\n",
      " [[0.72738534]]\n",
      "\n",
      " [[0.45416495]]\n",
      "\n",
      " ...[0.67171067]]\n",
      "\n",
      " [[0.57901716]]\n",
      "\n",
      " [[0.9649554 ]]\n",
      "\n",
      " [[0.52476525]]]), needle.Tensor([[[ 1.2300818 ]]\n",
      "\n",
      " [[-0.66841525]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576d9e90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5643545 ]]\n",
      "\n",
      " [[0.7555314 ]]\n",
      "\n",
      " [[0.57402384]]\n",
      "\n",
      " [[0.2415693 ]]\n",
      "\n",
      " [[0.72738534]]\n",
      "\n",
      " [[0.45416495]]\n",
      "\n",
      " [[0.8929539 ]]\n",
      "\n",
      " [[0.6704713 ]]\n",
      "\n",
      " [[0.0868278 ]]\n",
      "\n",
      " [[0.67171067]]\n",
      "\n",
      " [[0.57901716]]\n",
      "\n",
      " [[0.9649554 ]]\n",
      "\n",
      " [[0.52476525]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[1.2300818]])\n",
      "        h0         = (needle.Tensor([[1.2300818]]), needle.Tensor([[-0.66841525]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5643545]])\n",
      "        inputs     = [needle.Tensor([[0.5643545]]), needle.Tensor([[0.7555314]]), needle.Tensor([[0.57402384]]), needle.Tensor([[0.2415693]]), needle.Tensor([[0.72738534]]), needle.Tensor([[0.45416495]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576d96d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576d9e90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5643545]]), needle.Tensor([[1.2300818]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576d96d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5643545]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.8871541]])\n",
      "        bias_ih    = needle.Tensor([[-0.45130485]])\n",
      "        h          = needle.Tensor([[1.2300818]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576d96d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.39463723]])\n",
      "        self       = needle.Tensor([[0.5643545]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5643545]]), needle.Tensor([[0.39463723]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889f290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5643545]]), needle.Tensor([[0.39463723]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889f290>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5608bcf0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5889e550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd56088eb0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd5889e550>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5643545]], device=cpu())\n",
      "        b          = NDArray([[0.39463723]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5889f290>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5643545]], device=cpu())\n",
      "        b          = NDArray([[0.39463723]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5643545]], device=cpu())\n",
      "other = NDArray([[0.39463723]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56485470>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd576d81b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5889fcb0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.39463723]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.5643545]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.10077927]\n",
      "  [-1.4521494 ]\n",
      "  [-1.5829487 ]\n",
      "  [ 0.03144265]\n",
      "  [ 0.27373177]\n",
      "  [ 1.4618765 ]\n",
      "  [-0.22...72 ]\n",
      "  [ 0.7116055 ]\n",
      "  [-0.16807163]\n",
      "  [ 0.5689088 ]\n",
      "  [-1.1826497 ]\n",
      "  [-0.09682417]\n",
      "  [-0.26057675]\n",
      "  [-0.47927415]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.45684704]\n",
      "  [ 0.17711088]\n",
      "  [-0.7442306 ]\n",
      "  [ 1.0187736 ]\n",
      "  [ 0.13755547]\n",
      "  [ 0.20009604]\n",
      "  [-1.41...738]\n",
      "  [-1.0635487 ]\n",
      "  [ 0.3315099 ]\n",
      "  [-1.1081563 ]\n",
      "  [-0.11435709]\n",
      "  [ 0.30931923]\n",
      "  [-0.57166034]\n",
      "  [-0.38600755]]])\n",
      "h0         = needle.Tensor([[[-0.45684704]\n",
      "  [ 0.17711088]\n",
      "  [-0.7442306 ]\n",
      "  [ 1.0187736 ]\n",
      "  [ 0.13755547]\n",
      "  [ 0.20009604]\n",
      "  [-1.41...738]\n",
      "  [-1.0635487 ]\n",
      "  [ 0.3315099 ]\n",
      "  [-1.1081563 ]\n",
      "  [-0.11435709]\n",
      "  [ 0.30931923]\n",
      "  [-0.57166034]\n",
      "  [-0.38600755]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5708bd90>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[266., 434., 512., 989., 730., 997., 755., 142., 784., 887., 464.,\n",
      "        323., 255., 175.,  71.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[266. 434. 512. 989. 730. 997. 755. 142. 784. 887. 464. 323. 255. 175.\n",
      "   71.]]), needle.Tensor([[[-0....38]\n",
      "  [-1.0635487 ]\n",
      "  [ 0.3315099 ]\n",
      "  [-1.1081563 ]\n",
      "  [-0.11435709]\n",
      "  [ 0.30931923]\n",
      "  [-0.57166034]\n",
      "  [-0.38600755]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5708bd90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.45684704]\n",
      "  [ 0.17711088]\n",
      "  [-0.7442306 ]\n",
      "  [ 1.0187736 ]\n",
      "  [ 0.13755547]\n",
      "  [ 0.20009604]\n",
      "  [-1.41...738]\n",
      "  [-1.0635487 ]\n",
      "  [ 0.3315099 ]\n",
      "  [-1.1081563 ]\n",
      "  [-0.11435709]\n",
      "  [ 0.30931923]\n",
      "  [-0.57166034]\n",
      "  [-0.38600755]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd5708bd90>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[266. 434. 512. 989. 730. 997. 755. 142. 784. 887. 464. 323. 255. 175.\n",
      "   71.]])\n",
      "        x_emb      = needle.Tensor([[[0.29004085]\n",
      "  [0.10235576]\n",
      "  [0.4692803 ]\n",
      "  [0.08036542]\n",
      "  [0.3441372 ]\n",
      "  [0.29411444]\n",
      "  [0.9015218 ]...0.6656695 ]\n",
      "  [0.6192142 ]\n",
      "  [0.8654433 ]\n",
      "  [0.20468777]\n",
      "  [0.8332285 ]\n",
      "  [0.8825629 ]\n",
      "  [0.80284774]\n",
      "  [0.8501406 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.29004085]\n",
      "  [0.10235576]\n",
      "  [0.4692803 ]\n",
      "  [0.08036542]\n",
      "  [0.3441372 ]\n",
      "  [0.29411444]\n",
      "  [0.9015218 ...38]\n",
      "  [-1.0635487 ]\n",
      "  [ 0.3315099 ]\n",
      "  [-1.1081563 ]\n",
      "  [-0.11435709]\n",
      "  [ 0.30931923]\n",
      "  [-0.57166034]\n",
      "  [-0.38600755]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5708bb10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.29004085]\n",
      "  [0.10235576]\n",
      "  [0.4692803 ]\n",
      "  [0.08036542]\n",
      "  [0.3441372 ]\n",
      "  [0.29411444]\n",
      "  [0.9015218 ]...0.6656695 ]\n",
      "  [0.6192142 ]\n",
      "  [0.8654433 ]\n",
      "  [0.20468777]\n",
      "  [0.8332285 ]\n",
      "  [0.8825629 ]\n",
      "  [0.80284774]\n",
      "  [0.8501406 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.45684704]\n",
      " [ 0.17711088]\n",
      " [-0.7442306 ]\n",
      " [ 1.0187736 ]\n",
      " [ 0.13755547]\n",
      " [ 0.20009604]\n",
      " [-1.4174297 ]\n",
      " [ 0.30624738]\n",
      " [-1.0635487 ]\n",
      " [ 0.3315099 ]\n",
      " [-1.1081563 ]\n",
      " [-0.11435709]\n",
      " [ 0.30931923]\n",
      " [-0.57166034]\n",
      " [-0.38600755]])\n",
      "        h0         = (needle.Tensor([[-0.45684704]\n",
      " [ 0.17711088]\n",
      " [-0.7442306 ]\n",
      " [ 1.0187736 ]\n",
      " [ 0.13755547]\n",
      " [ 0.20009604]\n",
      " [-1.4174297 ....30624738]\n",
      " [-1.0635487 ]\n",
      " [ 0.3315099 ]\n",
      " [-1.1081563 ]\n",
      " [-0.11435709]\n",
      " [ 0.30931923]\n",
      " [-0.57166034]\n",
      " [-0.38600755]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]])\n",
      "        inputs     = [needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5708bb10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6...0.30624738]\n",
      " [-1.0635487 ]\n",
      " [ 0.3315099 ]\n",
      " [-1.1081563 ]\n",
      " [-0.11435709]\n",
      " [ 0.30931923]\n",
      " [-0.57166034]\n",
      " [-0.38600755]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]\n",
      " [0.75409544]])\n",
      "        bias_ih    = needle.Tensor([[-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]\n",
      " [-0.27774727]])\n",
      "        h          = needle.Tensor([[-0.45684704]\n",
      " [ 0.17711088]\n",
      " [-0.7442306 ]\n",
      " [ 1.0187736 ]\n",
      " [ 0.13755547]\n",
      " [ 0.20009604]\n",
      " [-1.4174297 ]\n",
      " [ 0.30624738]\n",
      " [-1.0635487 ]\n",
      " [ 0.3315099 ]\n",
      " [-1.1081563 ]\n",
      " [-0.11435709]\n",
      " [ 0.30931923]\n",
      " [-0.57166034]\n",
      " [-0.38600755]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.13192457]])\n",
      "        self       = needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6...\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]]), needle.Tensor([[-0.13192457]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708ac50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6...\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]]), needle.Tensor([[-0.13192457]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708ac50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd573398b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5708b090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5733a2b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5708b090>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]], device=cpu())\n",
      "        b          = NDArray([[-0.13192457]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708ac50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]], device=cpu())\n",
      "        b          = NDArray([[-0.13192457]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]], device=cpu())\n",
      "other = NDArray([[-0.13192457]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd570881f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd570888f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57089e70>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.13192457]], device=cuda())\n",
      "out        = NDArray([[7.9424227e-35]\n",
      " [0.0000000e+00]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985...]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]\n",
      " [2.3985544e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.29004085]\n",
      " [0.10235576]\n",
      " [0.4692803 ]\n",
      " [0.08036542]\n",
      " [0.3441372 ]\n",
      " [0.29411444]\n",
      " [0.9015218 ]\n",
      " [0.6656695 ]\n",
      " [0.6192142 ]\n",
      " [0.8654433 ]\n",
      " [0.20468777]\n",
      " [0.8332285 ]\n",
      " [0.8825629 ]\n",
      " [0.80284774]\n",
      " [0.8501406 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.10040045]\n",
      "  [ 1.7571777 ]\n",
      "  [-1.2884625 ]\n",
      "  [ 0.9814128 ]\n",
      "  [-2.4968827 ]\n",
      "  [-1.0953376 ]\n",
      "  [-1.24...715]\n",
      "  [ 0.85744435]\n",
      "  [ 0.28652218]\n",
      "  [-0.9010759 ]\n",
      "  [-0.37167647]\n",
      "  [ 1.3035501 ]\n",
      "  [ 0.8585697 ]\n",
      "  [ 0.35079747]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.9050186 ]\n",
      "  [-1.6003845 ]\n",
      "  [ 0.41478512]\n",
      "  [-1.2113546 ]\n",
      "  [-0.96696705]\n",
      "  [-0.6738001 ]\n",
      "  [-0.79...14 ]\n",
      "  [ 1.9517137 ]\n",
      "  [ 0.39359283]\n",
      "  [-1.5533065 ]\n",
      "  [-0.84751844]\n",
      "  [ 0.4030444 ]\n",
      "  [-1.0665674 ]\n",
      "  [ 0.4208941 ]]])\n",
      "h0         = needle.Tensor([[[-0.9050186 ]\n",
      "  [-1.6003845 ]\n",
      "  [ 0.41478512]\n",
      "  [-1.2113546 ]\n",
      "  [-0.96696705]\n",
      "  [-0.6738001 ]\n",
      "  [-0.79...14 ]\n",
      "  [ 1.9517137 ]\n",
      "  [ 0.39359283]\n",
      "  [-1.5533065 ]\n",
      "  [-0.84751844]\n",
      "  [ 0.4030444 ]\n",
      "  [-1.0665674 ]\n",
      "  [ 0.4208941 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58697050>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[549., 690.,  35.,  87., 545.,  41., 690., 378., 878., 227.,  64.,\n",
      "        266.,  13., 765., 410.],\n",
      "       [844...    [386., 926., 588., 578., 723., 156., 831., 717., 341., 287., 270.,\n",
      "        907., 531., 626., 984.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[549. 690.  35.  87. 545.  41. 690. 378. 878. 227.  64. 266.  13. 765.\n",
      "  410.]\n",
      " [844. 831. 495. 908. 2...4 ]\n",
      "  [ 1.9517137 ]\n",
      "  [ 0.39359283]\n",
      "  [-1.5533065 ]\n",
      "  [-0.84751844]\n",
      "  [ 0.4030444 ]\n",
      "  [-1.0665674 ]\n",
      "  [ 0.4208941 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58697050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.9050186 ]\n",
      "  [-1.6003845 ]\n",
      "  [ 0.41478512]\n",
      "  [-1.2113546 ]\n",
      "  [-0.96696705]\n",
      "  [-0.6738001 ]\n",
      "  [-0.79...14 ]\n",
      "  [ 1.9517137 ]\n",
      "  [ 0.39359283]\n",
      "  [-1.5533065 ]\n",
      "  [-0.84751844]\n",
      "  [ 0.4030444 ]\n",
      "  [-1.0665674 ]\n",
      "  [ 0.4208941 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58697050>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[549. 690.  35.  87. 545.  41. 690. 378. 878. 227.  64. 266.  13. 765.\n",
      "  410.]\n",
      " [844. 831. 495. 908. 21...415. 404. 736. 895. 776. 322.\n",
      "  591.]\n",
      " [386. 926. 588. 578. 723. 156. 831. 717. 341. 287. 270. 907. 531. 626.\n",
      "  984.]])\n",
      "        x_emb      = needle.Tensor([[[7.02781022e-01]\n",
      "  [1.04743175e-01]\n",
      "  [4.79657531e-01]\n",
      "  [1.52081341e-01]\n",
      "  [2.23776311e-01]\n",
      "  [9.5635...1]\n",
      "  [2.13393465e-01]\n",
      "  [1.18558019e-01]\n",
      "  [3.36955279e-01]\n",
      "  [5.72321892e-01]\n",
      "  [3.61008614e-01]\n",
      "  [1.74903274e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[7.02781022e-01]\n",
      "  [1.04743175e-01]\n",
      "  [4.79657531e-01]\n",
      "  [1.52081341e-01]\n",
      "  [2.23776311e-01]\n",
      "  [9.563...4 ]\n",
      "  [ 1.9517137 ]\n",
      "  [ 0.39359283]\n",
      "  [-1.5533065 ]\n",
      "  [-0.84751844]\n",
      "  [ 0.4030444 ]\n",
      "  [-1.0665674 ]\n",
      "  [ 0.4208941 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58696fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[7.02781022e-01]\n",
      "  [1.04743175e-01]\n",
      "  [4.79657531e-01]\n",
      "  [1.52081341e-01]\n",
      "  [2.23776311e-01]\n",
      "  [9.5635...1]\n",
      "  [2.13393465e-01]\n",
      "  [1.18558019e-01]\n",
      "  [3.36955279e-01]\n",
      "  [5.72321892e-01]\n",
      "  [3.61008614e-01]\n",
      "  [1.74903274e-01]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.9050186 ]\n",
      " [-1.6003845 ]\n",
      " [ 0.41478512]\n",
      " [-1.2113546 ]\n",
      " [-0.96696705]\n",
      " [-0.6738001 ]\n",
      " [-0.7948589 ]\n",
      " [ 0.6088914 ]\n",
      " [ 1.9517137 ]\n",
      " [ 0.39359283]\n",
      " [-1.5533065 ]\n",
      " [-0.84751844]\n",
      " [ 0.4030444 ]\n",
      " [-1.0665674 ]\n",
      " [ 0.4208941 ]])\n",
      "        h0         = (needle.Tensor([[-0.9050186 ]\n",
      " [-1.6003845 ]\n",
      " [ 0.41478512]\n",
      " [-1.2113546 ]\n",
      " [-0.96696705]\n",
      " [-0.6738001 ]\n",
      " [-0.7948589 ....6088914 ]\n",
      " [ 1.9517137 ]\n",
      " [ 0.39359283]\n",
      " [-1.5533065 ]\n",
      " [-0.84751844]\n",
      " [ 0.4030444 ]\n",
      " [-1.0665674 ]\n",
      " [ 0.4208941 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]])\n",
      "        inputs     = [needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.7... [0.2616488 ]\n",
      " [0.44066325]\n",
      " [0.7432014 ]\n",
      " [0.2300779 ]\n",
      " [0.48703066]\n",
      " [0.6296374 ]\n",
      " [0.5370751 ]\n",
      " [0.13394   ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58697c10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58696fd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.7...0.6088914 ]\n",
      " [ 1.9517137 ]\n",
      " [ 0.39359283]\n",
      " [-1.5533065 ]\n",
      " [-0.84751844]\n",
      " [ 0.4030444 ]\n",
      " [-1.0665674 ]\n",
      " [ 0.4208941 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58697c10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]\n",
      " [-0.39526898]])\n",
      "        bias_ih    = needle.Tensor([[0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]\n",
      " [0.89694]])\n",
      "        h          = needle.Tensor([[-0.9050186 ]\n",
      " [-1.6003845 ]\n",
      " [ 0.41478512]\n",
      " [-1.2113546 ]\n",
      " [-0.96696705]\n",
      " [-0.6738001 ]\n",
      " [-0.7948589 ]\n",
      " [ 0.6088914 ]\n",
      " [ 1.9517137 ]\n",
      " [ 0.39359283]\n",
      " [-1.5533065 ]\n",
      " [-0.84751844]\n",
      " [ 0.4030444 ]\n",
      " [-1.0665674 ]\n",
      " [ 0.4208941 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58697c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.11701405]])\n",
      "        self       = needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.7...]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]]), needle.Tensor([[0.11701405]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57124ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.7...]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]]), needle.Tensor([[0.11701405]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57124ad0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578ad270>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57124890>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd578af2f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57124890>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]], device=cpu())\n",
      "        b          = NDArray([[0.11701405]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57124ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]], device=cpu())\n",
      "        b          = NDArray([[0.11701405]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]], device=cpu())\n",
      "other = NDArray([[0.11701405]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580117f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58694db0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57125f70>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.11701405]], device=cuda())\n",
      "out        = NDArray([[3.1171587e-34]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.702781  ]\n",
      " [0.10474318]\n",
      " [0.47965753]\n",
      " [0.15208134]\n",
      " [0.22377631]\n",
      " [0.9563564 ]\n",
      " [0.10474318]\n",
      " [0.79313207]\n",
      " [0.8090025 ]\n",
      " [0.22157854]\n",
      " [0.29971942]\n",
      " [0.64832574]\n",
      " [0.88288003]\n",
      " [0.97450763]\n",
      " [0.78528106]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.9306844 ]\n",
      "  [ 2.347734  ]\n",
      "  [-1.4428575 ]\n",
      "  [-0.8469262 ]\n",
      "  [ 0.44492546]\n",
      "  [ 0.75046885]\n",
      "  [-1.39...57 ]\n",
      "  [-0.5098991 ]\n",
      "  [ 0.01659212]\n",
      "  [ 0.58838326]\n",
      "  [-0.80546993]\n",
      "  [ 0.41699305]\n",
      "  [ 0.18143141]\n",
      "  [ 0.19619872]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.76160806]\n",
      "  [-0.7353778 ]\n",
      "  [ 1.6336391 ]\n",
      "  [-0.54160273]\n",
      "  [-1.4369678 ]\n",
      "  [-0.9993193 ]\n",
      "  [-0.38...029]\n",
      "  [-1.2453793 ]\n",
      "  [ 2.0987017 ]\n",
      "  [ 0.77608484]\n",
      "  [ 1.3148452 ]\n",
      "  [ 0.7334375 ]\n",
      "  [-0.90711284]\n",
      "  [ 0.06042899]]])\n",
      "h0         = needle.Tensor([[[-0.76160806]\n",
      "  [-0.7353778 ]\n",
      "  [ 1.6336391 ]\n",
      "  [-0.54160273]\n",
      "  [-1.4369678 ]\n",
      "  [-0.9993193 ]\n",
      "  [-0.38...029]\n",
      "  [-1.2453793 ]\n",
      "  [ 2.0987017 ]\n",
      "  [ 0.77608484]\n",
      "  [ 1.3148452 ]\n",
      "  [ 0.7334375 ]\n",
      "  [-0.90711284]\n",
      "  [ 0.06042899]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56b4bd90>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[380., 531., 939., 448., 369., 397., 116., 213., 262., 383., 402.,\n",
      "        741., 552., 165., 312.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[380. 531. 939. 448. 369. 397. 116. 213. 262. 383. 402. 741. 552. 165.\n",
      "  312.]]), needle.Tensor([[[-0....29]\n",
      "  [-1.2453793 ]\n",
      "  [ 2.0987017 ]\n",
      "  [ 0.77608484]\n",
      "  [ 1.3148452 ]\n",
      "  [ 0.7334375 ]\n",
      "  [-0.90711284]\n",
      "  [ 0.06042899]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56b4bd90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.76160806]\n",
      "  [-0.7353778 ]\n",
      "  [ 1.6336391 ]\n",
      "  [-0.54160273]\n",
      "  [-1.4369678 ]\n",
      "  [-0.9993193 ]\n",
      "  [-0.38...029]\n",
      "  [-1.2453793 ]\n",
      "  [ 2.0987017 ]\n",
      "  [ 0.77608484]\n",
      "  [ 1.3148452 ]\n",
      "  [ 0.7334375 ]\n",
      "  [-0.90711284]\n",
      "  [ 0.06042899]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56b4bd90>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[380. 531. 939. 448. 369. 397. 116. 213. 262. 383. 402. 741. 552. 165.\n",
      "  312.]])\n",
      "        x_emb      = needle.Tensor([[[0.6907422 ]\n",
      "  [0.04690459]\n",
      "  [0.7410317 ]\n",
      "  [0.94875944]\n",
      "  [0.12096187]\n",
      "  [0.17119461]\n",
      "  [0.5211254 ]...0.7853831 ]\n",
      "  [0.26072437]\n",
      "  [0.56282717]\n",
      "  [0.27675483]\n",
      "  [0.55848527]\n",
      "  [0.9506377 ]\n",
      "  [0.07172067]\n",
      "  [0.7960629 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6907422 ]\n",
      "  [0.04690459]\n",
      "  [0.7410317 ]\n",
      "  [0.94875944]\n",
      "  [0.12096187]\n",
      "  [0.17119461]\n",
      "  [0.5211254 ...29]\n",
      "  [-1.2453793 ]\n",
      "  [ 2.0987017 ]\n",
      "  [ 0.77608484]\n",
      "  [ 1.3148452 ]\n",
      "  [ 0.7334375 ]\n",
      "  [-0.90711284]\n",
      "  [ 0.06042899]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b4b610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6907422 ]\n",
      "  [0.04690459]\n",
      "  [0.7410317 ]\n",
      "  [0.94875944]\n",
      "  [0.12096187]\n",
      "  [0.17119461]\n",
      "  [0.5211254 ]...0.7853831 ]\n",
      "  [0.26072437]\n",
      "  [0.56282717]\n",
      "  [0.27675483]\n",
      "  [0.55848527]\n",
      "  [0.9506377 ]\n",
      "  [0.07172067]\n",
      "  [0.7960629 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.76160806]\n",
      " [-0.7353778 ]\n",
      " [ 1.6336391 ]\n",
      " [-0.54160273]\n",
      " [-1.4369678 ]\n",
      " [-0.9993193 ]\n",
      " [-0.38150728]\n",
      " [ 1.6683146 ]\n",
      " [ 1.4793491 ]\n",
      " [ 1.8363904 ]\n",
      " [-0.38283792]\n",
      " [-0.9058261 ]\n",
      " [ 0.91596717]\n",
      " [ 0.55107707]\n",
      " [-0.535243  ]])\n",
      "        h0         = (needle.Tensor([[-0.76160806]\n",
      " [-0.7353778 ]\n",
      " [ 1.6336391 ]\n",
      " [-0.54160273]\n",
      " [-1.4369678 ]\n",
      " [-0.9993193 ]\n",
      " [-0.38150728...0.01374029]\n",
      " [-1.2453793 ]\n",
      " [ 2.0987017 ]\n",
      " [ 0.77608484]\n",
      " [ 1.3148452 ]\n",
      " [ 0.7334375 ]\n",
      " [-0.90711284]\n",
      " [ 0.06042899]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]])\n",
      "        inputs     = [needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b4b990>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b4b610>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7...1.6683146 ]\n",
      " [ 1.4793491 ]\n",
      " [ 1.8363904 ]\n",
      " [-0.38283792]\n",
      " [-0.9058261 ]\n",
      " [ 0.91596717]\n",
      " [ 0.55107707]\n",
      " [-0.535243  ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b4b990>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]\n",
      " [-0.03844291]])\n",
      "        bias_ih    = needle.Tensor([[-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]\n",
      " [-0.54743433]])\n",
      "        h          = needle.Tensor([[-0.76160806]\n",
      " [-0.7353778 ]\n",
      " [ 1.6336391 ]\n",
      " [-0.54160273]\n",
      " [-1.4369678 ]\n",
      " [-0.9993193 ]\n",
      " [-0.38150728]\n",
      " [ 1.6683146 ]\n",
      " [ 1.4793491 ]\n",
      " [ 1.8363904 ]\n",
      " [-0.38283792]\n",
      " [-0.9058261 ]\n",
      " [ 0.91596717]\n",
      " [ 0.55107707]\n",
      " [-0.535243  ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b4b990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.99545884]])\n",
      "        self       = needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7...]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]]), needle.Tensor([[0.99545884]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7...]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]]), needle.Tensor([[0.99545884]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7850>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5705eef0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57ab65d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5705d730>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd57ab65d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]], device=cpu())\n",
      "        b          = NDArray([[0.99545884]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ab7850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]], device=cpu())\n",
      "        b          = NDArray([[0.99545884]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]], device=cpu())\n",
      "other = NDArray([[0.99545884]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ab59f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56b48730>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57ab64b0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.99545884]], device=cuda())\n",
      "out        = NDArray([[7.6175707e-35]\n",
      " [0.0000000e+00]\n",
      " [1.2108418e-34]\n",
      " [0.0000000e+00]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061...]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]\n",
      " [1.4061475e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.6907422 ]\n",
      " [0.04690459]\n",
      " [0.7410317 ]\n",
      " [0.94875944]\n",
      " [0.12096187]\n",
      " [0.17119461]\n",
      " [0.5211254 ]\n",
      " [0.7853831 ]\n",
      " [0.26072437]\n",
      " [0.56282717]\n",
      " [0.27675483]\n",
      " [0.55848527]\n",
      " [0.9506377 ]\n",
      " [0.07172067]\n",
      " [0.7960629 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.8329223 ]\n",
      "  [-0.8352889 ]\n",
      "  [ 0.40779987]\n",
      "  [-0.06528258]\n",
      "  [ 0.5378941 ]\n",
      "  [ 0.6267627 ]\n",
      "  [-1.02...638]\n",
      "  [ 0.04107523]\n",
      "  [ 0.2337612 ]\n",
      "  [-0.47594512]\n",
      "  [-2.0065532 ]\n",
      "  [-0.24800727]\n",
      "  [-0.58359814]\n",
      "  [-0.9760967 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.5463408 ]\n",
      "  [ 0.7138557 ]\n",
      "  [ 2.2209918 ]\n",
      "  [-0.22157255]\n",
      "  [ 0.39188242]\n",
      "  [-0.44499496]\n",
      "  [ 1.67...93 ]\n",
      "  [-1.5695788 ]\n",
      "  [ 0.58689076]\n",
      "  [ 1.2920291 ]\n",
      "  [-1.5296336 ]\n",
      "  [ 0.3108223 ]\n",
      "  [ 2.2809012 ]\n",
      "  [ 1.8625896 ]]])\n",
      "h0         = needle.Tensor([[[-0.5463408 ]\n",
      "  [ 0.7138557 ]\n",
      "  [ 2.2209918 ]\n",
      "  [-0.22157255]\n",
      "  [ 0.39188242]\n",
      "  [-0.44499496]\n",
      "  [ 1.67...93 ]\n",
      "  [-1.5695788 ]\n",
      "  [ 0.58689076]\n",
      "  [ 1.2920291 ]\n",
      "  [-1.5296336 ]\n",
      "  [ 0.3108223 ]\n",
      "  [ 2.2809012 ]\n",
      "  [ 1.8625896 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57acb610>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[670., 325., 127.,  70., 482., 652., 151., 688.,   3., 889., 443.,\n",
      "        559., 740., 985., 927.],\n",
      "       [320...    [457., 109., 106., 654., 880., 428., 224., 562., 584.,  48., 659.,\n",
      "        209.,  41., 514.,  22.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[670. 325. 127.  70. 482. 652. 151. 688.   3. 889. 443. 559. 740. 985.\n",
      "  927.]\n",
      " [320. 711. 179. 437.  ...3 ]\n",
      "  [-1.5695788 ]\n",
      "  [ 0.58689076]\n",
      "  [ 1.2920291 ]\n",
      "  [-1.5296336 ]\n",
      "  [ 0.3108223 ]\n",
      "  [ 2.2809012 ]\n",
      "  [ 1.8625896 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57acb610>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.5463408 ]\n",
      "  [ 0.7138557 ]\n",
      "  [ 2.2209918 ]\n",
      "  [-0.22157255]\n",
      "  [ 0.39188242]\n",
      "  [-0.44499496]\n",
      "  [ 1.67...93 ]\n",
      "  [-1.5695788 ]\n",
      "  [ 0.58689076]\n",
      "  [ 1.2920291 ]\n",
      "  [-1.5296336 ]\n",
      "  [ 0.3108223 ]\n",
      "  [ 2.2809012 ]\n",
      "  [ 1.8625896 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57acb610>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[670. 325. 127.  70. 482. 652. 151. 688.   3. 889. 443. 559. 740. 985.\n",
      "  927.]\n",
      " [320. 711. 179. 437.   ...416. 944. 173. 467. 834. 693.\n",
      "  184.]\n",
      " [457. 109. 106. 654. 880. 428. 224. 562. 584.  48. 659. 209.  41. 514.\n",
      "   22.]])\n",
      "        x_emb      = needle.Tensor([[[0.7543101 ]\n",
      "  [0.9390068 ]\n",
      "  [0.5609507 ]\n",
      "  [0.6988576 ]\n",
      "  [0.47488979]\n",
      "  [0.02801226]\n",
      "  [0.48317724]...0.96416533]\n",
      "  [0.12962198]\n",
      "  [0.40790376]\n",
      "  [0.15609495]\n",
      "  [0.77432364]\n",
      "  [0.62039185]\n",
      "  [0.3321703 ]\n",
      "  [0.12975393]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7543101 ]\n",
      "  [0.9390068 ]\n",
      "  [0.5609507 ]\n",
      "  [0.6988576 ]\n",
      "  [0.47488979]\n",
      "  [0.02801226]\n",
      "  [0.48317724...3 ]\n",
      "  [-1.5695788 ]\n",
      "  [ 0.58689076]\n",
      "  [ 1.2920291 ]\n",
      "  [-1.5296336 ]\n",
      "  [ 0.3108223 ]\n",
      "  [ 2.2809012 ]\n",
      "  [ 1.8625896 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ac83d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7543101 ]\n",
      "  [0.9390068 ]\n",
      "  [0.5609507 ]\n",
      "  [0.6988576 ]\n",
      "  [0.47488979]\n",
      "  [0.02801226]\n",
      "  [0.48317724]...0.96416533]\n",
      "  [0.12962198]\n",
      "  [0.40790376]\n",
      "  [0.15609495]\n",
      "  [0.77432364]\n",
      "  [0.62039185]\n",
      "  [0.3321703 ]\n",
      "  [0.12975393]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.5463408 ]\n",
      " [ 0.7138557 ]\n",
      " [ 2.2209918 ]\n",
      " [-0.22157255]\n",
      " [ 0.39188242]\n",
      " [-0.44499496]\n",
      " [ 1.6736199 ]\n",
      " [-0.44772908]\n",
      " [-0.39173937]\n",
      " [ 0.2810163 ]\n",
      " [ 2.4688299 ]\n",
      " [ 0.699156  ]\n",
      " [-1.1795745 ]\n",
      " [ 0.66702795]\n",
      " [ 1.590136  ]])\n",
      "        h0         = (needle.Tensor([[-0.5463408 ]\n",
      " [ 0.7138557 ]\n",
      " [ 2.2209918 ]\n",
      " [-0.22157255]\n",
      " [ 0.39188242]\n",
      " [-0.44499496]\n",
      " [ 1.6736199 ...0.5857093 ]\n",
      " [-1.5695788 ]\n",
      " [ 0.58689076]\n",
      " [ 1.2920291 ]\n",
      " [-1.5296336 ]\n",
      " [ 0.3108223 ]\n",
      " [ 2.2809012 ]\n",
      " [ 1.8625896 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]])\n",
      "        inputs     = [needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0... [0.42716673]\n",
      " [0.48523363]\n",
      " [0.7585663 ]\n",
      " [0.29159933]\n",
      " [0.29702523]\n",
      " [0.31885162]\n",
      " [0.21210143]\n",
      " [0.30849448]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57acab50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57ac83d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0...0.44772908]\n",
      " [-0.39173937]\n",
      " [ 0.2810163 ]\n",
      " [ 2.4688299 ]\n",
      " [ 0.699156  ]\n",
      " [-1.1795745 ]\n",
      " [ 0.66702795]\n",
      " [ 1.590136  ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57acab50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]\n",
      " [-0.42941958]])\n",
      "        bias_ih    = needle.Tensor([[-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]\n",
      " [-0.9806854]])\n",
      "        h          = needle.Tensor([[-0.5463408 ]\n",
      " [ 0.7138557 ]\n",
      " [ 2.2209918 ]\n",
      " [-0.22157255]\n",
      " [ 0.39188242]\n",
      " [-0.44499496]\n",
      " [ 1.6736199 ]\n",
      " [-0.44772908]\n",
      " [-0.39173937]\n",
      " [ 0.2810163 ]\n",
      " [ 2.4688299 ]\n",
      " [ 0.699156  ]\n",
      " [-1.1795745 ]\n",
      " [ 0.66702795]\n",
      " [ 1.590136  ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57acab50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.42451704]])\n",
      "        self       = needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0...]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]]), needle.Tensor([[0.42451704]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58258fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0...]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]]), needle.Tensor([[0.42451704]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58258fd0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e890f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58259bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e8a3b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd58259bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]], device=cpu())\n",
      "        b          = NDArray([[0.42451704]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58258fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]], device=cpu())\n",
      "        b          = NDArray([[0.42451704]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]], device=cpu())\n",
      "other = NDArray([[0.42451704]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57acb830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57aca770>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5825adb0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.42451704]], device=cuda())\n",
      "out        = NDArray([[6.6772574e-34]\n",
      " [0.0000000e+00]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320...]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]\n",
      " [8.5320818e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.7543101 ]\n",
      " [0.9390068 ]\n",
      " [0.5609507 ]\n",
      " [0.6988576 ]\n",
      " [0.47488979]\n",
      " [0.02801226]\n",
      " [0.48317724]\n",
      " [0.0204588 ]\n",
      " [0.17392541]\n",
      " [0.71966743]\n",
      " [0.19132651]\n",
      " [0.0635156 ]\n",
      " [0.55109656]\n",
      " [0.5005206 ]\n",
      " [0.69240355]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.1178243]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[2.4564028]]])\n",
      "h0         = needle.Tensor([[[2.4564028]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58562150>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[646.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[646.]]), needle.Tensor([[[2.4564028]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58562150>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[2.4564028]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58562150>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[646.]])\n",
      "        x_emb      = needle.Tensor([[[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "   0.5249138  0.3863414  0.75121576...\n",
      "   0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "   0.1832823  0.85481477 0.025297   0.98397326]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "   0.5249138  0.3863414  0.7512157...57  0.21965109 0.6971004  0.7905217\n",
      "   0.1832823  0.85481477 0.025297   0.98397326]]]), needle.Tensor([[[2.4564028]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58560950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "   0.5249138  0.3863414  0.75121576...\n",
      "   0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "   0.1832823  0.85481477 0.025297   0.98397326]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[2.4564028]])\n",
      "        h0         = (needle.Tensor([[2.4564028]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0...745\n",
      "  0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]])\n",
      "        inputs     = [needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 ...45\n",
      "  0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585617d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58560950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 ...846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]]), needle.Tensor([[2.4564028]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585617d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0...745\n",
      "  0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.9906882]])\n",
      "        bias_ih    = needle.Tensor([[-0.08870363]])\n",
      "        h          = needle.Tensor([[2.4564028]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585617d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.07351589]\n",
      " [ 0.90695524]\n",
      " [-0.86998457]\n",
      " [ 0.81871796]\n",
      " [-0.68634593]\n",
      " [ 0.47055542]\n",
      " [-0.27284372]... 0.38670194]\n",
      " [-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]])\n",
      "        self       = needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0...745\n",
      "  0.85272896 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 ...0.38670194]\n",
      " [-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585612d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 ...0.38670194]\n",
      " [-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585612d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5716d7b0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd585613d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5716d530>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd585613d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0.31165...96 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]], device=cpu())\n",
      "        b          = NDArray([[-0.07351589]\n",
      " [ 0.90695524]\n",
      " [-0.86998457]\n",
      " [ 0.81871796]\n",
      " [-0.68634593]\n",
      " [ 0.47055542]\n",
      " [-0.27284372]\n",
      " [ 0....-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd585612d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0.31165...96 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]], device=cpu())\n",
      "        b          = NDArray([[-0.07351589]\n",
      " [ 0.90695524]\n",
      " [-0.86998457]\n",
      " [ 0.81871796]\n",
      " [-0.68634593]\n",
      " [ 0.47055542]\n",
      " [-0.27284372]\n",
      " [ 0....-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0.31165...96 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]], device=cpu())\n",
      "other = NDArray([[-0.07351589]\n",
      " [ 0.90695524]\n",
      " [-0.86998457]\n",
      " [ 0.81871796]\n",
      " [-0.68634593]\n",
      " [ 0.47055542]\n",
      " [-0.27284372]\n",
      " [ 0....-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585608f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58563b30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58562930>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.07351589]\n",
      " [ 0.90695524]\n",
      " [-0.86998457]\n",
      " [ 0.81871796]\n",
      " [-0.68634593]\n",
      " [ 0.47055542]\n",
      " [-0.27284372]\n",
      " [ 0....-0.2968812 ]\n",
      " [ 0.35636246]\n",
      " [ 0.815451  ]\n",
      " [ 0.4854002 ]\n",
      " [-0.60606265]\n",
      " [-0.32029867]\n",
      " [ 0.88128555]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8289521  0.48575005 0.5242361  0.6098689  0.18460168 0.01131925\n",
      "  0.5249138  0.3863414  0.75121576 0.31165...96 0.70025665 0.9846657  0.21965109 0.6971004  0.7905217\n",
      "  0.1832823  0.85481477 0.025297   0.98397326]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[2.16749]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[1.1574975]]])\n",
      "h0         = needle.Tensor([[[1.1574975]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57126010>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[378.],\n",
      "       [137.],\n",
      "       [125.],\n",
      "       [540.],\n",
      "       [888.],\n",
      "       [893.],\n",
      "       [210.],\n",
      "       [571.],\n",
      "       [363.],\n",
      "       [682.],\n",
      "       [ 96.],\n",
      "       [649.],\n",
      "       [267.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[378.]\n",
      " [137.]\n",
      " [125.]\n",
      " [540.]\n",
      " [888.]\n",
      " [893.]\n",
      " [210.]\n",
      " [571.]\n",
      " [363.]\n",
      " [682.]\n",
      " [ 96.]\n",
      " [649.]\n",
      " [267.]]), needle.Tensor([[[1.1574975]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57126010>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[1.1574975]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57126010>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[378.]\n",
      " [137.]\n",
      " [125.]\n",
      " [540.]\n",
      " [888.]\n",
      " [893.]\n",
      " [210.]\n",
      " [571.]\n",
      " [363.]\n",
      " [682.]\n",
      " [ 96.]\n",
      " [649.]\n",
      " [267.]])\n",
      "        x_emb      = needle.Tensor([[[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "   0.77672243 0.02363031 0.55013853 ...   0.1193111  0.01689413 0.31489772 0.5880338  0.8855337  0.30887583\n",
      "   0.4597706  0.3059172  0.5041814  0.61451083]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "   0.77672243 0.02363031 0.55013853...72 0.5880338  0.8855337  0.30887583\n",
      "   0.4597706  0.3059172  0.5041814  0.61451083]]]), needle.Tensor([[[1.1574975]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57125510>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "   0.77672243 0.02363031 0.55013853 ...   0.1193111  0.01689413 0.31489772 0.5880338  0.8855337  0.30887583\n",
      "   0.4597706  0.3059172  0.5041814  0.61451083]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[1.1574975]])\n",
      "        h0         = (needle.Tensor([[1.1574975]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0....958\n",
      "  0.5263961  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]])\n",
      "        inputs     = [needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0....4581576  0.00646375 0.8445297  0.6645869  0.23766991 0.08615959\n",
      "  0.33331785 0.5531099  0.17959933 0.29633123]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57127290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57125510>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0...5296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]]), needle.Tensor([[1.1574975]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57127290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0....958\n",
      "  0.5263961  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.13782352]])\n",
      "        bias_ih    = needle.Tensor([[-0.91754276]])\n",
      "        h          = needle.Tensor([[1.1574975]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57127290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.66280127]\n",
      " [ 0.93452966]\n",
      " [-0.19520032]\n",
      " [ 0.29561913]\n",
      " [-0.5231614 ]\n",
      " [-0.05459285]\n",
      " [ 0.37384307]... 0.3471713 ]\n",
      " [-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]])\n",
      "        self       = needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0....958\n",
      "  0.5263961  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0...0.3471713 ]\n",
      " [-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0...0.3471713 ]\n",
      " [-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3510>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5733b6b0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd584c36d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5733b170>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd584c36d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0.543185...1  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]], device=cpu())\n",
      "        b          = NDArray([[-0.66280127]\n",
      " [ 0.93452966]\n",
      " [-0.19520032]\n",
      " [ 0.29561913]\n",
      " [-0.5231614 ]\n",
      " [-0.05459285]\n",
      " [ 0.37384307]\n",
      " [ 0....-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c3510>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0.543185...1  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]], device=cpu())\n",
      "        b          = NDArray([[-0.66280127]\n",
      " [ 0.93452966]\n",
      " [-0.19520032]\n",
      " [ 0.29561913]\n",
      " [-0.5231614 ]\n",
      " [-0.05459285]\n",
      " [ 0.37384307]\n",
      " [ 0....-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0.543185...1  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]], device=cpu())\n",
      "other = NDArray([[-0.66280127]\n",
      " [ 0.93452966]\n",
      " [-0.19520032]\n",
      " [ 0.29561913]\n",
      " [-0.5231614 ]\n",
      " [-0.05459285]\n",
      " [ 0.37384307]\n",
      " [ 0....-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57dd87f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57127a70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c1730>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.66280127]\n",
      " [ 0.93452966]\n",
      " [-0.19520032]\n",
      " [ 0.29561913]\n",
      " [-0.5231614 ]\n",
      " [-0.05459285]\n",
      " [ 0.37384307]\n",
      " [ 0....-0.63665664]\n",
      " [-0.8885038 ]\n",
      " [ 0.802111  ]\n",
      " [ 0.12770724]\n",
      " [ 0.5935273 ]\n",
      " [ 0.30327678]\n",
      " [-0.7567241 ]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.40183935 0.24011396 0.4002045  0.03776898 0.62373245 0.4225543\n",
      "  0.77672243 0.02363031 0.55013853 0.543185...1  0.49904197 0.35296118 0.6453579  0.11118672 0.4914056\n",
      "  0.3966519  0.6749481  0.90111774 0.5079986 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.0847635]]\n",
      "\n",
      " [[ 0.5337371]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[0.8213806 ]]\n",
      "\n",
      " [[0.10038885]]])\n",
      "h0         = needle.Tensor([[[0.8213806 ]]\n",
      "\n",
      " [[0.10038885]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd58011a90>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[663.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[663.]]), needle.Tensor([[[0.8213806 ]]\n",
      "\n",
      " [[0.10038885]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58011a90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[0.8213806 ]]\n",
      "\n",
      " [[0.10038885]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd58011a90>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[663.]])\n",
      "        x_emb      = needle.Tensor([[[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "   0.4743119  0.02103991 0.969766  ...1\n",
      "   0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "   0.5425972  0.2136708  0.52533966 0.47002217]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "   0.4743119  0.02103991 0.969766 ...0914277  0.844816\n",
      "   0.5425972  0.2136708  0.52533966 0.47002217]]]), needle.Tensor([[[0.8213806 ]]\n",
      "\n",
      " [[0.10038885]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58012510>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "   0.4743119  0.02103991 0.969766  ...1\n",
      "   0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "   0.5425972  0.2136708  0.52533966 0.47002217]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.8213806]])\n",
      "        h0         = (needle.Tensor([[0.8213806]]), needle.Tensor([[0.10038885]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0...1681\n",
      "  0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]])\n",
      "        inputs     = [needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   ...681\n",
      "  0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58010450>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58012510>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   ...17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]]), needle.Tensor([[0.8213806]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58010450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0...1681\n",
      "  0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.9064884]])\n",
      "        bias_ih    = needle.Tensor([[-0.9885229]])\n",
      "        h          = needle.Tensor([[0.8213806]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58010450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.9476292 ]\n",
      " [ 0.8684821 ]\n",
      " [-0.3547774 ]\n",
      " [-0.28753936]\n",
      " [ 0.0858357 ]\n",
      " [ 0.3700186 ]\n",
      " [ 0.19870877]...-0.5305328 ]\n",
      " [ 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]])\n",
      "        self       = needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0...1681\n",
      "  0.25346246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   ...0.5305328 ]\n",
      " [ 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58402750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   ...0.5305328 ]\n",
      " [ 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58402750>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5833e3b0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58401fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5833c130>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd58401fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0.95808...246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]], device=cpu())\n",
      "        b          = NDArray([[-0.9476292 ]\n",
      " [ 0.8684821 ]\n",
      " [-0.3547774 ]\n",
      " [-0.28753936]\n",
      " [ 0.0858357 ]\n",
      " [ 0.3700186 ]\n",
      " [ 0.19870877]\n",
      " [-0.... 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58402750>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0.95808...246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]], device=cpu())\n",
      "        b          = NDArray([[-0.9476292 ]\n",
      " [ 0.8684821 ]\n",
      " [-0.3547774 ]\n",
      " [-0.28753936]\n",
      " [ 0.0858357 ]\n",
      " [ 0.3700186 ]\n",
      " [ 0.19870877]\n",
      " [-0.... 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0.95808...246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]], device=cpu())\n",
      "other = NDArray([[-0.9476292 ]\n",
      " [ 0.8684821 ]\n",
      " [-0.3547774 ]\n",
      " [-0.28753936]\n",
      " [ 0.0858357 ]\n",
      " [ 0.3700186 ]\n",
      " [ 0.19870877]\n",
      " [-0.... 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58403cf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580122f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584010b0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.9476292 ]\n",
      " [ 0.8684821 ]\n",
      " [-0.3547774 ]\n",
      " [-0.28753936]\n",
      " [ 0.0858357 ]\n",
      " [ 0.3700186 ]\n",
      " [ 0.19870877]\n",
      " [-0.... 0.7473173 ]\n",
      " [ 0.19989586]\n",
      " [ 0.71005416]\n",
      " [ 0.922583  ]\n",
      " [ 0.52862036]\n",
      " [-0.73467743]\n",
      " [ 0.41672504]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.76159215 0.20145486 0.02308402 0.3786575  0.93355703 0.12470815\n",
      "  0.4743119  0.02103991 0.969766   0.95808...246 0.499296   0.17500454 0.14391515 0.0914277  0.844816\n",
      "  0.5425972  0.2136708  0.52533966 0.47002217]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.97633594]]\n",
      "\n",
      " [[-1.2388427 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.11054464]]\n",
      "\n",
      " [[ 2.3471038 ]]])\n",
      "h0         = needle.Tensor([[[-0.11054464]]\n",
      "\n",
      " [[ 2.3471038 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57edae10>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[719.],\n",
      "       [313.],\n",
      "       [476.],\n",
      "       [480.],\n",
      "       [ 88.],\n",
      "       [813.],\n",
      "       [771.],\n",
      "       [151.],\n",
      "       [856.],\n",
      "       [425.],\n",
      "       [ 18.],\n",
      "       [521.],\n",
      "       [251.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[719.]\n",
      " [313.]\n",
      " [476.]\n",
      " [480.]\n",
      " [ 88.]\n",
      " [813.]\n",
      " [771.]\n",
      " [151.]\n",
      " [856.]\n",
      " [425.]\n",
      " [ 18.]\n",
      " [521.]\n",
      " [251.]]), needle.Tensor([[[-0.11054464]]\n",
      "\n",
      " [[ 2.3471038 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57edae10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.11054464]]\n",
      "\n",
      " [[ 2.3471038 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57edae10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[719.]\n",
      " [313.]\n",
      " [476.]\n",
      " [480.]\n",
      " [ 88.]\n",
      " [813.]\n",
      " [771.]\n",
      " [151.]\n",
      " [856.]\n",
      " [425.]\n",
      " [ 18.]\n",
      " [521.]\n",
      " [251.]])\n",
      "        x_emb      = needle.Tensor([[[2.15866402e-01 1.34818956e-01 1.79687202e-01 1.13231957e-01\n",
      "   5.92221737e-01 7.52282441e-01 5.722789...e-01 6.85872495e-01\n",
      "   6.59253716e-01 9.21232820e-01 8.35222542e-01 2.99766272e-01\n",
      "   1.47581184e-02 5.76230049e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[2.15866402e-01 1.34818956e-01 1.79687202e-01 1.13231957e-01\n",
      "   5.92221737e-01 7.52282441e-01 5.72278...8.35222542e-01 2.99766272e-01\n",
      "   1.47581184e-02 5.76230049e-01]]]), needle.Tensor([[[-0.11054464]]\n",
      "\n",
      " [[ 2.3471038 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57eda9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[2.15866402e-01 1.34818956e-01 1.79687202e-01 1.13231957e-01\n",
      "   5.92221737e-01 7.52282441e-01 5.722789...e-01 6.85872495e-01\n",
      "   6.59253716e-01 9.21232820e-01 8.35222542e-01 2.99766272e-01\n",
      "   1.47581184e-02 5.76230049e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.11054464]])\n",
      "        h0         = (needle.Tensor([[-0.11054464]]), needle.Tensor([[2.3471038]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0...65\n",
      "  0.21778348 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]])\n",
      "        inputs     = [needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 ....5057709  0.48061365 0.6775373  0.44507265 0.6460851  0.18153578\n",
      "  0.03372322 0.24567589 0.5998249  0.8102955 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ed8050>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57eda9d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 ...5585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]]), needle.Tensor([[-0.11054464]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ed8050>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0...65\n",
      "  0.21778348 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.5505111]])\n",
      "        bias_ih    = needle.Tensor([[0.8217367]])\n",
      "        h          = needle.Tensor([[-0.11054464]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ed8050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.5102495 ]\n",
      " [-0.2605071 ]\n",
      " [ 0.63906574]\n",
      " [ 0.10897839]\n",
      " [-0.34767252]\n",
      " [-0.34201932]\n",
      " [-0.7865884 ]...-0.865821  ]\n",
      " [-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]])\n",
      "        self       = needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0...65\n",
      "  0.21778348 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 ...0.865821  ]\n",
      " [-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56e7e950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 ...0.865821  ]\n",
      " [-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56e7e950>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d4f30>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56e7ec10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd581d7430>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56e7ec10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0.16304...8 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]], device=cpu())\n",
      "        b          = NDArray([[ 0.5102495 ]\n",
      " [-0.2605071 ]\n",
      " [ 0.63906574]\n",
      " [ 0.10897839]\n",
      " [-0.34767252]\n",
      " [-0.34201932]\n",
      " [-0.7865884 ]\n",
      " [-0....-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56e7e950>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0.16304...8 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]], device=cpu())\n",
      "        b          = NDArray([[ 0.5102495 ]\n",
      " [-0.2605071 ]\n",
      " [ 0.63906574]\n",
      " [ 0.10897839]\n",
      " [-0.34767252]\n",
      " [-0.34201932]\n",
      " [-0.7865884 ]\n",
      " [-0....-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0.16304...8 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]], device=cpu())\n",
      "other = NDArray([[ 0.5102495 ]\n",
      " [-0.2605071 ]\n",
      " [ 0.63906574]\n",
      " [ 0.10897839]\n",
      " [-0.34767252]\n",
      " [-0.34201932]\n",
      " [-0.7865884 ]\n",
      " [-0....-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56e7ddb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57ed87b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56e7d630>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.5102495 ]\n",
      " [-0.2605071 ]\n",
      " [ 0.63906574]\n",
      " [ 0.10897839]\n",
      " [-0.34767252]\n",
      " [-0.34201932]\n",
      " [-0.7865884 ]\n",
      " [-0....-0.39340788]\n",
      " [-0.8621124 ]\n",
      " [-0.84801555]\n",
      " [ 0.46529377]\n",
      " [-0.15739763]\n",
      " [-0.99769557]\n",
      " [-0.8939492 ]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.2158664  0.13481896 0.1796872  0.11323196 0.59222174 0.75228244\n",
      "  0.5722789  0.13214305 0.50725424 0.16304...8 0.5795218  0.02485585 0.7504859  0.9959333  0.14487702\n",
      "  0.65501267 0.03598773 0.6566312  0.61154556]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.15481433]\n",
      "  [ 1.2027135 ]\n",
      "  [-0.56483203]\n",
      "  [-0.698694  ]\n",
      "  [ 0.226336  ]\n",
      "  [-0.44473085]\n",
      "  [-1.52...495]\n",
      "  [-0.05249077]\n",
      "  [-1.6574811 ]\n",
      "  [ 0.6555154 ]\n",
      "  [-0.64849454]\n",
      "  [-1.3823769 ]\n",
      "  [-0.62628937]\n",
      "  [-0.8652205 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 1.6642044 ]\n",
      "  [-0.52413046]\n",
      "  [-0.14369048]\n",
      "  [ 0.9772035 ]\n",
      "  [-0.25148055]\n",
      "  [-0.96600914]\n",
      "  [ 1.25...651]\n",
      "  [ 1.151141  ]\n",
      "  [ 0.45806697]\n",
      "  [ 0.69960815]\n",
      "  [-1.5726835 ]\n",
      "  [ 0.40371376]\n",
      "  [ 0.7251419 ]\n",
      "  [-0.19027087]]])\n",
      "h0         = needle.Tensor([[[ 1.6642044 ]\n",
      "  [-0.52413046]\n",
      "  [-0.14369048]\n",
      "  [ 0.9772035 ]\n",
      "  [-0.25148055]\n",
      "  [-0.96600914]\n",
      "  [ 1.25...651]\n",
      "  [ 1.151141  ]\n",
      "  [ 0.45806697]\n",
      "  [ 0.69960815]\n",
      "  [-1.5726835 ]\n",
      "  [ 0.40371376]\n",
      "  [ 0.7251419 ]\n",
      "  [-0.19027087]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584ba610>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[493., 707.,  75., 567., 199., 203., 227., 664., 718., 715., 254.,\n",
      "        729., 256., 860., 530.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[493. 707.  75. 567. 199. 203. 227. 664. 718. 715. 254. 729. 256. 860.\n",
      "  530.]]), needle.Tensor([[[ 1....51]\n",
      "  [ 1.151141  ]\n",
      "  [ 0.45806697]\n",
      "  [ 0.69960815]\n",
      "  [-1.5726835 ]\n",
      "  [ 0.40371376]\n",
      "  [ 0.7251419 ]\n",
      "  [-0.19027087]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584ba610>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 1.6642044 ]\n",
      "  [-0.52413046]\n",
      "  [-0.14369048]\n",
      "  [ 0.9772035 ]\n",
      "  [-0.25148055]\n",
      "  [-0.96600914]\n",
      "  [ 1.25...651]\n",
      "  [ 1.151141  ]\n",
      "  [ 0.45806697]\n",
      "  [ 0.69960815]\n",
      "  [-1.5726835 ]\n",
      "  [ 0.40371376]\n",
      "  [ 0.7251419 ]\n",
      "  [-0.19027087]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584ba610>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[493. 707.  75. 567. 199. 203. 227. 664. 718. 715. 254. 729. 256. 860.\n",
      "  530.]])\n",
      "        x_emb      = needle.Tensor([[[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "   0.60321456 0.74734527 0.00212515 ...\n",
      "   0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "   0.19121955 0.48442954 0.17031293 0.59681535]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "   0.60321456 0.74734527 0.00212515...51]\n",
      "  [ 1.151141  ]\n",
      "  [ 0.45806697]\n",
      "  [ 0.69960815]\n",
      "  [-1.5726835 ]\n",
      "  [ 0.40371376]\n",
      "  [ 0.7251419 ]\n",
      "  [-0.19027087]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b9bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "   0.60321456 0.74734527 0.00212515 ...\n",
      "   0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "   0.19121955 0.48442954 0.17031293 0.59681535]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 1.6642044 ]\n",
      " [-0.52413046]\n",
      " [-0.14369048]\n",
      " [ 0.9772035 ]\n",
      " [-0.25148055]\n",
      " [-0.96600914]\n",
      " [ 1.2538764 ]\n",
      " [-0.05981651]\n",
      " [ 1.151141  ]\n",
      " [ 0.45806697]\n",
      " [ 0.69960815]\n",
      " [-1.5726835 ]\n",
      " [ 0.40371376]\n",
      " [ 0.7251419 ]\n",
      " [-0.19027087]])\n",
      "        h0         = (needle.Tensor([[ 1.6642044 ]\n",
      " [-0.52413046]\n",
      " [-0.14369048]\n",
      " [ 0.9772035 ]\n",
      " [-0.25148055]\n",
      " [-0.96600914]\n",
      " [ 1.2538764 ....05981651]\n",
      " [ 1.151141  ]\n",
      " [ 0.45806697]\n",
      " [ 0.69960815]\n",
      " [-1.5726835 ]\n",
      " [ 0.40371376]\n",
      " [ 0.7251419 ]\n",
      " [-0.19027087]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0....143\n",
      "  0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]])\n",
      "        inputs     = [needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0...43\n",
      "  0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b9c90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b9bd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0...0.05981651]\n",
      " [ 1.151141  ]\n",
      " [ 0.45806697]\n",
      " [ 0.69960815]\n",
      " [-1.5726835 ]\n",
      " [ 0.40371376]\n",
      " [ 0.7251419 ]\n",
      " [-0.19027087]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b9c90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0....143\n",
      "  0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]\n",
      " [0.24322689]])\n",
      "        bias_ih    = needle.Tensor([[-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]\n",
      " [-0.46654195]])\n",
      "        h          = needle.Tensor([[ 1.6642044 ]\n",
      " [-0.52413046]\n",
      " [-0.14369048]\n",
      " [ 0.9772035 ]\n",
      " [-0.25148055]\n",
      " [-0.96600914]\n",
      " [ 1.2538764 ]\n",
      " [-0.05981651]\n",
      " [ 1.151141  ]\n",
      " [ 0.45806697]\n",
      " [ 0.69960815]\n",
      " [-1.5726835 ]\n",
      " [ 0.40371376]\n",
      " [ 0.7251419 ]\n",
      " [-0.19027087]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b9c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.9025471 ]\n",
      " [ 0.24287927]\n",
      " [-0.09051549]\n",
      " [-0.91202676]\n",
      " [ 0.26179075]\n",
      " [-0.86011994]\n",
      " [-0.33588743]... 0.66319025]\n",
      " [ 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]])\n",
      "        self       = needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0....143\n",
      "  0.35763443 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0...0.66319025]\n",
      " [ 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b9790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0...0.66319025]\n",
      " [ 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b9790>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57663930>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58400fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd576637f0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58400fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0.285238...43 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]], device=cpu())\n",
      "        b          = NDArray([[-0.9025471 ]\n",
      " [ 0.24287927]\n",
      " [-0.09051549]\n",
      " [-0.91202676]\n",
      " [ 0.26179075]\n",
      " [-0.86011994]\n",
      " [-0.33588743]\n",
      " [ 0.... 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b9790>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0.285238...43 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]], device=cpu())\n",
      "        b          = NDArray([[-0.9025471 ]\n",
      " [ 0.24287927]\n",
      " [-0.09051549]\n",
      " [-0.91202676]\n",
      " [ 0.26179075]\n",
      " [-0.86011994]\n",
      " [-0.33588743]\n",
      " [ 0.... 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0.285238...43 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]], device=cpu())\n",
      "other = NDArray([[-0.9025471 ]\n",
      " [ 0.24287927]\n",
      " [-0.09051549]\n",
      " [-0.91202676]\n",
      " [ 0.26179075]\n",
      " [-0.86011994]\n",
      " [-0.33588743]\n",
      " [ 0.... 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b89b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b8530>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58400330>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.9025471 ]\n",
      " [ 0.24287927]\n",
      " [-0.09051549]\n",
      " [-0.91202676]\n",
      " [ 0.26179075]\n",
      " [-0.86011994]\n",
      " [-0.33588743]\n",
      " [ 0.... 0.38379538]\n",
      " [-0.7174012 ]\n",
      " [ 0.46948552]\n",
      " [ 0.01663458]\n",
      " [-0.48536974]\n",
      " [-0.12527627]\n",
      " [ 0.9376782 ]], device=cuda())\n",
      "out        = NDArray([[7.3321997e-34]\n",
      " [0.0000000e+00]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833...]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]\n",
      " [9.4833118e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.006093   0.71570396 0.5185734  0.1163587  0.24939786 0.5551005\n",
      "  0.60321456 0.74734527 0.00212515 0.285238...43 0.25659406 0.99456257 0.4808933  0.9532321  0.2788852\n",
      "  0.19121955 0.48442954 0.17031293 0.59681535]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.7619828 ]\n",
      "  [ 0.20416835]\n",
      "  [-0.47502652]\n",
      "  [-0.4233934 ]\n",
      "  [-1.8771731 ]\n",
      "  [-1.0436578 ]\n",
      "  [ 0.30...46 ]\n",
      "  [ 1.6338216 ]\n",
      "  [ 0.50321317]\n",
      "  [ 0.21926045]\n",
      "  [ 0.11254458]\n",
      "  [-0.81041634]\n",
      "  [ 1.3816196 ]\n",
      "  [ 1.4445528 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-1.0584843 ]\n",
      "  [-0.8943624 ]\n",
      "  [ 2.8986576 ]\n",
      "  [ 0.01606954]\n",
      "  [ 1.106714  ]\n",
      "  [-1.4540335 ]\n",
      "  [ 0.31...321]\n",
      "  [-0.34471872]\n",
      "  [ 0.82813615]\n",
      "  [-0.03859239]\n",
      "  [-0.8312961 ]\n",
      "  [-1.2337873 ]\n",
      "  [-0.32349887]\n",
      "  [-0.48708543]]])\n",
      "h0         = needle.Tensor([[[-1.0584843 ]\n",
      "  [-0.8943624 ]\n",
      "  [ 2.8986576 ]\n",
      "  [ 0.01606954]\n",
      "  [ 1.106714  ]\n",
      "  [-1.4540335 ]\n",
      "  [ 0.31...321]\n",
      "  [-0.34471872]\n",
      "  [ 0.82813615]\n",
      "  [-0.03859239]\n",
      "  [-0.8312961 ]\n",
      "  [-1.2337873 ]\n",
      "  [-0.32349887]\n",
      "  [-0.48708543]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd585fe290>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[860., 309., 726., 902., 500.,  23., 347., 127., 113., 612., 114.,\n",
      "        147., 269., 314., 496.],\n",
      "       [143...    [773., 337., 674., 144., 936., 267., 980., 477., 942., 980.,  25.,\n",
      "         39., 128., 193., 159.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[860. 309. 726. 902. 500.  23. 347. 127. 113. 612. 114. 147. 269. 314.\n",
      "  496.]\n",
      " [143.  77. 314. 610.  ...21]\n",
      "  [-0.34471872]\n",
      "  [ 0.82813615]\n",
      "  [-0.03859239]\n",
      "  [-0.8312961 ]\n",
      "  [-1.2337873 ]\n",
      "  [-0.32349887]\n",
      "  [-0.48708543]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585fe290>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-1.0584843 ]\n",
      "  [-0.8943624 ]\n",
      "  [ 2.8986576 ]\n",
      "  [ 0.01606954]\n",
      "  [ 1.106714  ]\n",
      "  [-1.4540335 ]\n",
      "  [ 0.31...321]\n",
      "  [-0.34471872]\n",
      "  [ 0.82813615]\n",
      "  [-0.03859239]\n",
      "  [-0.8312961 ]\n",
      "  [-1.2337873 ]\n",
      "  [-0.32349887]\n",
      "  [-0.48708543]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd585fe290>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[860. 309. 726. 902. 500.  23. 347. 127. 113. 612. 114. 147. 269. 314.\n",
      "  496.]\n",
      " [143.  77. 314. 610.  8...310. 550. 140. 411. 735. 840.\n",
      "  106.]\n",
      " [773. 337. 674. 144. 936. 267. 980. 477. 942. 980.  25.  39. 128. 193.\n",
      "  159.]])\n",
      "        x_emb      = needle.Tensor([[[5.7872933e-01 8.1144564e-02 3.8037369e-01 ... 2.3243980e-01\n",
      "   9.8058903e-01 8.7731403e-01]\n",
      "  [4.1225...45e-01 8.0856103e-01]\n",
      "  [4.5918500e-01 8.1351143e-01 6.8868917e-01 ... 2.2710800e-01\n",
      "   2.9691771e-01 7.3389018e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[5.7872933e-01 8.1144564e-02 3.8037369e-01 ... 2.3243980e-01\n",
      "   9.8058903e-01 8.7731403e-01]\n",
      "  [4.122...21]\n",
      "  [-0.34471872]\n",
      "  [ 0.82813615]\n",
      "  [-0.03859239]\n",
      "  [-0.8312961 ]\n",
      "  [-1.2337873 ]\n",
      "  [-0.32349887]\n",
      "  [-0.48708543]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fe150>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[5.7872933e-01 8.1144564e-02 3.8037369e-01 ... 2.3243980e-01\n",
      "   9.8058903e-01 8.7731403e-01]\n",
      "  [4.1225...45e-01 8.0856103e-01]\n",
      "  [4.5918500e-01 8.1351143e-01 6.8868917e-01 ... 2.2710800e-01\n",
      "   2.9691771e-01 7.3389018e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-1.0584843 ]\n",
      " [-0.8943624 ]\n",
      " [ 2.8986576 ]\n",
      " [ 0.01606954]\n",
      " [ 1.106714  ]\n",
      " [-1.4540335 ]\n",
      " [ 0.3195009 ]\n",
      " [-0.01367321]\n",
      " [-0.34471872]\n",
      " [ 0.82813615]\n",
      " [-0.03859239]\n",
      " [-0.8312961 ]\n",
      " [-1.2337873 ]\n",
      " [-0.32349887]\n",
      " [-0.48708543]])\n",
      "        h0         = (needle.Tensor([[-1.0584843 ]\n",
      " [-0.8943624 ]\n",
      " [ 2.8986576 ]\n",
      " [ 0.01606954]\n",
      " [ 1.106714  ]\n",
      " [-1.4540335 ]\n",
      " [ 0.3195009 ....01367321]\n",
      " [-0.34471872]\n",
      " [ 0.82813615]\n",
      " [-0.03859239]\n",
      " [-0.8312961 ]\n",
      " [-1.2337873 ]\n",
      " [-0.32349887]\n",
      " [-0.48708543]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0...77\n",
      "  0.54145163 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]])\n",
      "        inputs     = [needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   ...1 2.11566985e-01\n",
      "  2.60680348e-01 1.34548843e-01 1.55454129e-01 8.12046528e-01\n",
      "  9.69489992e-01 9.39469114e-02]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fc290>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fe150>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   ...0.01367321]\n",
      " [-0.34471872]\n",
      " [ 0.82813615]\n",
      " [-0.03859239]\n",
      " [-0.8312961 ]\n",
      " [-1.2337873 ]\n",
      " [-0.32349887]\n",
      " [-0.48708543]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fc290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0...77\n",
      "  0.54145163 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]\n",
      " [0.61025894]])\n",
      "        bias_ih    = needle.Tensor([[-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]\n",
      " [-0.33511412]])\n",
      "        h          = needle.Tensor([[-1.0584843 ]\n",
      " [-0.8943624 ]\n",
      " [ 2.8986576 ]\n",
      " [ 0.01606954]\n",
      " [ 1.106714  ]\n",
      " [-1.4540335 ]\n",
      " [ 0.3195009 ]\n",
      " [-0.01367321]\n",
      " [-0.34471872]\n",
      " [ 0.82813615]\n",
      " [-0.03859239]\n",
      " [-0.8312961 ]\n",
      " [-1.2337873 ]\n",
      " [-0.32349887]\n",
      " [-0.48708543]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fc290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.44240868]\n",
      " [-0.3699816 ]\n",
      " [ 0.76056814]\n",
      " [ 0.93324614]\n",
      " [-0.42021328]\n",
      " [ 0.7443969 ]\n",
      " [ 0.23557162]...-0.4711057 ]\n",
      " [-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]])\n",
      "        self       = needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0...77\n",
      "  0.54145163 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   ...0.4711057 ]\n",
      " [-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58818590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   ...0.4711057 ]\n",
      " [-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58818590>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584258f0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd588188d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584269b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd588188d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0.46197...3 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]], device=cpu())\n",
      "        b          = NDArray([[ 0.44240868]\n",
      " [-0.3699816 ]\n",
      " [ 0.76056814]\n",
      " [ 0.93324614]\n",
      " [-0.42021328]\n",
      " [ 0.7443969 ]\n",
      " [ 0.23557162]\n",
      " [ 0....-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58818590>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0.46197...3 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]], device=cpu())\n",
      "        b          = NDArray([[ 0.44240868]\n",
      " [-0.3699816 ]\n",
      " [ 0.76056814]\n",
      " [ 0.93324614]\n",
      " [-0.42021328]\n",
      " [ 0.7443969 ]\n",
      " [ 0.23557162]\n",
      " [ 0....-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0.46197...3 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]], device=cpu())\n",
      "other = NDArray([[ 0.44240868]\n",
      " [-0.3699816 ]\n",
      " [ 0.76056814]\n",
      " [ 0.93324614]\n",
      " [-0.42021328]\n",
      " [ 0.7443969 ]\n",
      " [ 0.23557162]\n",
      " [ 0....-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561c02b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585fde30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58818030>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.44240868]\n",
      " [-0.3699816 ]\n",
      " [ 0.76056814]\n",
      " [ 0.93324614]\n",
      " [-0.42021328]\n",
      " [ 0.7443969 ]\n",
      " [ 0.23557162]\n",
      " [ 0....-0.7539488 ]\n",
      " [ 0.41668713]\n",
      " [-0.833627  ]\n",
      " [-0.0125922 ]\n",
      " [ 0.9343351 ]\n",
      " [-0.42374778]\n",
      " [-0.05911887]], device=cuda())\n",
      "out        = NDArray([[6.741139e-35]\n",
      " [0.000000e+00]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01...54e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]\n",
      " [8.689554e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.57872933 0.08114456 0.3803737  0.8884367  0.88518023 0.15491079\n",
      "  0.30194253 0.5697202  0.776611   0.46197...3 0.8777918  0.46988234 0.8685377  0.919542   0.37406546\n",
      "  0.68716896 0.30533427 0.80222225 0.75383866]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.37079862]\n",
      "  [-0.95096886]\n",
      "  [ 2.5623925 ]\n",
      "  [ 0.6567615 ]\n",
      "  [ 0.01824365]\n",
      "  [ 2.0714583 ]\n",
      "  [ 0.58...67 ]\n",
      "  [-0.33820748]\n",
      "  [ 0.52675784]\n",
      "  [-1.049045  ]\n",
      "  [ 1.4851142 ]\n",
      "  [-1.4145378 ]\n",
      "  [-0.29455516]\n",
      "  [-0.20847687]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 1.0858924 ]\n",
      "  [ 0.7699465 ]\n",
      "  [ 0.39058673]\n",
      "  [-0.14239953]\n",
      "  [-0.4950446 ]\n",
      "  [ 1.8394659 ]\n",
      "  [ 0.52...995]\n",
      "  [-0.79772437]\n",
      "  [ 1.8897104 ]\n",
      "  [ 1.0050851 ]\n",
      "  [-0.9809602 ]\n",
      "  [-1.5586687 ]\n",
      "  [-0.17576824]\n",
      "  [-1.0365292 ]]])\n",
      "h0         = needle.Tensor([[[ 1.0858924 ]\n",
      "  [ 0.7699465 ]\n",
      "  [ 0.39058673]\n",
      "  [-0.14239953]\n",
      "  [-0.4950446 ]\n",
      "  [ 1.8394659 ]\n",
      "  [ 0.52...995]\n",
      "  [-0.79772437]\n",
      "  [ 1.8897104 ]\n",
      "  [ 1.0050851 ]\n",
      "  [-0.9809602 ]\n",
      "  [-1.5586687 ]\n",
      "  [-0.17576824]\n",
      "  [-1.0365292 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd587abd50>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[296., 440., 529., 793., 229., 426., 681., 845., 985., 304., 665.,\n",
      "        327., 231., 555., 817.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[296. 440. 529. 793. 229. 426. 681. 845. 985. 304. 665. 327. 231. 555.\n",
      "  817.]]), needle.Tensor([[[ 1....95]\n",
      "  [-0.79772437]\n",
      "  [ 1.8897104 ]\n",
      "  [ 1.0050851 ]\n",
      "  [-0.9809602 ]\n",
      "  [-1.5586687 ]\n",
      "  [-0.17576824]\n",
      "  [-1.0365292 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd587abd50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 1.0858924 ]\n",
      "  [ 0.7699465 ]\n",
      "  [ 0.39058673]\n",
      "  [-0.14239953]\n",
      "  [-0.4950446 ]\n",
      "  [ 1.8394659 ]\n",
      "  [ 0.52...995]\n",
      "  [-0.79772437]\n",
      "  [ 1.8897104 ]\n",
      "  [ 1.0050851 ]\n",
      "  [-0.9809602 ]\n",
      "  [-1.5586687 ]\n",
      "  [-0.17576824]\n",
      "  [-1.0365292 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd587abd50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[296. 440. 529. 793. 229. 426. 681. 845. 985. 304. 665. 327. 231. 555.\n",
      "  817.]])\n",
      "        x_emb      = needle.Tensor([[[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "   0.46898666 0.5145337  0.09779898...   0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "   0.98197967 0.7008753  0.12868133 0.10095327]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "   0.46898666 0.5145337  0.0977989...95]\n",
      "  [-0.79772437]\n",
      "  [ 1.8897104 ]\n",
      "  [ 1.0050851 ]\n",
      "  [-0.9809602 ]\n",
      "  [-1.5586687 ]\n",
      "  [-0.17576824]\n",
      "  [-1.0365292 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd587a9610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "   0.46898666 0.5145337  0.09779898...   0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "   0.98197967 0.7008753  0.12868133 0.10095327]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 1.0858924 ]\n",
      " [ 0.7699465 ]\n",
      " [ 0.39058673]\n",
      " [-0.14239953]\n",
      " [-0.4950446 ]\n",
      " [ 1.8394659 ]\n",
      " [ 0.5216551 ]\n",
      " [-0.27745098]\n",
      " [ 0.05021379]\n",
      " [-0.4338413 ]\n",
      " [ 0.4710919 ]\n",
      " [ 0.6318798 ]\n",
      " [-0.21020734]\n",
      " [ 0.684365  ]\n",
      " [-0.54064226]])\n",
      "        h0         = (needle.Tensor([[ 1.0858924 ]\n",
      " [ 0.7699465 ]\n",
      " [ 0.39058673]\n",
      " [-0.14239953]\n",
      " [-0.4950446 ]\n",
      " [ 1.8394659 ]\n",
      " [ 0.5216551 ...0.75395995]\n",
      " [-0.79772437]\n",
      " [ 1.8897104 ]\n",
      " [ 1.0050851 ]\n",
      " [-0.9809602 ]\n",
      " [-1.5586687 ]\n",
      " [-0.17576824]\n",
      " [-1.0365292 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0...35\n",
      "  0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]])\n",
      "        inputs     = [needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 ...5\n",
      "  0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587aa010>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd587a9610>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 ...0.27745098]\n",
      " [ 0.05021379]\n",
      " [-0.4338413 ]\n",
      " [ 0.4710919 ]\n",
      " [ 0.6318798 ]\n",
      " [-0.21020734]\n",
      " [ 0.684365  ]\n",
      " [-0.54064226]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587aa010>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0...35\n",
      "  0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]\n",
      " [-0.1926421]])\n",
      "        bias_ih    = needle.Tensor([[0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]\n",
      " [0.92190504]])\n",
      "        h          = needle.Tensor([[ 1.0858924 ]\n",
      " [ 0.7699465 ]\n",
      " [ 0.39058673]\n",
      " [-0.14239953]\n",
      " [-0.4950446 ]\n",
      " [ 1.8394659 ]\n",
      " [ 0.5216551 ]\n",
      " [-0.27745098]\n",
      " [ 0.05021379]\n",
      " [-0.4338413 ]\n",
      " [ 0.4710919 ]\n",
      " [ 0.6318798 ]\n",
      " [-0.21020734]\n",
      " [ 0.684365  ]\n",
      " [-0.54064226]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd587aa010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.47460163]\n",
      " [-0.2757064 ]\n",
      " [ 0.4907745 ]\n",
      " [ 0.9136969 ]\n",
      " [ 0.3500948 ]\n",
      " [ 0.7209153 ]\n",
      " [-0.95743465]...-0.8891571 ]\n",
      " [ 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]])\n",
      "        self       = needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0...35\n",
      "  0.6503477  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 ...0.8891571 ]\n",
      " [ 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587ab590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 ...0.8891571 ]\n",
      " [ 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587ab590>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd588b4230>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd587a99d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd588b45f0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd587a99d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0.37306...  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]], device=cpu())\n",
      "        b          = NDArray([[-0.47460163]\n",
      " [-0.2757064 ]\n",
      " [ 0.4907745 ]\n",
      " [ 0.9136969 ]\n",
      " [ 0.3500948 ]\n",
      " [ 0.7209153 ]\n",
      " [-0.95743465]\n",
      " [-0.... 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587ab590>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0.37306...  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]], device=cpu())\n",
      "        b          = NDArray([[-0.47460163]\n",
      " [-0.2757064 ]\n",
      " [ 0.4907745 ]\n",
      " [ 0.9136969 ]\n",
      " [ 0.3500948 ]\n",
      " [ 0.7209153 ]\n",
      " [-0.95743465]\n",
      " [-0.... 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0.37306...  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]], device=cpu())\n",
      "other = NDArray([[-0.47460163]\n",
      " [-0.2757064 ]\n",
      " [ 0.4907745 ]\n",
      " [ 0.9136969 ]\n",
      " [ 0.3500948 ]\n",
      " [ 0.7209153 ]\n",
      " [-0.95743465]\n",
      " [-0.... 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587aab30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd587ab030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587ab470>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.47460163]\n",
      " [-0.2757064 ]\n",
      " [ 0.4907745 ]\n",
      " [ 0.9136969 ]\n",
      " [ 0.3500948 ]\n",
      " [ 0.7209153 ]\n",
      " [-0.95743465]\n",
      " [-0.... 0.2044357 ]\n",
      " [ 0.8033589 ]\n",
      " [-0.4350826 ]\n",
      " [ 0.88309383]\n",
      " [-0.97019804]\n",
      " [ 0.6964065 ]\n",
      " [-0.12251163]], device=cuda())\n",
      "out        = NDArray([[5.7459257e-34]\n",
      " [0.0000000e+00]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895...]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.19698077 0.04342386 0.8559747  0.2937585  0.32138565 0.17048043\n",
      "  0.46898666 0.5145337  0.09779898 0.37306...  0.29892936 0.74465895 0.7278739  0.78218764 0.06993158\n",
      "  0.98197967 0.7008753  0.12868133 0.10095327]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.8089731 ]\n",
      "  [-1.2649084 ]\n",
      "  [-0.5790811 ]\n",
      "  [-0.06616941]\n",
      "  [ 0.8790192 ]\n",
      "  [-0.39841932]\n",
      "  [-0.85...896]\n",
      "  [ 0.8340311 ]\n",
      "  [-1.1499711 ]\n",
      "  [ 1.2744296 ]\n",
      "  [ 0.45724598]\n",
      "  [-0.4277115 ]\n",
      "  [-0.4834805 ]\n",
      "  [ 1.4584229 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.00544706]\n",
      "  [-0.49000767]\n",
      "  [ 0.05694223]\n",
      "  [-0.32807973]\n",
      "  [ 0.63428086]\n",
      "  [-1.1813265 ]\n",
      "  [-0.00...79 ]\n",
      "  [ 0.90273273]\n",
      "  [-0.9380333 ]\n",
      "  [ 0.5589611 ]\n",
      "  [ 0.5160185 ]\n",
      "  [-1.4312258 ]\n",
      "  [ 0.525313  ]\n",
      "  [-1.2818058 ]]])\n",
      "h0         = needle.Tensor([[[ 0.00544706]\n",
      "  [-0.49000767]\n",
      "  [ 0.05694223]\n",
      "  [-0.32807973]\n",
      "  [ 0.63428086]\n",
      "  [-1.1813265 ]\n",
      "  [-0.00...79 ]\n",
      "  [ 0.90273273]\n",
      "  [-0.9380333 ]\n",
      "  [ 0.5589611 ]\n",
      "  [ 0.5160185 ]\n",
      "  [-1.4312258 ]\n",
      "  [ 0.525313  ]\n",
      "  [-1.2818058 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd580b58d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[892., 285., 747., 646., 424., 620., 942., 891., 352., 119., 192.,\n",
      "         13., 299.,  21., 876.],\n",
      "       [675...    [131., 979., 482., 333., 278., 833., 688., 412., 423., 992.,  45.,\n",
      "        804., 188., 653., 382.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[892. 285. 747. 646. 424. 620. 942. 891. 352. 119. 192.  13. 299.  21.\n",
      "  876.]\n",
      " [675. 457. 378.  22. 9...9 ]\n",
      "  [ 0.90273273]\n",
      "  [-0.9380333 ]\n",
      "  [ 0.5589611 ]\n",
      "  [ 0.5160185 ]\n",
      "  [-1.4312258 ]\n",
      "  [ 0.525313  ]\n",
      "  [-1.2818058 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b58d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.00544706]\n",
      "  [-0.49000767]\n",
      "  [ 0.05694223]\n",
      "  [-0.32807973]\n",
      "  [ 0.63428086]\n",
      "  [-1.1813265 ]\n",
      "  [-0.00...79 ]\n",
      "  [ 0.90273273]\n",
      "  [-0.9380333 ]\n",
      "  [ 0.5589611 ]\n",
      "  [ 0.5160185 ]\n",
      "  [-1.4312258 ]\n",
      "  [ 0.525313  ]\n",
      "  [-1.2818058 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd580b58d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[892. 285. 747. 646. 424. 620. 942. 891. 352. 119. 192.  13. 299.  21.\n",
      "  876.]\n",
      " [675. 457. 378.  22. 92... 98. 463. 543. 901. 596. 540.\n",
      "   39.]\n",
      " [131. 979. 482. 333. 278. 833. 688. 412. 423. 992.  45. 804. 188. 653.\n",
      "  382.]])\n",
      "        x_emb      = needle.Tensor([[[0.28343943 0.89517117 0.35525325 ... 0.8022118  0.94554096 0.7852226 ]\n",
      "  [0.28933343 0.38549694 0.574...4854 ... 0.706044   0.29653364 0.96197873]\n",
      "  [0.02601724 0.42307907 0.55562127 ... 0.7031627  0.53623176 0.93219656]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.28343943 0.89517117 0.35525325 ... 0.8022118  0.94554096 0.7852226 ]\n",
      "  [0.28933343 0.38549694 0.57...9 ]\n",
      "  [ 0.90273273]\n",
      "  [-0.9380333 ]\n",
      "  [ 0.5589611 ]\n",
      "  [ 0.5160185 ]\n",
      "  [-1.4312258 ]\n",
      "  [ 0.525313  ]\n",
      "  [-1.2818058 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b7f90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.28343943 0.89517117 0.35525325 ... 0.8022118  0.94554096 0.7852226 ]\n",
      "  [0.28933343 0.38549694 0.574...4854 ... 0.706044   0.29653364 0.96197873]\n",
      "  [0.02601724 0.42307907 0.55562127 ... 0.7031627  0.53623176 0.93219656]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.00544706]\n",
      " [-0.49000767]\n",
      " [ 0.05694223]\n",
      " [-0.32807973]\n",
      " [ 0.63428086]\n",
      " [-1.1813265 ]\n",
      " [-0.00566396]\n",
      " [ 0.61376095]\n",
      " [ 0.10133341]\n",
      " [ 2.091777  ]\n",
      " [-0.38281804]\n",
      " [-0.9373802 ]\n",
      " [-1.1697475 ]\n",
      " [-2.6800659 ]\n",
      " [-0.1665598 ]])\n",
      "        h0         = (needle.Tensor([[ 0.00544706]\n",
      " [-0.49000767]\n",
      " [ 0.05694223]\n",
      " [-0.32807973]\n",
      " [ 0.63428086]\n",
      " [-1.1813265 ]\n",
      " [-0.00566396...1.2244779 ]\n",
      " [ 0.90273273]\n",
      " [-0.9380333 ]\n",
      " [ 0.5589611 ]\n",
      " [ 0.5160185 ]\n",
      " [-1.4312258 ]\n",
      " [ 0.525313  ]\n",
      " [-1.2818058 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561...015e-01 9.46372271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]])\n",
      "        inputs     = [needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.4160456....12032244 0.4292068  0.9256207  0.7542123  0.5485497  0.17270327\n",
      "  0.5479246  0.07073788 0.5531555  0.86765695]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4d90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd580b7f90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.4160456...0.61376095]\n",
      " [ 0.10133341]\n",
      " [ 2.091777  ]\n",
      " [-0.38281804]\n",
      " [-0.9373802 ]\n",
      " [-1.1697475 ]\n",
      " [-2.6800659 ]\n",
      " [-0.1665598 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4d90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561...015e-01 9.46372271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]\n",
      " [-0.08325505]])\n",
      "        bias_ih    = needle.Tensor([[-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]\n",
      " [-0.14854759]])\n",
      "        h          = needle.Tensor([[ 0.00544706]\n",
      " [-0.49000767]\n",
      " [ 0.05694223]\n",
      " [-0.32807973]\n",
      " [ 0.63428086]\n",
      " [-1.1813265 ]\n",
      " [-0.00566396]\n",
      " [ 0.61376095]\n",
      " [ 0.10133341]\n",
      " [ 2.091777  ]\n",
      " [-0.38281804]\n",
      " [-0.9373802 ]\n",
      " [-1.1697475 ]\n",
      " [-2.6800659 ]\n",
      " [-0.1665598 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd580b4d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.04115474]\n",
      " [-0.484069  ]\n",
      " [-0.98487276]\n",
      " [-0.0803569 ]\n",
      " [-0.09707129]\n",
      " [-0.18002218]\n",
      " [ 0.45888603]...-0.12332892]\n",
      " [ 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]])\n",
      "        self       = needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561...015e-01 9.46372271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.4160456...0.12332892]\n",
      " [ 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c0f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.4160456...0.12332892]\n",
      " [ 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c0f10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5855a8b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd561c2e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58559a30>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd561c2e50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561e-01 1...2271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]], device=cpu())\n",
      "        b          = NDArray([[ 0.04115474]\n",
      " [-0.484069  ]\n",
      " [-0.98487276]\n",
      " [-0.0803569 ]\n",
      " [-0.09707129]\n",
      " [-0.18002218]\n",
      " [ 0.45888603]\n",
      " [ 0.... 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd561c0f10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561e-01 1...2271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]], device=cpu())\n",
      "        b          = NDArray([[ 0.04115474]\n",
      " [-0.484069  ]\n",
      " [-0.98487276]\n",
      " [-0.0803569 ]\n",
      " [-0.09707129]\n",
      " [-0.18002218]\n",
      " [ 0.45888603]\n",
      " [ 0.... 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561e-01 1...2271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]], device=cpu())\n",
      "other = NDArray([[ 0.04115474]\n",
      " [-0.484069  ]\n",
      " [-0.98487276]\n",
      " [-0.0803569 ]\n",
      " [-0.09707129]\n",
      " [-0.18002218]\n",
      " [ 0.45888603]\n",
      " [ 0.... 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585fd1b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd580b5030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561c1030>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.04115474]\n",
      " [-0.484069  ]\n",
      " [-0.98487276]\n",
      " [-0.0803569 ]\n",
      " [-0.09707129]\n",
      " [-0.18002218]\n",
      " [ 0.45888603]\n",
      " [ 0.... 0.7285683 ]\n",
      " [-0.00456148]\n",
      " [-0.6691564 ]\n",
      " [ 0.48673713]\n",
      " [ 0.73474216]\n",
      " [-0.8669962 ]\n",
      " [-0.7402315 ]], device=cuda())\n",
      "out        = NDArray([[2.6843541e-33]\n",
      " [0.0000000e+00]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895...]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]\n",
      " [8.6895537e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[2.83439428e-01 8.95171165e-01 3.55253249e-01 3.01984489e-01\n",
      "  1.67646617e-01 2.51005322e-01 3.41604561e-01 1...2271e-01\n",
      "  3.96635473e-01 2.17127606e-01 2.91095972e-01 9.61003125e-01\n",
      "  4.75426584e-01 4.98656213e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.4190569  -0.03510071 -0.95517474  0.01672301 -0.00619238\n",
      "   -0.10921539 -0.305536    0.12173688  0.803991    0.993043\n",
      "   -1.0441601   0.7325564 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376\n",
      "   -0.41802138  1.2896659   0.56866413  0.06800185 -1.8855633\n",
      "   -0.7146524  -0.8225561 ]]])\n",
      "h0         = needle.Tensor([[[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376\n",
      "   -0.41802138  1.2896659   0.56866413  0.06800185 -1.8855633\n",
      "   -0.7146524  -0.8225561 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57660510>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[990.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[990.]]), needle.Tensor([[[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376\n",
      "   -0.41802138  1.2896659   0.56866413  0.06800185 -1.8855633\n",
      "   -0.7146524  -0.8225561 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57660510>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376\n",
      "   -0.41802138  1.2896659   0.56866413  0.06800185 -1.8855633\n",
      "   -0.7146524  -0.8225561 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57660510>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[990.]])\n",
      "        x_emb      = needle.Tensor([[[0.5482662]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5482662]]]), needle.Tensor([[[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376\n",
      "   -0.41802138  1.2896659   0.56866413  0.06800185 -1.8855633\n",
      "   -0.7146524  -0.8225561 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57661a50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5482662]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376  -0.41802138\n",
      "   1.2896659   0.56866413  0.06800185 -1.8855633  -0.7146524  -0.8225561 ]])\n",
      "        h0         = (needle.Tensor([[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376  -0.41802138\n",
      "   1.2896659   0.56866413  0.06800185 -1.8855633  -0.7146524  -0.8225561 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5482662]])\n",
      "        inputs     = [needle.Tensor([[0.5482662]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576626d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57661a50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5482662]]), needle.Tensor([[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376  -0.41802138\n",
      "   1.2896659   0.56866413  0.06800185 -1.8855633  -0.7146524  -0.8225561 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576626d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5482662]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.00183406 -0.16967152  0.1941354   0.04819781  0.01733539 -0.20206754\n",
      "   0.16340205 -0.03898768 -0.14282718 -0.0539621  -0.23957542 -0.04741137]])\n",
      "        bias_ih    = needle.Tensor([[-0.20299327  0.03825924  0.26778895  0.01282007 -0.19007277 -0.0901143\n",
      "   0.21197307 -0.16442916  0.2839465   0.14866376  0.22932827  0.18861717]])\n",
      "        h          = needle.Tensor([[ 0.6415209  -0.7596724   0.29450908 -1.9601967   1.0388376  -0.41802138\n",
      "   1.2896659   0.56866413  0.06800185 -1.8855633  -0.7146524  -0.8225561 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576626d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]])\n",
      "        self       = needle.Tensor([[0.5482662]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5482662]]), needle.Tensor([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576602d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5482662]]), needle.Tensor([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576602d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f11830>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58512a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f106b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58512a90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5482662]], device=cpu())\n",
      "        b          = NDArray([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd576602d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5482662]], device=cpu())\n",
      "        b          = NDArray([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5482662]], device=cpu())\n",
      "other = NDArray([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57661770>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd576638f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58510ff0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.18145844  0.04544342  0.22678113  0.185206    0.05658075 -0.23866719\n",
      "  -0.0633972  -0.10297543 -0.23828831 -0.06938471  0.2574038   0.01144099]], device=cuda())\n",
      "out        = NDArray([[2.929475e-34 0.000000e+00 1.785486e-01 1.785486e-01 1.785486e-01\n",
      "  1.785486e-01 1.785486e-01 1.785486e-01 1.785486e-01 1.785486e-01\n",
      "  1.785486e-01 1.785486e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.5482662]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.804012   -0.15607454  0.80716527  0.4393697   1.7340863\n",
      "    0.01990543  0.48657092  1.2175514  -2.077741    1.6673628\n",
      "    0.42957124 -0.05239766]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304\n",
      "   -0.44940948 -0.52689433 -1.0492283  -2.78782     0.3704845\n",
      "   -0.93641627  0.08840918]]])\n",
      "h0         = needle.Tensor([[[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304\n",
      "   -0.44940948 -0.52689433 -1.0492283  -2.78782     0.3704845\n",
      "   -0.93641627  0.08840918]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57acadd0>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[363.],\n",
      "       [813.],\n",
      "       [155.],\n",
      "       [979.],\n",
      "       [423.],\n",
      "       [789.],\n",
      "       [794.],\n",
      "       [520.],\n",
      "       [665.],\n",
      "       [807.],\n",
      "       [171.],\n",
      "       [911.],\n",
      "       [921.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[363.]\n",
      " [813.]\n",
      " [155.]\n",
      " [979.]\n",
      " [423.]\n",
      " [789.]\n",
      " [794.]\n",
      " [520.]\n",
      " [665.]\n",
      " [807.]\n",
      " [171.]\n",
      " [911.]\n",
      " [921.]...  -1.9622259   0.93662304\n",
      "   -0.44940948 -0.52689433 -1.0492283  -2.78782     0.3704845\n",
      "   -0.93641627  0.08840918]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57acadd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304\n",
      "   -0.44940948 -0.52689433 -1.0492283  -2.78782     0.3704845\n",
      "   -0.93641627  0.08840918]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57acadd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[363.]\n",
      " [813.]\n",
      " [155.]\n",
      " [979.]\n",
      " [423.]\n",
      " [789.]\n",
      " [794.]\n",
      " [520.]\n",
      " [665.]\n",
      " [807.]\n",
      " [171.]\n",
      " [911.]\n",
      " [921.]])\n",
      "        x_emb      = needle.Tensor([[[0.5654033 ]]\n",
      "\n",
      " [[0.1356745 ]]\n",
      "\n",
      " [[0.9133912 ]]\n",
      "\n",
      " [[0.634555  ]]\n",
      "\n",
      " [[0.7916171 ]]\n",
      "\n",
      " [[0.78637475]]\n",
      "\n",
      " [[0.82196355]]\n",
      "\n",
      " [[0.8329628 ]]\n",
      "\n",
      " [[0.15260313]]\n",
      "\n",
      " [[0.5770275 ]]\n",
      "\n",
      " [[0.58287   ]]\n",
      "\n",
      " [[0.71497333]]\n",
      "\n",
      " [[0.98562735]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5654033 ]]\n",
      "\n",
      " [[0.1356745 ]]\n",
      "\n",
      " [[0.9133912 ]]\n",
      "\n",
      " [[0.634555  ]]\n",
      "\n",
      " [[0.7916171 ]]\n",
      "\n",
      " [[0.78637475]]\n",
      "\n",
      " ...  -1.9622259   0.93662304\n",
      "   -0.44940948 -0.52689433 -1.0492283  -2.78782     0.3704845\n",
      "   -0.93641627  0.08840918]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57acac10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5654033 ]]\n",
      "\n",
      " [[0.1356745 ]]\n",
      "\n",
      " [[0.9133912 ]]\n",
      "\n",
      " [[0.634555  ]]\n",
      "\n",
      " [[0.7916171 ]]\n",
      "\n",
      " [[0.78637475]]\n",
      "\n",
      " [[0.82196355]]\n",
      "\n",
      " [[0.8329628 ]]\n",
      "\n",
      " [[0.15260313]]\n",
      "\n",
      " [[0.5770275 ]]\n",
      "\n",
      " [[0.58287   ]]\n",
      "\n",
      " [[0.71497333]]\n",
      "\n",
      " [[0.98562735]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304 -0.44940948\n",
      "  -0.52689433 -1.0492283  -2.78782     0.3704845  -0.93641627  0.08840918]])\n",
      "        h0         = (needle.Tensor([[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304 -0.44940948\n",
      "  -0.52689433 -1.0492283  -2.78782     0.3704845  -0.93641627  0.08840918]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5654033]])\n",
      "        inputs     = [needle.Tensor([[0.5654033]]), needle.Tensor([[0.1356745]]), needle.Tensor([[0.9133912]]), needle.Tensor([[0.634555]]), needle.Tensor([[0.7916171]]), needle.Tensor([[0.78637475]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ac83d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57acac10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5654033]]), needle.Tensor([[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304 -0.44940948\n",
      "  -0.52689433 -1.0492283  -2.78782     0.3704845  -0.93641627  0.08840918]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ac83d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5654033]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.0431008   0.15653774  0.24361956 -0.05384704  0.24389166  0.18828711\n",
      "  -0.0159761  -0.22553468 -0.19116992 -0.24604785 -0.2544881  -0.05120279]])\n",
      "        bias_ih    = needle.Tensor([[-0.05046643  0.14804006 -0.11024785 -0.14326893 -0.12610488 -0.14782089\n",
      "   0.19192436  0.06146833 -0.08804673 -0.19240712  0.09689388  0.19776887]])\n",
      "        h          = needle.Tensor([[-0.32374808  0.4620579   0.8659986  -1.9622259   0.93662304 -0.44940948\n",
      "  -0.52689433 -1.0492283  -2.78782     0.3704845  -0.93641627  0.08840918]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57ac83d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]])\n",
      "        self       = needle.Tensor([[0.5654033]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5654033]]), needle.Tensor([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd571b7a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5654033]]), needle.Tensor([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd571b7a50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585de6b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd571b6ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585dd270>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd571b6ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5654033]], device=cpu())\n",
      "        b          = NDArray([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd571b7a50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5654033]], device=cpu())\n",
      "        b          = NDArray([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5654033]], device=cpu())\n",
      "other = NDArray([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57acb2b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57acae70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd571b7cf0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.13332544  0.00988206 -0.13407983 -0.07257365 -0.1706813   0.15972587\n",
      "   0.13606784 -0.27631778  0.18349165 -0.25808427  0.24890995 -0.11205842]], device=cuda())\n",
      "out        = NDArray([[ 8.0154268e-35  0.0000000e+00  1.2287307e-01 -1.2148678e-02\n",
      "  -5.3199190e-01  5.2386767e-01 -4.8083586e-01  3.0880648e-01\n",
      "   7.0646554e-01 -3.3597583e-01 -2.3057726e-01  9.2163086e-03]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.5654033]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.8065246   0.25272304 -0.16321215  0.8628081   0.064952\n",
      "   -0.50029254  0.92438316  0.41246274  0.2...72  -0.99247843  1.5115384\n",
      "    0.22548348  0.09938174  0.68326217 -0.9692317  -1.2753175\n",
      "    1.221503    1.109483  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715\n",
      "   -1.3949147  -0.76599556 -0.4770055  -0...6   0.88262755  0.8146572\n",
      "    0.6170803  -0.07072669 -0.06645139 -0.9118136  -0.34696844\n",
      "    0.80886286 -0.53585327]]])\n",
      "h0         = needle.Tensor([[[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715\n",
      "   -1.3949147  -0.76599556 -0.4770055  -0...6   0.88262755  0.8146572\n",
      "    0.6170803  -0.07072669 -0.06645139 -0.9118136  -0.34696844\n",
      "    0.80886286 -0.53585327]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56487ed0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[249.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[249.]]), needle.Tensor([[[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715\n",
      "   -1.3949147  ...   0.88262755  0.8146572\n",
      "    0.6170803  -0.07072669 -0.06645139 -0.9118136  -0.34696844\n",
      "    0.80886286 -0.53585327]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56487ed0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715\n",
      "   -1.3949147  -0.76599556 -0.4770055  -0...6   0.88262755  0.8146572\n",
      "    0.6170803  -0.07072669 -0.06645139 -0.9118136  -0.34696844\n",
      "    0.80886286 -0.53585327]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56487ed0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[249.]])\n",
      "        x_emb      = needle.Tensor([[[0.7533471]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7533471]]]), needle.Tensor([[[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715\n",
      "   -1.39...   0.88262755  0.8146572\n",
      "    0.6170803  -0.07072669 -0.06645139 -0.9118136  -0.34696844\n",
      "    0.80886286 -0.53585327]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd564859d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7533471]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715 -1.3949147\n",
      "  -0.76599556 -0.4770055  -0.9144706  -0.28061524  1.3582724   0.74084824]])\n",
      "        h0         = (needle.Tensor([[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715 -1.3949147\n",
      "  -0.76599556 -0.4770055  -0.9...29586   0.88262755  0.8146572   0.6170803\n",
      "  -0.07072669 -0.06645139 -0.9118136  -0.34696844  0.80886286 -0.53585327]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7533471]])\n",
      "        inputs     = [needle.Tensor([[0.7533471]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56484710>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd564859d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7533471]]), needle.Tensor([[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715 -1.3949147\n",
      "  -0.76599556 -0.4770055  -0.9144706  -0.28061524  1.3582724   0.74084824]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56484710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7533471]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.1904788  -0.08286029  0.17240176 -0.09302746 -0.24482378 -0.04270145\n",
      "   0.04563397 -0.05528457 -0.11590983  0.03123164  0.27883458  0.26408076]])\n",
      "        bias_ih    = needle.Tensor([[-0.28284848 -0.13972157 -0.04084583  0.06183714 -0.06729238  0.22457516\n",
      "  -0.19098924 -0.2260623  -0.25220662  0.1934863  -0.13355853 -0.18113472]])\n",
      "        h          = needle.Tensor([[-0.11157321 -1.1746598  -0.22791445 -0.07434698 -0.41722715 -1.3949147\n",
      "  -0.76599556 -0.4770055  -0.9144706  -0.28061524  1.3582724   0.74084824]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56484710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]])\n",
      "        self       = needle.Tensor([[0.7533471]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7533471]]), needle.Tensor([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56947f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7533471]]), needle.Tensor([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56947f90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585619b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56947d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585605f0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56947d50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7533471]], device=cpu())\n",
      "        b          = NDArray([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56947f90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7533471]], device=cpu())\n",
      "        b          = NDArray([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7533471]], device=cpu())\n",
      "other = NDArray([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56945ff0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56487670>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd569479b0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.27029303  0.07396585  0.13616928 -0.04086299  0.10287526  0.20663059\n",
      "  -0.24137457  0.16841981 -0.08747894 -0.2115545  -0.10070103  0.19866195]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 3.9793937e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.7533471]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.0381992   0.86806655 -1.5828912  -0.02475927 -0.3380924\n",
      "   -1.1933168  -1.3319211   0.9912539  -0....41    0.2360578  -0.60565645\n",
      "   -0.19239086  1.1270325   1.0254381  -0.07533653  0.95459\n",
      "   -1.4358343   0.13709217]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456\n",
      "    0.24913551 -0.45760646 -2.438876    0....4   1.6906502  -0.47552943\n",
      "   -1.9301128   0.3928285   0.8128103   0.07635771  1.0402868\n",
      "    0.3273404  -1.6530418 ]]])\n",
      "h0         = needle.Tensor([[[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456\n",
      "    0.24913551 -0.45760646 -2.438876    0....4   1.6906502  -0.47552943\n",
      "   -1.9301128   0.3928285   0.8128103   0.07635771  1.0402868\n",
      "    0.3273404  -1.6530418 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584b1110>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[512.],\n",
      "       [522.],\n",
      "       [909.],\n",
      "       [391.],\n",
      "       [910.],\n",
      "       [ 46.],\n",
      "       [237.],\n",
      "       [490.],\n",
      "       [908.],\n",
      "       [ 54.],\n",
      "       [204.],\n",
      "       [911.],\n",
      "       [577.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[512.]\n",
      " [522.]\n",
      " [909.]\n",
      " [391.]\n",
      " [910.]\n",
      " [ 46.]\n",
      " [237.]\n",
      " [490.]\n",
      " [908.]\n",
      " [ 54.]\n",
      " [204.]\n",
      " [911.]\n",
      " [577.]...   1.6906502  -0.47552943\n",
      "   -1.9301128   0.3928285   0.8128103   0.07635771  1.0402868\n",
      "    0.3273404  -1.6530418 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b1110>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456\n",
      "    0.24913551 -0.45760646 -2.438876    0....4   1.6906502  -0.47552943\n",
      "   -1.9301128   0.3928285   0.8128103   0.07635771  1.0402868\n",
      "    0.3273404  -1.6530418 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b1110>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[512.]\n",
      " [522.]\n",
      " [909.]\n",
      " [391.]\n",
      " [910.]\n",
      " [ 46.]\n",
      " [237.]\n",
      " [490.]\n",
      " [908.]\n",
      " [ 54.]\n",
      " [204.]\n",
      " [911.]\n",
      " [577.]])\n",
      "        x_emb      = needle.Tensor([[[0.31317744]]\n",
      "\n",
      " [[0.91219693]]\n",
      "\n",
      " [[0.4056882 ]]\n",
      "\n",
      " [[0.8643823 ]]\n",
      "\n",
      " [[0.20504181]]\n",
      "\n",
      " [[0.70688456]]\n",
      "\n",
      " [[0.16992198]]\n",
      "\n",
      " [[0.23072846]]\n",
      "\n",
      " [[0.1717828 ]]\n",
      "\n",
      " [[0.11934496]]\n",
      "\n",
      " [[0.31410807]]\n",
      "\n",
      " [[0.99445283]]\n",
      "\n",
      " [[0.33123472]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.31317744]]\n",
      "\n",
      " [[0.91219693]]\n",
      "\n",
      " [[0.4056882 ]]\n",
      "\n",
      " [[0.8643823 ]]\n",
      "\n",
      " [[0.20504181]]\n",
      "\n",
      " [[0.70688456]]\n",
      "\n",
      " ...   1.6906502  -0.47552943\n",
      "   -1.9301128   0.3928285   0.8128103   0.07635771  1.0402868\n",
      "    0.3273404  -1.6530418 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b1a10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.31317744]]\n",
      "\n",
      " [[0.91219693]]\n",
      "\n",
      " [[0.4056882 ]]\n",
      "\n",
      " [[0.8643823 ]]\n",
      "\n",
      " [[0.20504181]]\n",
      "\n",
      " [[0.70688456]]\n",
      "\n",
      " [[0.16992198]]\n",
      "\n",
      " [[0.23072846]]\n",
      "\n",
      " [[0.1717828 ]]\n",
      "\n",
      " [[0.11934496]]\n",
      "\n",
      " [[0.31410807]]\n",
      "\n",
      " [[0.99445283]]\n",
      "\n",
      " [[0.33123472]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456   0.24913551\n",
      "  -0.45760646 -2.438876    0.6156574  -0.1621717  -0.1558213  -0.46905273]])\n",
      "        h0         = (needle.Tensor([[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456   0.24913551\n",
      "  -0.45760646 -2.438876    0....91634   1.6906502  -0.47552943 -1.9301128\n",
      "   0.3928285   0.8128103   0.07635771  1.0402868   0.3273404  -1.6530418 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.31317744]])\n",
      "        inputs     = [needle.Tensor([[0.31317744]]), needle.Tensor([[0.91219693]]), needle.Tensor([[0.4056882]]), needle.Tensor([[0.8643823]]), needle.Tensor([[0.20504181]]), needle.Tensor([[0.70688456]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b1550>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b1a10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.31317744]]), needle.Tensor([[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456   0.24913551\n",
      "  -0.45760646 -2.438876    0.6156574  -0.1621717  -0.1558213  -0.46905273]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b1550>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.31317744]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.20286909  0.18320644 -0.12927979 -0.17223027  0.26492357 -0.12344649\n",
      "   0.21553206 -0.102514    0.12641355  0.2586903   0.01918063  0.11540434]])\n",
      "        bias_ih    = needle.Tensor([[-0.15300573 -0.21796425  0.01854217  0.11654392  0.08324397  0.07110986\n",
      "   0.17996904 -0.11304328  0.04612643  0.0241034  -0.2758098  -0.18409696]])\n",
      "        h          = needle.Tensor([[-0.8828893   1.4863186  -0.3448645   0.8796557   0.7464456   0.24913551\n",
      "  -0.45760646 -2.438876    0.6156574  -0.1621717  -0.1558213  -0.46905273]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b1550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]])\n",
      "        self       = needle.Tensor([[0.31317744]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.31317744]]), needle.Tensor([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd572b1e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.31317744]]), needle.Tensor([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd572b1e50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56faf070>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd572b02d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56fadb70>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd572b02d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.31317744]], device=cpu())\n",
      "        b          = NDArray([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd572b1e50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.31317744]], device=cpu())\n",
      "        b          = NDArray([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.31317744]], device=cpu())\n",
      "other = NDArray([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573a77b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b2c30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd572b14f0>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.1735051  -0.12257394  0.00328487 -0.18094563 -0.2865328  -0.00345558\n",
      "   0.17199108 -0.28833723 -0.27873188 -0.03247136 -0.22658846 -0.10712802]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 7.6812587e-35 0.0000000e+00 6.8158150e-01\n",
      "  4.2426032e-01 4.8163259e-01 3.6092508e-01 6.1511189e-01 1.2441344e-01\n",
      "  3.0403408e-01 5.6519073e-02]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.31317744]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.30298105  0.44639152 -0.05094087 -1.9428495  -0.89841014\n",
      "    1.1147847  -0.7250902  -0.37364513  1...5  -0.35207042 -1.3866923\n",
      "    1.6620501   0.90082145 -0.05428009 -0.299066   -0.01887497\n",
      "    1.1639159  -0.5401236 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "   -1.5091684e+00  5.4490203e-01  6.43518...709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "    1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]])\n",
      "h0         = needle.Tensor([[[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "   -1.5091684e+00  5.4490203e-01  6.43518...709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "    1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd566df750>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[ 68., 610., 186., 772.,  30.,  77., 805., 104.,  51., 938., 769.,\n",
      "        843., 658., 334., 749.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 68. 610. 186. 772.  30.  77. 805. 104.  51. 938. 769. 843. 658. 334.\n",
      "  749.]]), needle.Tensor([[[-1....09e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "    1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd566df750>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "   -1.5091684e+00  5.4490203e-01  6.43518...709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "    1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd566df750>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[ 68. 610. 186. 772.  30.  77. 805. 104.  51. 938. 769. 843. 658. 334.\n",
      "  749.]])\n",
      "        x_emb      = needle.Tensor([[[0.8417439 ]\n",
      "  [0.5052365 ]\n",
      "  [0.931541  ]\n",
      "  [0.71145153]\n",
      "  [0.9843277 ]\n",
      "  [0.23673457]\n",
      "  [0.01159498]...0.37217447]\n",
      "  [0.04949995]\n",
      "  [0.8786141 ]\n",
      "  [0.2776738 ]\n",
      "  [0.77326363]\n",
      "  [0.5507729 ]\n",
      "  [0.94033647]\n",
      "  [0.95827866]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8417439 ]\n",
      "  [0.5052365 ]\n",
      "  [0.931541  ]\n",
      "  [0.71145153]\n",
      "  [0.9843277 ]\n",
      "  [0.23673457]\n",
      "  [0.01159498...09e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "    1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd566dd8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8417439 ]\n",
      "  [0.5052365 ]\n",
      "  [0.931541  ]\n",
      "  [0.71145153]\n",
      "  [0.9843277 ]\n",
      "  [0.23673457]\n",
      "  [0.01159498]...0.37217447]\n",
      "  [0.04949995]\n",
      "  [0.8786141 ]\n",
      "  [0.2776738 ]\n",
      "  [0.77326363]\n",
      "  [0.5507729 ]\n",
      "  [0.94033647]\n",
      "  [0.95827866]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "  -1.5091684e+00  5.4490203e-01  6.4351809...95709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "   1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]])\n",
      "        h0         = (needle.Tensor([[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "  -1.5091684e+00  5.4490203e-01  6.435180...709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "   1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]])\n",
      "        inputs     = [needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566dc490>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd566dd8d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.3...5709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "   1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566dc490>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.06066751  0.0866836   0.12085861  0.08286163  0.18579829  0.14892793\n",
      "  -0.04862347  0.12116671 -0.0...085861  0.08286163  0.18579829  0.14892793\n",
      "  -0.04862347  0.12116671 -0.06472825 -0.04612522 -0.0827193  -0.12446943]])\n",
      "        bias_ih    = needle.Tensor([[-0.1882214  -0.09768935 -0.02166569 -0.17570055  0.2793498   0.28092933\n",
      "   0.17278993  0.22386962  0.1...166569 -0.17570055  0.2793498   0.28092933\n",
      "   0.17278993  0.22386962  0.11789161  0.27375013  0.14133555  0.08956894]])\n",
      "        h          = needle.Tensor([[-1.2370228e+00 -5.6176275e-01 -1.3492256e+00 -4.0397549e-01\n",
      "  -1.5091684e+00  5.4490203e-01  6.4351809...95709e-01 -7.6558197e-01  5.3662026e-01 -9.0468651e-01\n",
      "   1.1979473e+00  1.6776990e+00 -1.5862466e+00 -2.5300694e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd566dc490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.22614922 -0.2520951  -0.18543106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]])\n",
      "        self       = needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.3...43106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566dc150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.3...43106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566dc150>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57e7d2b0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd566dce90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57e7c4b0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd566dce90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]], device=cpu())\n",
      "        b          = NDArray([[-0.22614922 -0.2520951  -0.18543106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd566dc150>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]], device=cpu())\n",
      "        b          = NDArray([[-0.22614922 -0.2520951  -0.18543106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]], device=cpu())\n",
      "other = NDArray([[-0.22614922 -0.2520951  -0.18543106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd566df670>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd566df3b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd566de9b0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.22614922 -0.2520951  -0.18543106  0.1791819  -0.00927824 -0.20545268\n",
      "   0.13103712 -0.19176961  0.02992859 -0.27337685 -0.14402622  0.10726416]], device=cuda())\n",
      "out        = NDArray([[9.6153877e-35 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.8417439 ]\n",
      " [0.5052365 ]\n",
      " [0.931541  ]\n",
      " [0.71145153]\n",
      " [0.9843277 ]\n",
      " [0.23673457]\n",
      " [0.01159498]\n",
      " [0.37217447]\n",
      " [0.04949995]\n",
      " [0.8786141 ]\n",
      " [0.2776738 ]\n",
      " [0.77326363]\n",
      " [0.5507729 ]\n",
      " [0.94033647]\n",
      " [0.95827866]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.4033723  -0.17347379 -1.2003986  -0.62258315  0.58112234\n",
      "   -0.38132995 -0.7061707  -1.2767731  -0...717 -0.38475978 -0.7567761\n",
      "   -1.2101748  -0.47516266  2.1764395   0.09091963 -1.6565772\n",
      "   -1.0104498  -0.97197604]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648\n",
      "    0.9639684  -0.96181047  1.5189248   1....9  -1.771705    0.5283334\n",
      "    0.74219084 -1.3656052  -0.39674917  0.7579036  -0.35039863\n",
      "    1.5628945   1.4461044 ]]])\n",
      "h0         = needle.Tensor([[[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648\n",
      "    0.9639684  -0.96181047  1.5189248   1....9  -1.771705    0.5283334\n",
      "    0.74219084 -1.3656052  -0.39674917  0.7579036  -0.35039863\n",
      "    1.5628945   1.4461044 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57259910>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[725., 988., 714., 226., 492., 197., 610., 720., 633., 144.,  46.,\n",
      "        525., 925., 944., 762.],\n",
      "       [788...    [257., 820., 545., 484., 262., 101., 413., 556., 738., 586., 165.,\n",
      "        238., 150., 518., 853.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[725. 988. 714. 226. 492. 197. 610. 720. 633. 144.  46. 525. 925. 944.\n",
      "  762.]\n",
      " [788. 508. 699. 730. 2...  -1.771705    0.5283334\n",
      "    0.74219084 -1.3656052  -0.39674917  0.7579036  -0.35039863\n",
      "    1.5628945   1.4461044 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57259910>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648\n",
      "    0.9639684  -0.96181047  1.5189248   1....9  -1.771705    0.5283334\n",
      "    0.74219084 -1.3656052  -0.39674917  0.7579036  -0.35039863\n",
      "    1.5628945   1.4461044 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd57259910>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[725. 988. 714. 226. 492. 197. 610. 720. 633. 144.  46. 525. 925. 944.\n",
      "  762.]\n",
      " [788. 508. 699. 730. 21...503. 802. 902. 313. 952. 351.\n",
      "  681.]\n",
      " [257. 820. 545. 484. 262. 101. 413. 556. 738. 586. 165. 238. 150. 518.\n",
      "  853.]])\n",
      "        x_emb      = needle.Tensor([[[0.0244443 ]\n",
      "  [0.6949392 ]\n",
      "  [0.2213801 ]\n",
      "  [0.36720252]\n",
      "  [0.922436  ]\n",
      "  [0.6214581 ]\n",
      "  [0.489798  ]...0.14118353]\n",
      "  [0.9791261 ]\n",
      "  [0.0326191 ]\n",
      "  [0.81698716]\n",
      "  [0.9913887 ]\n",
      "  [0.5737164 ]\n",
      "  [0.12469546]\n",
      "  [0.1190108 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.0244443 ]\n",
      "  [0.6949392 ]\n",
      "  [0.2213801 ]\n",
      "  [0.36720252]\n",
      "  [0.922436  ]\n",
      "  [0.6214581 ]\n",
      "  [0.489798  ...  -1.771705    0.5283334\n",
      "    0.74219084 -1.3656052  -0.39674917  0.7579036  -0.35039863\n",
      "    1.5628945   1.4461044 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5725bad0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.0244443 ]\n",
      "  [0.6949392 ]\n",
      "  [0.2213801 ]\n",
      "  [0.36720252]\n",
      "  [0.922436  ]\n",
      "  [0.6214581 ]\n",
      "  [0.489798  ]...0.14118353]\n",
      "  [0.9791261 ]\n",
      "  [0.0326191 ]\n",
      "  [0.81698716]\n",
      "  [0.9913887 ]\n",
      "  [0.5737164 ]\n",
      "  [0.12469546]\n",
      "  [0.1190108 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648   0.9639684\n",
      "  -0.96181047  1.5189248   1.02...86789  -1.771705    0.5283334   0.74219084\n",
      "  -1.3656052  -0.39674917  0.7579036  -0.35039863  1.5628945   1.4461044 ]])\n",
      "        h0         = (needle.Tensor([[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648   0.9639684\n",
      "  -0.96181047  1.5189248   1.0...789  -1.771705    0.5283334   0.74219084\n",
      "  -1.3656052  -0.39674917  0.7579036  -0.35039863  1.5628945   1.4461044 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]])\n",
      "        inputs     = [needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.9... [0.90904105]\n",
      " [0.35628393]\n",
      " [0.7325837 ]\n",
      " [0.43991014]\n",
      " [0.65626377]\n",
      " [0.28459588]\n",
      " [0.80381083]\n",
      " [0.81036437]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725b590>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5725bad0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.9...6789  -1.771705    0.5283334   0.74219084\n",
      "  -1.3656052  -0.39674917  0.7579036  -0.35039863  1.5628945   1.4461044 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725b590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.15898709  0.04154426 -0.22279525  0.24170768 -0.17434135 -0.24989279\n",
      "  -0.28682703  0.05502123 -0.0...279525  0.24170768 -0.17434135 -0.24989279\n",
      "  -0.28682703  0.05502123 -0.05223945 -0.15166025  0.16752523 -0.02922234]])\n",
      "        bias_ih    = needle.Tensor([[-0.031349    0.11305469  0.05738723  0.17834315 -0.20493783 -0.11999367\n",
      "  -0.26881307 -0.10093129 -0.1...738723  0.17834315 -0.20493783 -0.11999367\n",
      "  -0.26881307 -0.10093129 -0.17879118  0.03608051  0.07385933  0.10899323]])\n",
      "        h          = needle.Tensor([[ 0.05783956  0.19145387  0.02909547  0.26782802 -0.6620648   0.9639684\n",
      "  -0.96181047  1.5189248   1.02...86789  -1.771705    0.5283334   0.74219084\n",
      "  -1.3656052  -0.39674917  0.7579036  -0.35039863  1.5628945   1.4461044 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725b590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.14875038  0.23163962  0.00407192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]])\n",
      "        self       = needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.9...07192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584bac50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.9...07192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584bac50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd55bcd5f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584bbbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd55bced30>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584bbbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]], device=cpu())\n",
      "        b          = NDArray([[-0.14875038  0.23163962  0.00407192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584bac50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]], device=cpu())\n",
      "        b          = NDArray([[-0.14875038  0.23163962  0.00407192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]], device=cpu())\n",
      "other = NDArray([[-0.14875038  0.23163962  0.00407192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5725adb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57258cf0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b9d70>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.14875038  0.23163962  0.00407192  0.18436766 -0.25225362  0.12945342\n",
      "  -0.19397709 -0.03984167 -0.19444609 -0.09212112 -0.24386081  0.20702326]], device=cuda())\n",
      "out        = NDArray([[2.0456658e-33 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.0244443 ]\n",
      " [0.6949392 ]\n",
      " [0.2213801 ]\n",
      " [0.36720252]\n",
      " [0.922436  ]\n",
      " [0.6214581 ]\n",
      " [0.489798  ]\n",
      " [0.97254264]\n",
      " [0.6446319 ]\n",
      " [0.501788  ]\n",
      " [0.09653147]\n",
      " [0.47975352]\n",
      " [0.30061775]\n",
      " [0.8102432 ]\n",
      " [0.5830348 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 9.45136011e-01  8.50309789e-01 -1.38279951e+00  5.84981978e-01\n",
      "   -4.70953465e-01 -1.28850436e+00  2... -1.63635719e+00 -3.22975032e-02  8.21584046e-01\n",
      "    1.38398167e-03 -1.48953542e-01  6.41847134e-01  6.38284564e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[ 2.30030090e-01 -7.16074049e-01  3.15209717e-01  1.28214151e-01\n",
      "    1.23456693e+00  7.69744337e-01  1... -2.26424914e-02 -1.04222620e+00  4.67875972e-02\n",
      "   -6.23402357e-01  2.18332052e-01 -1.74125183e+00 -9.72262084e-01]]])\n",
      "h0         = needle.Tensor([[[ 2.30030090e-01 -7.16074049e-01  3.15209717e-01  1.28214151e-01\n",
      "    1.23456693e+00  7.69744337e-01  1... -2.26424914e-02 -1.04222620e+00  4.67875972e-02\n",
      "   -6.23402357e-01  2.18332052e-01 -1.74125183e+00 -9.72262084e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd561c2090>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[281., 366., 729.,   9., 656., 707., 193., 883., 679., 542., 472.,\n",
      "         95., 953., 589., 260.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[281. 366. 729.   9. 656. 707. 193. 883. 679. 542. 472.  95. 953. 589.\n",
      "  260.]]), needle.Tensor([[[ 2....-2.26424914e-02 -1.04222620e+00  4.67875972e-02\n",
      "   -6.23402357e-01  2.18332052e-01 -1.74125183e+00 -9.72262084e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd561c2090>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 2.30030090e-01 -7.16074049e-01  3.15209717e-01  1.28214151e-01\n",
      "    1.23456693e+00  7.69744337e-01  1... -2.26424914e-02 -1.04222620e+00  4.67875972e-02\n",
      "   -6.23402357e-01  2.18332052e-01 -1.74125183e+00 -9.72262084e-01]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd561c2090>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[281. 366. 729.   9. 656. 707. 193. 883. 679. 542. 472.  95. 953. 589.\n",
      "  260.]])\n",
      "        x_emb      = needle.Tensor([[[0.5514546 ]\n",
      "  [0.62184685]\n",
      "  [0.5452215 ]\n",
      "  [0.99856716]\n",
      "  [0.8602587 ]\n",
      "  [0.17649354]\n",
      "  [0.03285117]...0.20199765]\n",
      "  [0.74052966]\n",
      "  [0.68964493]\n",
      "  [0.47060567]\n",
      "  [0.697882  ]\n",
      "  [0.48456243]\n",
      "  [0.3878889 ]\n",
      "  [0.7382396 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5514546 ]\n",
      "  [0.62184685]\n",
      "  [0.5452215 ]\n",
      "  [0.99856716]\n",
      "  [0.8602587 ]\n",
      "  [0.17649354]\n",
      "  [0.03285117...-2.26424914e-02 -1.04222620e+00  4.67875972e-02\n",
      "   -6.23402357e-01  2.18332052e-01 -1.74125183e+00 -9.72262084e-01]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd561c3510>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5514546 ]\n",
      "  [0.62184685]\n",
      "  [0.5452215 ]\n",
      "  [0.99856716]\n",
      "  [0.8602587 ]\n",
      "  [0.17649354]\n",
      "  [0.03285117]...0.20199765]\n",
      "  [0.74052966]\n",
      "  [0.68964493]\n",
      "  [0.47060567]\n",
      "  [0.697882  ]\n",
      "  [0.48456243]\n",
      "  [0.3878889 ]\n",
      "  [0.7382396 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 2.3003009e-01 -7.1607405e-01  3.1520972e-01  1.2821415e-01\n",
      "   1.2345669e+00  7.6974434e-01  1.2989564...51177e-01  8.8920611e-01 -2.9228443e-01 -5.7840455e-01\n",
      "   4.9838626e-01  2.0338204e+00  1.5856265e+00 -3.9346650e-02]])\n",
      "        h0         = (needle.Tensor([[ 2.3003009e-01 -7.1607405e-01  3.1520972e-01  1.2821415e-01\n",
      "   1.2345669e+00  7.6974434e-01  1.298956...5597   0.1234908  -1.2145118  -0.02264249\n",
      "  -1.0422262   0.0467876  -0.62340236  0.21833205 -1.7412518  -0.9722621 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]])\n",
      "        inputs     = [needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c1f90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd561c3510>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.2...1177e-01  8.8920611e-01 -2.9228443e-01 -5.7840455e-01\n",
      "   4.9838626e-01  2.0338204e+00  1.5856265e+00 -3.9346650e-02]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c1f90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.09200579  0.08242929 -0.13765244 -0.1687307  -0.16554248 -0.21756412\n",
      "  -0.13833436 -0.22656305  0.0...765244 -0.1687307  -0.16554248 -0.21756412\n",
      "  -0.13833436 -0.22656305  0.0362241  -0.09784682  0.24110079 -0.0932743 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.16594166  0.25447822  0.23460776 -0.13354194 -0.19144969 -0.19865242\n",
      "  -0.07832967 -0.26049188 -0.2...460776 -0.13354194 -0.19144969 -0.19865242\n",
      "  -0.07832967 -0.26049188 -0.23041822 -0.14608714 -0.23041911 -0.09007744]])\n",
      "        h          = needle.Tensor([[ 2.3003009e-01 -7.1607405e-01  3.1520972e-01  1.2821415e-01\n",
      "   1.2345669e+00  7.6974434e-01  1.2989564...51177e-01  8.8920611e-01 -2.9228443e-01 -5.7840455e-01\n",
      "   4.9838626e-01  2.0338204e+00  1.5856265e+00 -3.9346650e-02]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c1f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.04612297 -0.09021655 -0.2059907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]])\n",
      "        self       = needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.2...59907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aae10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.2...59907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aae10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd578af270>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd587a93d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd578af9f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd587a93d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.04612297 -0.09021655 -0.2059907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aae10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.04612297 -0.09021655 -0.2059907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]], device=cpu())\n",
      "other = NDArray([[ 0.04612297 -0.09021655 -0.2059907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587a94b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd561c18b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587ab830>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.04612297 -0.09021655 -0.2059907   0.08359507  0.28000343 -0.2685052\n",
      "  -0.12298846 -0.11309154 -0.14234369 -0.05781847 -0.12862201 -0.20750442]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 3.4485970e+13 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.5514546 ]\n",
      " [0.62184685]\n",
      " [0.5452215 ]\n",
      " [0.99856716]\n",
      " [0.8602587 ]\n",
      " [0.17649354]\n",
      " [0.03285117]\n",
      " [0.20199765]\n",
      " [0.74052966]\n",
      " [0.68964493]\n",
      " [0.47060567]\n",
      " [0.697882  ]\n",
      " [0.48456243]\n",
      " [0.3878889 ]\n",
      " [0.7382396 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 5.81086636e-01 -1.87083796e-01  6.59593880e-01 -8.35031867e-01\n",
      "    1.51084769e+00 -1.82881141e+00  8... -7.27733970e-01 -6.39921665e-01 -2.23030972e+00\n",
      "    1.32913244e+00  3.54646593e-01 -1.93109006e-01  9.45489943e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = needle.Tensor([[[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "   -6.1889249e-01  1.4634957e+00  6.62494...463e-01  7.7337243e-02 -6.8684322e-01 -1.0911864e+00\n",
      "   -1.1388727e-01 -8.6536318e-01 -1.0050801e+00  7.3022589e-02]]])\n",
      "h0         = needle.Tensor([[[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "   -6.1889249e-01  1.4634957e+00  6.62494...463e-01  7.7337243e-02 -6.8684322e-01 -1.0911864e+00\n",
      "   -1.1388727e-01 -8.6536318e-01 -1.0050801e+00  7.3022589e-02]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584b05d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[827., 374., 562., 936., 425., 123., 479., 213., 695., 285., 120.,\n",
      "        422., 172., 586., 891.],\n",
      "       [988...    [ 13., 459.,  47., 345., 815., 242., 251., 175., 484., 619., 695.,\n",
      "        755., 864., 901., 573.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[827. 374. 562. 936. 425. 123. 479. 213. 695. 285. 120. 422. 172. 586.\n",
      "  891.]\n",
      " [988. 961. 566. 906.  ...63e-01  7.7337243e-02 -6.8684322e-01 -1.0911864e+00\n",
      "   -1.1388727e-01 -8.6536318e-01 -1.0050801e+00  7.3022589e-02]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b05d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "   -6.1889249e-01  1.4634957e+00  6.62494...463e-01  7.7337243e-02 -6.8684322e-01 -1.0911864e+00\n",
      "   -1.1388727e-01 -8.6536318e-01 -1.0050801e+00  7.3022589e-02]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b05d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[827. 374. 562. 936. 425. 123. 479. 213. 695. 285. 120. 422. 172. 586.\n",
      "  891.]\n",
      " [988. 961. 566. 906.  4... 12. 920. 240. 188. 959. 441.\n",
      "    0.]\n",
      " [ 13. 459.  47. 345. 815. 242. 251. 175. 484. 619. 695. 755. 864. 901.\n",
      "  573.]])\n",
      "        x_emb      = needle.Tensor([[[4.65218216e-01]\n",
      "  [9.46323931e-01]\n",
      "  [2.71654755e-01]\n",
      "  [7.07582176e-01]\n",
      "  [5.99941015e-01]\n",
      "  [8.4636...1]\n",
      "  [5.96964359e-01]\n",
      "  [1.47403598e-01]\n",
      "  [3.40463340e-01]\n",
      "  [5.80017567e-01]\n",
      "  [4.28570598e-01]\n",
      "  [6.61750674e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[4.65218216e-01]\n",
      "  [9.46323931e-01]\n",
      "  [2.71654755e-01]\n",
      "  [7.07582176e-01]\n",
      "  [5.99941015e-01]\n",
      "  [8.463...63e-01  7.7337243e-02 -6.8684322e-01 -1.0911864e+00\n",
      "   -1.1388727e-01 -8.6536318e-01 -1.0050801e+00  7.3022589e-02]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b1f10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[4.65218216e-01]\n",
      "  [9.46323931e-01]\n",
      "  [2.71654755e-01]\n",
      "  [7.07582176e-01]\n",
      "  [5.99941015e-01]\n",
      "  [8.4636...1]\n",
      "  [5.96964359e-01]\n",
      "  [1.47403598e-01]\n",
      "  [3.40463340e-01]\n",
      "  [5.80017567e-01]\n",
      "  [4.28570598e-01]\n",
      "  [6.61750674e-01]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "  -6.1889249e-01  1.4634957e+00  6.6249406...50891e-01  6.9255352e-01  1.1240829e-01 -8.0244295e-02\n",
      "   2.0940179e-02 -3.8175008e-01  1.0398610e+00  1.4464483e-01]])\n",
      "        h0         = (needle.Tensor([[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "  -6.1889249e-01  1.4634957e+00  6.624940...83615 -0.043667   -0.25117463  0.07733724\n",
      "  -0.6868432  -1.0911864  -0.11388727 -0.8653632  -1.0050801   0.07302259]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]])\n",
      "        inputs     = [needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.6... [0.4410764 ]\n",
      " [0.86087126]\n",
      " [0.16712224]\n",
      " [0.59696436]\n",
      " [0.9094003 ]\n",
      " [0.0171454 ]\n",
      " [0.2777726 ]\n",
      " [0.47209498]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b2850>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b1f10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.6...0891e-01  6.9255352e-01  1.1240829e-01 -8.0244295e-02\n",
      "   2.0940179e-02 -3.8175008e-01  1.0398610e+00  1.4464483e-01]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b2850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.22094554  0.23154068 -0.08600581 -0.12673126 -0.27470532  0.27551907\n",
      "   0.02612051 -0.0775965   0.1...600581 -0.12673126 -0.27470532  0.27551907\n",
      "   0.02612051 -0.0775965   0.12070569 -0.17427865  0.13003597 -0.14990468]])\n",
      "        bias_ih    = needle.Tensor([[ 0.26839942 -0.00747937 -0.03694329 -0.1075248  -0.21625814 -0.269594\n",
      "   0.02637514 -0.12189674 -0.084...03694329 -0.1075248  -0.21625814 -0.269594\n",
      "   0.02637514 -0.12189674 -0.08492726  0.10517883  0.22081381  0.17795327]])\n",
      "        h          = needle.Tensor([[-6.0733366e-01 -2.1340325e+00 -5.2499343e-02 -1.8127002e-01\n",
      "  -6.1889249e-01  1.4634957e+00  6.6249406...50891e-01  6.9255352e-01  1.1240829e-01 -8.0244295e-02\n",
      "   2.0940179e-02 -3.8175008e-01  1.0398610e+00  1.4464483e-01]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b2850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.11731222 -0.27713975  0.27970392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]])\n",
      "        self       = needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.6...70392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56485490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.6...70392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56485490>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56c5bbf0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd56486690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56c580b0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd56486690>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]], device=cpu())\n",
      "        b          = NDArray([[-0.11731222 -0.27713975  0.27970392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56485490>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]], device=cpu())\n",
      "        b          = NDArray([[-0.11731222 -0.27713975  0.27970392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]], device=cpu())\n",
      "other = NDArray([[-0.11731222 -0.27713975  0.27970392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573a63b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b2b30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56487ab0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.11731222 -0.27713975  0.27970392  0.18705693 -0.24625988  0.07033128\n",
      "  -0.01564655 -0.15797314  0.21010312  0.23756236 -0.02373952  0.04658484]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 6.1379807e+14 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.46521822]\n",
      " [0.94632393]\n",
      " [0.27165475]\n",
      " [0.7075822 ]\n",
      " [0.599941  ]\n",
      " [0.8463647 ]\n",
      " [0.05671145]\n",
      " [0.68858993]\n",
      " [0.1474036 ]\n",
      " [0.59791684]\n",
      " [0.37449092]\n",
      " [0.22123802]\n",
      " [0.69990313]\n",
      " [0.768026  ]\n",
      " [0.44628793]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.7613289   1.4330641   0.14047766  2.870479    0.48073947\n",
      "    1.3901386   0.8090212   0.30197605  0.49592254 -0.68246686\n",
      "    1.3656285  -0.7515805 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306\n",
      "   -0.06360489 -1.3314799  -0.2774736  -0.66548616 -1.1141677\n",
      "    0.75815564 -0.28035876]]])\n",
      "h0         = needle.Tensor([[[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306\n",
      "   -0.06360489 -1.3314799  -0.2774736  -0.66548616 -1.1141677\n",
      "    0.75815564 -0.28035876]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd563875d0>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[208.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[208.]]), needle.Tensor([[[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306\n",
      "   -0.06360489 -1.3314799  -0.2774736  -0.66548616 -1.1141677\n",
      "    0.75815564 -0.28035876]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd563875d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306\n",
      "   -0.06360489 -1.3314799  -0.2774736  -0.66548616 -1.1141677\n",
      "    0.75815564 -0.28035876]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd563875d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[208.]])\n",
      "        x_emb      = needle.Tensor([[[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "   0.43006223 0.9906824  0.63814414 ...\n",
      "   0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "   0.46800253 0.68779993 0.729881   0.28443757]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "   0.43006223 0.9906824  0.63814414...   1.7515246  -0.96069306\n",
      "   -0.06360489 -1.3314799  -0.2774736  -0.66548616 -1.1141677\n",
      "    0.75815564 -0.28035876]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56385d90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "   0.43006223 0.9906824  0.63814414 ...\n",
      "   0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "   0.46800253 0.68779993 0.729881   0.28443757]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306 -0.06360489\n",
      "  -1.3314799  -0.2774736  -0.66548616 -1.1141677   0.75815564 -0.28035876]])\n",
      "        h0         = (needle.Tensor([[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306 -0.06360489\n",
      "  -1.3314799  -0.2774736  -0.66548616 -1.1141677   0.75815564 -0.28035876]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0....575\n",
      "  0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]])\n",
      "        inputs     = [needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0...75\n",
      "  0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563857d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56385d90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0...007    1.7515246  -0.96069306 -0.06360489\n",
      "  -1.3314799  -0.2774736  -0.66548616 -1.1141677   0.75815564 -0.28035876]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563857d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0....575\n",
      "  0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.0108794   0.14165321  0.03875253  0.10050946  0.04648396 -0.27435544\n",
      "   0.09030998  0.178047    0.18838173 -0.10168505  0.16651943 -0.00055858]])\n",
      "        bias_ih    = needle.Tensor([[-0.10329117 -0.04552621  0.03011838 -0.23487365 -0.07883434  0.13687631\n",
      "   0.24891043 -0.11430182 -0.2754942   0.23321813 -0.12348323 -0.23547152]])\n",
      "        h          = needle.Tensor([[ 0.524288   -0.4488012   0.920007    1.7515246  -0.96069306 -0.06360489\n",
      "  -1.3314799  -0.2774736  -0.66548616 -1.1141677   0.75815564 -0.28035876]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563857d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 1.40187144e-01 -1.90557718e-01  5.05295992e-02  8.69665146e-02\n",
      "   1.67759597e-01  2.07846195e-01 -1.2...01  5.21379709e-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]])\n",
      "        self       = needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0....575\n",
      "  0.9442546  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0...1  5.21379709e-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58702510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0...1  5.21379709e-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58702510>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5810e270>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd58701350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5810dcf0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd58701350>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0.534240...6  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]], device=cpu())\n",
      "        b          = NDArray([[ 1.40187144e-01 -1.90557718e-01  5.05295992e-02  8.69665146e-02\n",
      "   1.67759597e-01  2.07846195e-01 -1.2834727...-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58702510>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0.534240...6  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]], device=cpu())\n",
      "        b          = NDArray([[ 1.40187144e-01 -1.90557718e-01  5.05295992e-02  8.69665146e-02\n",
      "   1.67759597e-01  2.07846195e-01 -1.2834727...-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0.534240...6  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]], device=cpu())\n",
      "other = NDArray([[ 1.40187144e-01 -1.90557718e-01  5.05295992e-02  8.69665146e-02\n",
      "   1.67759597e-01  2.07846195e-01 -1.2834727...-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56385f70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56386570>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58701b30>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 1.40187144e-01 -1.90557718e-01  5.05295992e-02  8.69665146e-02\n",
      "   1.67759597e-01  2.07846195e-01 -1.2834727...-02 -1.11644357e-01  1.90760612e-01\n",
      "  -1.58453330e-01 -1.76424474e-01 -1.41025230e-01  1.09673768e-01]], device=cuda())\n",
      "out        = NDArray([[ 6.2627308e-35  0.0000000e+00  7.6812587e-35  0.0000000e+00\n",
      "   8.3716810e-02 -5.9016562e-01  6.5045005e-01  5.5913210e-02\n",
      "   4.5600599e-01  6.2021333e-01  2.0055872e-01 -6.1161053e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.6047139  0.9343471  0.8550005  0.5025846  0.7244332  0.0977432\n",
      "  0.43006223 0.9906824  0.63814414 0.534240...6  0.12151557 0.794651   0.14964367 0.34478825 0.8232811\n",
      "  0.46800253 0.68779993 0.729881   0.28443757]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-2.405438    1.6754975   0.5660172   0.45797613 -0.2391421\n",
      "    2.0339577   0.9535217  -0.55215394  0.5849293  -2.5409179\n",
      "    0.7303631   1.1575189 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004\n",
      "   -1.7932112   0.294797   -3.3891342  -1.774949    0.9067326\n",
      "    0.60037893 -1.17518   ]]])\n",
      "h0         = needle.Tensor([[[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004\n",
      "   -1.7932112   0.294797   -3.3891342  -1.774949    0.9067326\n",
      "    0.60037893 -1.17518   ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56485e90>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[702.],\n",
      "       [967.],\n",
      "       [936.],\n",
      "       [976.],\n",
      "       [888.],\n",
      "       [100.],\n",
      "       [638.],\n",
      "       [274.],\n",
      "       [671.],\n",
      "       [710.],\n",
      "       [632.],\n",
      "       [705.],\n",
      "       [508.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[702.]\n",
      " [967.]\n",
      " [936.]\n",
      " [976.]\n",
      " [888.]\n",
      " [100.]\n",
      " [638.]\n",
      " [274.]\n",
      " [671.]\n",
      " [710.]\n",
      " [632.]\n",
      " [705.]\n",
      " [508.]...  -0.65539205  0.14950004\n",
      "   -1.7932112   0.294797   -3.3891342  -1.774949    0.9067326\n",
      "    0.60037893 -1.17518   ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56485e90>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004\n",
      "   -1.7932112   0.294797   -3.3891342  -1.774949    0.9067326\n",
      "    0.60037893 -1.17518   ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56485e90>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[702.]\n",
      " [967.]\n",
      " [936.]\n",
      " [976.]\n",
      " [888.]\n",
      " [100.]\n",
      " [638.]\n",
      " [274.]\n",
      " [671.]\n",
      " [710.]\n",
      " [632.]\n",
      " [705.]\n",
      " [508.]])\n",
      "        x_emb      = needle.Tensor([[[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "   0.9497411  0.09499533 0.70749694...\n",
      "   0.8199505  0.03336114 0.34190637 0.30777514 0.8361863  0.8932631\n",
      "   0.7605565  0.4982259  0.06095067 0.9385783 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "   0.9497411  0.09499533 0.7074969...  -0.65539205  0.14950004\n",
      "   -1.7932112   0.294797   -3.3891342  -1.774949    0.9067326\n",
      "    0.60037893 -1.17518   ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56487850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "   0.9497411  0.09499533 0.70749694...\n",
      "   0.8199505  0.03336114 0.34190637 0.30777514 0.8361863  0.8932631\n",
      "   0.7605565  0.4982259  0.06095067 0.9385783 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004 -1.7932112\n",
      "   0.294797   -3.3891342  -1.774949    0.9067326   0.60037893 -1.17518   ]])\n",
      "        h0         = (needle.Tensor([[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004 -1.7932112\n",
      "   0.294797   -3.3891342  -1.774949    0.9067326   0.60037893 -1.17518   ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0...424\n",
      "  0.15784334 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]])\n",
      "        inputs     = [needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 ....491656   0.39404476 0.5170226  0.8726742  0.9815805  0.21341728\n",
      "  0.19083713 0.22013025 0.83615947 0.5559828 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56485390>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56487850>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 ...87512  -0.65539205  0.14950004 -1.7932112\n",
      "   0.294797   -3.3891342  -1.774949    0.9067326   0.60037893 -1.17518   ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56485390>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0...424\n",
      "  0.15784334 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.08186358 -0.25579077 -0.10528076 -0.21938837  0.13435581 -0.13221316\n",
      "   0.14178026 -0.2013711  -0.26969942 -0.03772774 -0.08602972  0.27764618]])\n",
      "        bias_ih    = needle.Tensor([[-0.0624975  -0.23183323 -0.08330908  0.00220644 -0.13752043 -0.2023944\n",
      "  -0.1655739  -0.12484118  0.02687448  0.2685318  -0.17876649 -0.18308243]])\n",
      "        h          = needle.Tensor([[ 0.3710387   0.3200789  -1.4987512  -0.65539205  0.14950004 -1.7932112\n",
      "   0.294797   -3.3891342  -1.774949    0.9067326   0.60037893 -1.17518   ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56485390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.00695148 -0.0197981  -0.27533734 -0.04843228 -0.02030241 -0.12403066\n",
      "   0.01668027 -0.16375655 -0.2...9452417  0.12046337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]])\n",
      "        self       = needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0...424\n",
      "  0.15784334 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 ...452417  0.12046337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f130d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 ...452417  0.12046337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f130d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ebf1b0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56f11dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ebf570>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56f11dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0.97452...34 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.00695148 -0.0197981  -0.27533734 -0.04843228 -0.02030241 -0.12403066\n",
      "   0.01668027 -0.16375655 -0.2711386...6337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56f130d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0.97452...34 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.00695148 -0.0197981  -0.27533734 -0.04843228 -0.02030241 -0.12403066\n",
      "   0.01668027 -0.16375655 -0.2711386...6337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0.97452...34 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]], device=cpu())\n",
      "other = NDArray([[ 0.00695148 -0.0197981  -0.27533734 -0.04843228 -0.02030241 -0.12403066\n",
      "   0.01668027 -0.16375655 -0.2711386...6337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5830cd30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56487230>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56f106b0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.00695148 -0.0197981  -0.27533734 -0.04843228 -0.02030241 -0.12403066\n",
      "   0.01668027 -0.16375655 -0.2711386...6337  0.23195022  0.1726102\n",
      "  -0.14186998 -0.08002184  0.20720273  0.08764553 -0.09920783  0.01209921]], device=cuda())\n",
      "out        = NDArray([[ 4.5576180e-34  0.0000000e+00 -3.2111335e-01 -2.3464131e-01\n",
      "  -6.2639993e-01 -9.3599141e-02  2.1384120e-02 -1.6338038e-01\n",
      "   5.7449490e-01 -4.1260764e-01  1.5970469e-03 -7.2128296e-02]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.4452907  0.9880867  0.23028873 0.46911347 0.5820604  0.33580354\n",
      "  0.9497411  0.09499533 0.70749694 0.97452...34 0.8994156  0.10891341 0.9611173  0.63742244 0.6773488\n",
      "  0.4285007  0.22478177 0.5937715  0.9821695 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.698766   -1.8308039  -1.7583079  -0.722409   -0.14333625\n",
      "   -0.6104735   0.05178177 -0.24648568 -1...333 -0.61469924 -0.6660059\n",
      "    0.37161273  0.9930895   0.5886929  -0.26949134  3.5789683\n",
      "   -1.6029606  -1.2073406 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607\n",
      "    0.5179346   0.00642321 -1.3721193   1...57  2.4456365   1.7881161\n",
      "   -0.8216934  -0.4284506  -0.053561    1.1217643   0.21522106\n",
      "   -0.45727316  0.6662457 ]]])\n",
      "h0         = needle.Tensor([[[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607\n",
      "    0.5179346   0.00642321 -1.3721193   1...57  2.4456365   1.7881161\n",
      "   -0.8216934  -0.4284506  -0.053561    1.1217643   0.21522106\n",
      "   -0.45727316  0.6662457 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584b3ad0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[382.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[382.]]), needle.Tensor([[[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607\n",
      "    0.5179346  ...7  2.4456365   1.7881161\n",
      "   -0.8216934  -0.4284506  -0.053561    1.1217643   0.21522106\n",
      "   -0.45727316  0.6662457 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b3ad0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607\n",
      "    0.5179346   0.00642321 -1.3721193   1...57  2.4456365   1.7881161\n",
      "   -0.8216934  -0.4284506  -0.053561    1.1217643   0.21522106\n",
      "   -0.45727316  0.6662457 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b3ad0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[382.]])\n",
      "        x_emb      = needle.Tensor([[[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "   0.00805822 0.3408254  0.86592126...   0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "   0.29259032 0.9799878  0.9614281  0.46163663]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "   0.00805822 0.3408254  0.8659212...7  2.4456365   1.7881161\n",
      "   -0.8216934  -0.4284506  -0.053561    1.1217643   0.21522106\n",
      "   -0.45727316  0.6662457 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b0690>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "   0.00805822 0.3408254  0.86592126...   0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "   0.29259032 0.9799878  0.9614281  0.46163663]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607  0.5179346\n",
      "   0.00642321 -1.3721193   1.1854372  -0.60805744  0.55202365 -0.9229924 ]])\n",
      "        h0         = (needle.Tensor([[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607  0.5179346\n",
      "   0.00642321 -1.3721193   1.1...078957  2.4456365   1.7881161  -0.8216934\n",
      "  -0.4284506  -0.053561    1.1217643   0.21522106 -0.45727316  0.6662457 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0...18\n",
      "  0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]])\n",
      "        inputs     = [needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 ...8\n",
      "  0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3fd0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b0690>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 ...634682  0.37626746 -0.48441607  0.5179346\n",
      "   0.00642321 -1.3721193   1.1854372  -0.60805744  0.55202365 -0.9229924 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0...18\n",
      "  0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.16856703 -0.08871453  0.18318522 -0.11757103 -0.12712443 -0.23655185\n",
      "  -0.28495517 -0.23167595 -0.25114846 -0.15828128  0.23460358 -0.28798738]])\n",
      "        bias_ih    = needle.Tensor([[ 0.03426364 -0.03235361  0.13418046  0.2649879  -0.11780064 -0.16414341\n",
      "  -0.06960756  0.27242875 -0.17061912  0.27674806  0.07807553  0.02612916]])\n",
      "        h          = needle.Tensor([[ 0.2175252   1.2681214   0.13634682  0.37626746 -0.48441607  0.5179346\n",
      "   0.00642321 -1.3721193   1.1854372  -0.60805744  0.55202365 -0.9229924 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b3fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.22494894 -0.03271562 -0.04622275  0.19308147  0.01343259 -0.00236335\n",
      "   0.22603804  0.08656362  0.1...855873  0.16274732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]])\n",
      "        self       = needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0...18\n",
      "  0.4462823  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 ...55873  0.16274732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725a990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 ...55873  0.16274732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725a990>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ddbb70>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57259c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57dd8c30>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd57259c10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0.90122...  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]], device=cpu())\n",
      "        b          = NDArray([[ 0.22494894 -0.03271562 -0.04622275  0.19308147  0.01343259 -0.00236335\n",
      "   0.22603804  0.08656362  0.1382604...732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725a990>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0.90122...  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]], device=cpu())\n",
      "        b          = NDArray([[ 0.22494894 -0.03271562 -0.04622275  0.19308147  0.01343259 -0.00236335\n",
      "   0.22603804  0.08656362  0.1382604...732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0.90122...  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]], device=cpu())\n",
      "other = NDArray([[ 0.22494894 -0.03271562 -0.04622275  0.19308147  0.01343259 -0.00236335\n",
      "   0.22603804  0.08656362  0.1382604...732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57258bf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b2330>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5725b270>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.22494894 -0.03271562 -0.04622275  0.19308147  0.01343259 -0.00236335\n",
      "   0.22603804  0.08656362  0.1382604...732 -0.01369807  0.12196505\n",
      "   0.2848035   0.08477554 -0.2126737  -0.28424844 -0.13970332 -0.15455799]], device=cuda())\n",
      "out        = NDArray([[ 2.3144126e-33  0.0000000e+00 -6.3603878e-02 -2.0858631e-01\n",
      "   3.9139014e-01  1.2190443e-01 -5.8477026e-01 -5.6647491e-01\n",
      "   3.7093085e-01 -3.3700526e-02 -6.2569195e-01  6.1317581e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.35993078 0.87804466 0.2815188  0.5242576  0.47169688 0.12268221\n",
      "  0.00805822 0.3408254  0.86592126 0.90122...  0.26572156 0.20017219 0.5599379  0.6271215  0.84561914\n",
      "  0.29259032 0.9799878  0.9614281  0.46163663]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.05139183  1.0616511  -0.22104894 -0.11150302  0.16832547\n",
      "    0.4144065   1.3959293   0.08550373 -1...33 -0.28707436  2.2337813\n",
      "    0.65853405  0.56763566 -0.57332474 -0.19035357  0.92098856\n",
      "   -0.3223546  -0.00445204]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383\n",
      "    1.3989267  -0.15040545 -0.75832045  1....61  1.3829849  -0.48989218\n",
      "   -0.88128763 -0.34566805 -0.5638874   2.758195    0.7267934\n",
      "    0.40326598 -0.984426  ]]])\n",
      "h0         = needle.Tensor([[[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383\n",
      "    1.3989267  -0.15040545 -0.75832045  1....61  1.3829849  -0.48989218\n",
      "   -0.88128763 -0.34566805 -0.5638874   2.758195    0.7267934\n",
      "    0.40326598 -0.984426  ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584a4d10>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[398.],\n",
      "       [825.],\n",
      "       [735.],\n",
      "       [678.],\n",
      "       [ 80.],\n",
      "       [640.],\n",
      "       [574.],\n",
      "       [ 42.],\n",
      "       [417.],\n",
      "       [405.],\n",
      "       [ 14.],\n",
      "       [987.],\n",
      "       [342.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[398.]\n",
      " [825.]\n",
      " [735.]\n",
      " [678.]\n",
      " [ 80.]\n",
      " [640.]\n",
      " [574.]\n",
      " [ 42.]\n",
      " [417.]\n",
      " [405.]\n",
      " [ 14.]\n",
      " [987.]\n",
      " [342.]...1  1.3829849  -0.48989218\n",
      "   -0.88128763 -0.34566805 -0.5638874   2.758195    0.7267934\n",
      "    0.40326598 -0.984426  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a4d10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = needle.Tensor([[[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383\n",
      "    1.3989267  -0.15040545 -0.75832045  1....61  1.3829849  -0.48989218\n",
      "   -0.88128763 -0.34566805 -0.5638874   2.758195    0.7267934\n",
      "    0.40326598 -0.984426  ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a4d10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[398.]\n",
      " [825.]\n",
      " [735.]\n",
      " [678.]\n",
      " [ 80.]\n",
      " [640.]\n",
      " [574.]\n",
      " [ 42.]\n",
      " [417.]\n",
      " [405.]\n",
      " [ 14.]\n",
      " [987.]\n",
      " [342.]])\n",
      "        x_emb      = needle.Tensor([[[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "   0.39876536 0.08599018 0.20988095 ...   0.8712218  0.18591397 0.37504703 0.12967296 0.8642374  0.10137609\n",
      "   0.6608129  0.63188577 0.48111236 0.9971699 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "   0.39876536 0.08599018 0.20988095...1  1.3829849  -0.48989218\n",
      "   -0.88128763 -0.34566805 -0.5638874   2.758195    0.7267934\n",
      "    0.40326598 -0.984426  ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a5d50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "   0.39876536 0.08599018 0.20988095 ...   0.8712218  0.18591397 0.37504703 0.12967296 0.8642374  0.10137609\n",
      "   0.6608129  0.63188577 0.48111236 0.9971699 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383   1.3989267\n",
      "  -0.15040545 -0.75832045  1.932191   -1.9764274   0.80157924  0.380969  ]])\n",
      "        h0         = (needle.Tensor([[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383   1.3989267\n",
      "  -0.15040545 -0.75832045  1.9...99061  1.3829849  -0.48989218 -0.88128763\n",
      "  -0.34566805 -0.5638874   2.758195    0.7267934   0.40326598 -0.984426  ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0....352\n",
      "  0.9272569  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]])\n",
      "        inputs     = [needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0...0.15919824 0.76556075 0.72024965 0.32062072 0.63631487 0.8671823\n",
      "  0.41104022 0.46165428 0.3516995  0.20514178]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a6a90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a5d50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0...87088  -0.26795623  2.1095383   1.3989267\n",
      "  -0.15040545 -0.75832045  1.932191   -1.9764274   0.80157924  0.380969  ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a6a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0....352\n",
      "  0.9272569  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.06574833 -0.27640036 -0.22179203 -0.17167604 -0.17100832 -0.0725437\n",
      "  -0.01625234 -0.23829009  0.27514154  0.23014891 -0.21648875  0.13749138]])\n",
      "        bias_ih    = needle.Tensor([[-0.25399655 -0.12689625  0.25167394  0.22099048 -0.09196004  0.02479848\n",
      "   0.17564532  0.22991574  0.2651795   0.2668767   0.2153213   0.05897567]])\n",
      "        h          = needle.Tensor([[-0.09545552 -0.77327836  2.6187088  -0.26795623  2.1095383   1.3989267\n",
      "  -0.15040545 -0.75832045  1.932191   -1.9764274   0.80157924  0.380969  ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a6a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.12714851e-01 -5.55738807e-02 -7.65883923e-03 -1.95179328e-01\n",
      "  -1.33964032e-01 -8.89807940e-04  1.8...01  5.28863072e-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]])\n",
      "        self       = needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0....352\n",
      "  0.9272569  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0...1  5.28863072e-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bb3810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0...1  5.28863072e-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bb3810>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58770270>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56bb0d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58772ab0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd56bb0d90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0.996090...9  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]], device=cpu())\n",
      "        b          = NDArray([[-2.12714851e-01 -5.55738807e-02 -7.65883923e-03 -1.95179328e-01\n",
      "  -1.33964032e-01 -8.89807940e-04  1.8567916...-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bb3810>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0.996090...9  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]], device=cpu())\n",
      "        b          = NDArray([[-2.12714851e-01 -5.55738807e-02 -7.65883923e-03 -1.95179328e-01\n",
      "  -1.33964032e-01 -8.89807940e-04  1.8567916...-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0.996090...9  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]], device=cpu())\n",
      "other = NDArray([[-2.12714851e-01 -5.55738807e-02 -7.65883923e-03 -1.95179328e-01\n",
      "  -1.33964032e-01 -8.89807940e-04  1.8567916...-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57124130>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584a6e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56bb1b70>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-2.12714851e-01 -5.55738807e-02 -7.65883923e-03 -1.95179328e-01\n",
      "  -1.33964032e-01 -8.89807940e-04  1.8567916...-03  9.47797298e-02 -1.88572794e-01\n",
      "   9.53159332e-02  1.60704404e-01 -9.02634263e-02 -1.22221529e-01]], device=cuda())\n",
      "out        = NDArray([[ 8.1775126e+35  4.5823861e-41  2.7177400e-33  0.0000000e+00\n",
      "  -6.1766124e-01 -2.6368451e-01  6.6634971e-01 -5.3749490e-01\n",
      "   2.7000022e-01 -3.4298208e-01 -1.3265949e-01  3.6954421e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.79896104 0.10661613 0.8598379  0.15476656 0.8541403  0.5234437\n",
      "  0.39876536 0.08599018 0.20988095 0.996090...9  0.19022423 0.27662313 0.41309142 0.41126904 0.6768926\n",
      "  0.9230754  0.9465222  0.35541728 0.92534786]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-6.96603477e-01  7.04434991e-01 -3.50397915e-01 -3.85371506e-01\n",
      "    8.30142856e-01  9.89743248e-02  6... -5.38219273e-01 -1.11829978e-03 -8.45776200e-01\n",
      "    2.39930466e-01 -8.39460194e-01  2.27842137e-01  2.07301885e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518\n",
      "   -0.13268296  0.24020547  0.7182372   0....175  -0.00601085 -1.9357834\n",
      "    0.32361984 -1.6383653   0.6919574   0.1617431  -2.586448\n",
      "   -1.8231784  -2.2502906 ]]])\n",
      "h0         = needle.Tensor([[[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518\n",
      "   -0.13268296  0.24020547  0.7182372   0....175  -0.00601085 -1.9357834\n",
      "    0.32361984 -1.6383653   0.6919574   0.1617431  -2.586448\n",
      "   -1.8231784  -2.2502906 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56326110>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[998., 142., 898., 284., 508., 745.,  85., 349., 483., 377., 232.,\n",
      "        885., 927., 856., 823.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[998. 142. 898. 284. 508. 745.  85. 349. 483. 377. 232. 885. 927. 856.\n",
      "  823.]]), needle.Tensor([[[ 0....75  -0.00601085 -1.9357834\n",
      "    0.32361984 -1.6383653   0.6919574   0.1617431  -2.586448\n",
      "   -1.8231784  -2.2502906 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56326110>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518\n",
      "   -0.13268296  0.24020547  0.7182372   0....175  -0.00601085 -1.9357834\n",
      "    0.32361984 -1.6383653   0.6919574   0.1617431  -2.586448\n",
      "   -1.8231784  -2.2502906 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd56326110>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[998. 142. 898. 284. 508. 745.  85. 349. 483. 377. 232. 885. 927. 856.\n",
      "  823.]])\n",
      "        x_emb      = needle.Tensor([[[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "   2.58968860e-01 1.43062845e-01 2.127575...e-01 7.94565827e-02\n",
      "   7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "   7.25015521e-01 4.72886264e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "   2.58968860e-01 1.43062845e-01 2.12757...75  -0.00601085 -1.9357834\n",
      "    0.32361984 -1.6383653   0.6919574   0.1617431  -2.586448\n",
      "   -1.8231784  -2.2502906 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56325f50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "   2.58968860e-01 1.43062845e-01 2.127575...e-01 7.94565827e-02\n",
      "   7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "   7.25015521e-01 4.72886264e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518  -0.13268296\n",
      "   0.24020547  0.7182372   0.8...12175  -0.00601085 -1.9357834   0.32361984\n",
      "  -1.6383653   0.6919574   0.1617431  -2.586448   -1.8231784  -2.2502906 ]])\n",
      "        h0         = (needle.Tensor([[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518  -0.13268296\n",
      "   0.24020547  0.7182372   0....175  -0.00601085 -1.9357834   0.32361984\n",
      "  -1.6383653   0.6919574   0.1617431  -2.586448   -1.8231784  -2.2502906 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513...085e-01 7.94565827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]])\n",
      "        inputs     = [needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.1275751...85e-01 7.94565827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56325f90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56325f50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.1275751...2175  -0.00601085 -1.9357834   0.32361984\n",
      "  -1.6383653   0.6919574   0.1617431  -2.586448   -1.8231784  -2.2502906 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56325f90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513...085e-01 7.94565827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.04968056  0.0281643   0.04941994  0.16585413 -0.13845308 -0.04750001\n",
      "  -0.2586574   0.27837855  0.0...941994  0.16585413 -0.13845308 -0.04750001\n",
      "  -0.2586574   0.27837855  0.0717614   0.169566   -0.1942195  -0.05034159]])\n",
      "        bias_ih    = needle.Tensor([[ 0.1463052   0.2869292   0.05053991  0.16654539 -0.10425542  0.11400941\n",
      "   0.2504028  -0.24040484  0.0...053991  0.16654539 -0.10425542  0.11400941\n",
      "   0.2504028  -0.24040484  0.01995146  0.00293848 -0.1320307   0.12891793]])\n",
      "        h          = needle.Tensor([[ 0.7311498  -0.43625966  1.0596892   0.20067191 -0.5388518  -0.13268296\n",
      "   0.24020547  0.7182372   0.8...12175  -0.00601085 -1.9357834   0.32361984\n",
      "  -1.6383653   0.6919574   0.1617431  -2.586448   -1.8231784  -2.2502906 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56325f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.09172378 -0.111296    0.00529712 -0.0992354   0.2168538  -0.23531716\n",
      "  -0.13422062 -0.06668976 -0.1...6514999 -0.01063058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]])\n",
      "        self       = needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513...085e-01 7.94565827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.1275751...514999 -0.01063058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563240d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.1275751...514999 -0.01063058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563240d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5881b8b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd56325ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd57662e70>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd56325ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513e-01 3...5827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.09172378 -0.111296    0.00529712 -0.0992354   0.2168538  -0.23531716\n",
      "  -0.13422062 -0.06668976 -0.1904526...3058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd563240d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513e-01 3...5827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.09172378 -0.111296    0.00529712 -0.0992354   0.2168538  -0.23531716\n",
      "  -0.13422062 -0.06668976 -0.1904526...3058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513e-01 3...5827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]], device=cpu())\n",
      "other = NDArray([[-0.09172378 -0.111296    0.00529712 -0.0992354   0.2168538  -0.23531716\n",
      "  -0.13422062 -0.06668976 -0.1904526...3058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd563243f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd563261f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd563244f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.09172378 -0.111296    0.00529712 -0.0992354   0.2168538  -0.23531716\n",
      "  -0.13422062 -0.06668976 -0.1904526...3058 -0.04239765 -0.2198852\n",
      "  -0.00104308 -0.16535023 -0.15763651 -0.18292876  0.17654973  0.01605448]], device=cuda())\n",
      "out        = NDArray([[1.35558992e-19 4.05572571e-08 1.69708798e-07 5.25953423e+22\n",
      "  1.50084232e-19 4.15371505e-05 1.05035078e-05 5...285166e-11 1.35691328e-19 4.28828493e-08\n",
      "  6.74993657e-07 1.30289607e-11 9.10586884e-12 1.03714708e-08]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[4.77479577e-01 6.90685630e-01 7.50817537e-01 7.09388733e-01\n",
      "  2.58968860e-01 1.43062845e-01 2.12757513e-01 3...5827e-02\n",
      "  7.46656775e-01 1.80087969e-01 8.89635146e-01 1.98925346e-01\n",
      "  7.25015521e-01 4.72886264e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.17497894  0.1568564  -0.89975417 -2.193392    2.4103076\n",
      "    1.797924   -0.78501326  0.6318146   1....563  1.409664   -0.760972\n",
      "   -0.31780276 -0.06377296  0.4756646   0.44151813  0.93579924\n",
      "    0.92021435 -0.50224006]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127\n",
      "    1.8634561   1.0793378  -0.91358083  0...285  1.120812   -0.2236798\n",
      "   -0.514127   -1.1988976  -0.27870217 -0.14940251  1.3765613\n",
      "   -1.8526697  -1.4330207 ]]])\n",
      "h0         = needle.Tensor([[[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127\n",
      "    1.8634561   1.0793378  -0.91358083  0...285  1.120812   -0.2236798\n",
      "   -0.514127   -1.1988976  -0.27870217 -0.14940251  1.3765613\n",
      "   -1.8526697  -1.4330207 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd572b3f10>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[814., 946., 423., 592., 507., 514., 972., 670., 324.,  50.,  87.,\n",
      "        890., 995., 615.,  34.],\n",
      "       [ 99...    [836., 499., 721., 880., 534.,  80., 553., 873., 770., 807., 292.,\n",
      "        362., 885., 495., 682.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[814. 946. 423. 592. 507. 514. 972. 670. 324.  50.  87. 890. 995. 615.\n",
      "   34.]\n",
      " [ 99. 669. 915. 399.  ...85  1.120812   -0.2236798\n",
      "   -0.514127   -1.1988976  -0.27870217 -0.14940251  1.3765613\n",
      "   -1.8526697  -1.4330207 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd572b3f10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127\n",
      "    1.8634561   1.0793378  -0.91358083  0...285  1.120812   -0.2236798\n",
      "   -0.514127   -1.1988976  -0.27870217 -0.14940251  1.3765613\n",
      "   -1.8526697  -1.4330207 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd572b3f10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[814. 946. 423. 592. 507. 514. 972. 670. 324.  50.  87. 890. 995. 615.\n",
      "   34.]\n",
      " [ 99. 669. 915. 399.  5...273. 721. 792.  74. 683.  63.\n",
      "  482.]\n",
      " [836. 499. 721. 880. 534.  80. 553. 873. 770. 807. 292. 362. 885. 495.\n",
      "  682.]])\n",
      "        x_emb      = needle.Tensor([[[0.84250623 0.9948732  0.7285006  ... 0.23726644 0.7344683  0.9909531 ]\n",
      "  [0.96549594 0.05657672 0.036...7537 ... 0.45070094 0.01331908 0.37108997]\n",
      "  [0.76716703 0.7704577  0.00363375 ... 0.2870589  0.2593387  0.15333901]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.84250623 0.9948732  0.7285006  ... 0.23726644 0.7344683  0.9909531 ]\n",
      "  [0.96549594 0.05657672 0.03...85  1.120812   -0.2236798\n",
      "   -0.514127   -1.1988976  -0.27870217 -0.14940251  1.3765613\n",
      "   -1.8526697  -1.4330207 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd572b29d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.84250623 0.9948732  0.7285006  ... 0.23726644 0.7344683  0.9909531 ]\n",
      "  [0.96549594 0.05657672 0.036...7537 ... 0.45070094 0.01331908 0.37108997]\n",
      "  [0.76716703 0.7704577  0.00363375 ... 0.2870589  0.2593387  0.15333901]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127  1.8634561\n",
      "   1.0793378  -0.91358083  0.00...04701285  1.120812   -0.2236798  -0.514127\n",
      "  -1.1988976  -0.27870217 -0.14940251  1.3765613  -1.8526697  -1.4330207 ]])\n",
      "        h0         = (needle.Tensor([[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127  1.8634561\n",
      "   1.0793378  -0.91358083  0.0...701285  1.120812   -0.2236798  -0.514127\n",
      "  -1.1988976  -0.27870217 -0.14940251  1.3765613  -1.8526697  -1.4330207 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0....86\n",
      "  0.9541476  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]])\n",
      "        inputs     = [needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0...0.7753963  0.58475846 0.98205    0.20940349 0.6118452  0.9866115\n",
      "  0.957685   0.42150834 0.736829   0.5852742 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b23d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd572b29d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0...4701285  1.120812   -0.2236798  -0.514127\n",
      "  -1.1988976  -0.27870217 -0.14940251  1.3765613  -1.8526697  -1.4330207 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b23d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0....86\n",
      "  0.9541476  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.22661698  0.052959   -0.16317093 -0.05290022  0.12786397  0.2816409\n",
      "   0.03304541 -0.09468237  0.24...6317093 -0.05290022  0.12786397  0.2816409\n",
      "   0.03304541 -0.09468237  0.24032569 -0.19704264  0.28015512  0.11373839]])\n",
      "        bias_ih    = needle.Tensor([[-0.1908716   0.21322262 -0.16291656  0.22462207  0.21261019 -0.1826638\n",
      "   0.14328921 -0.1755975   0.25...6291656  0.22462207  0.21261019 -0.1826638\n",
      "   0.14328921 -0.1755975   0.2564264   0.02976787  0.11638889  0.23209548]])\n",
      "        h          = needle.Tensor([[ 0.6351167  -1.2133062  -0.4377142  -1.1587954  -0.20836127  1.8634561\n",
      "   1.0793378  -0.91358083  0.00...04701285  1.120812   -0.2236798  -0.514127\n",
      "  -1.1988976  -0.27870217 -0.14940251  1.3765613  -1.8526697  -1.4330207 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd572b23d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.65569955e-01 -1.77795112e-01  9.07767713e-02  1.12795740e-01\n",
      "  -2.84435511e-01 -5.13844639e-02  4.0...01 -1.73822135e-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]])\n",
      "        self       = needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0....86\n",
      "  0.9541476  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0...1 -1.73822135e-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806e010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0...1 -1.73822135e-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806e010>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5876cb70>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd588bd550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd5876f2b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd588bd550>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0.643667...  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]], device=cpu())\n",
      "        b          = NDArray([[-2.65569955e-01 -1.77795112e-01  9.07767713e-02  1.12795740e-01\n",
      "  -2.84435511e-01 -5.13844639e-02  4.0220618...-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806e010>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0.643667...  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]], device=cpu())\n",
      "        b          = NDArray([[-2.65569955e-01 -1.77795112e-01  9.07767713e-02  1.12795740e-01\n",
      "  -2.84435511e-01 -5.13844639e-02  4.0220618...-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0.643667...  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]], device=cpu())\n",
      "other = NDArray([[-2.65569955e-01 -1.77795112e-01  9.07767713e-02  1.12795740e-01\n",
      "  -2.84435511e-01 -5.13844639e-02  4.0220618...-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b9bf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b9d30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd588bcfb0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-2.65569955e-01 -1.77795112e-01  9.07767713e-02  1.12795740e-01\n",
      "  -2.84435511e-01 -5.13844639e-02  4.0220618...-01  1.39272720e-01 -1.00190505e-01\n",
      "   2.64631748e-01 -1.32582083e-01  1.33659452e-01  1.12622947e-01]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 2.9900522e+13 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.84250623 0.9948732  0.7285006  0.10197636 0.19731043 0.4758166\n",
      "  0.29021308 0.8266001  0.13167626 0.643667...  0.37262642 0.12879094 0.4323266  0.7150868  0.71052474\n",
      "  0.374094   0.80232805 0.18699023 0.2988481 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.3109151  -0.4968932  -0.07375076  0.6029886  -0.38433018\n",
      "    0.7896545  -0.20221135  0.85070366  0...63  -0.01012595  2.5431888\n",
      "    0.19520706  0.4694006   0.60122126 -1.0681741   0.5349479\n",
      "   -0.9338181   0.53500825]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896\n",
      "    0.47881365  0.29801756  1.3978047   1....827 -0.15507327  2.9691718\n",
      "   -1.3898587  -0.77745074  1.0062776   0.3783247   0.7351724\n",
      "   -0.38791922 -2.1175332 ]]])\n",
      "h0         = needle.Tensor([[[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896\n",
      "    0.47881365  0.29801756  1.3978047   1....827 -0.15507327  2.9691718\n",
      "   -1.3898587  -0.77745074  1.0062776   0.3783247   0.7351724\n",
      "   -0.38791922 -2.1175332 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd585ff090>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[ 39.,  35., 648., 471., 542., 395., 532., 653., 387., 159., 612.,\n",
      "        263., 522., 407., 657.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 39.  35. 648. 471. 542. 395. 532. 653. 387. 159. 612. 263. 522. 407.\n",
      "  657.]]), needle.Tensor([[[-0....27 -0.15507327  2.9691718\n",
      "   -1.3898587  -0.77745074  1.0062776   0.3783247   0.7351724\n",
      "   -0.38791922 -2.1175332 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd585ff090>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896\n",
      "    0.47881365  0.29801756  1.3978047   1....827 -0.15507327  2.9691718\n",
      "   -1.3898587  -0.77745074  1.0062776   0.3783247   0.7351724\n",
      "   -0.38791922 -2.1175332 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd585ff090>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[ 39.  35. 648. 471. 542. 395. 532. 653. 387. 159. 612. 263. 522. 407.\n",
      "  657.]])\n",
      "        x_emb      = needle.Tensor([[[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "   0.03204945 0.15144438 0.98096144 ...\n",
      "   0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "   0.73885345 0.16827026 0.6985919  0.49669456]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "   0.03204945 0.15144438 0.98096144...27 -0.15507327  2.9691718\n",
      "   -1.3898587  -0.77745074  1.0062776   0.3783247   0.7351724\n",
      "   -0.38791922 -2.1175332 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fdf10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "   0.03204945 0.15144438 0.98096144 ...\n",
      "   0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "   0.73885345 0.16827026 0.6985919  0.49669456]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896   0.47881365\n",
      "   0.29801756  1.3978047   1.0...725958 -0.28484577  0.10221865  0.28202513\n",
      "   0.5437899  -2.5826545  -0.93425465 -0.42269588  0.16054074  1.0293114 ]])\n",
      "        h0         = (needle.Tensor([[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896   0.47881365\n",
      "   0.29801756  1.3978047   1....468827 -0.15507327  2.9691718  -1.3898587\n",
      "  -0.77745074  1.0062776   0.3783247   0.7351724  -0.38791922 -2.1175332 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0....115\n",
      "  0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]])\n",
      "        inputs     = [needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0...15\n",
      "  0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fdf90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd585fdf10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0...25958 -0.28484577  0.10221865  0.28202513\n",
      "   0.5437899  -2.5826545  -0.93425465 -0.42269588  0.16054074  1.0293114 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fdf90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0....115\n",
      "  0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.10929881  0.01217777  0.23997009  0.25462103  0.12805104  0.12099829\n",
      "  -0.02685204 -0.14149036  0.0...997009  0.25462103  0.12805104  0.12099829\n",
      "  -0.02685204 -0.14149036  0.03792974 -0.03595254  0.0763891   0.01448476]])\n",
      "        bias_ih    = needle.Tensor([[-0.12784792  0.26842678  0.21657926  0.03534442  0.11961699 -0.28328693\n",
      "  -0.09845518  0.06545329 -0.0...657926  0.03534442  0.11961699 -0.28328693\n",
      "  -0.09845518  0.06545329 -0.01521012  0.15100834  0.00361535 -0.08588175]])\n",
      "        h          = needle.Tensor([[-0.9767225  -0.61787105 -1.7786853  -0.6814718  -1.8388896   0.47881365\n",
      "   0.29801756  1.3978047   1.0...725958 -0.28484577  0.10221865  0.28202513\n",
      "   0.5437899  -2.5826545  -0.93425465 -0.42269588  0.16054074  1.0293114 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd585fdf90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.03157005  0.10869765 -0.01112881  0.10680249  0.23781824 -0.05786078\n",
      "   0.26554906 -0.2618171  -0.0...599658 -0.27503648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]])\n",
      "        self       = needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0....115\n",
      "  0.11591587 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0...99658 -0.27503648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584366d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0...99658 -0.27503648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584366d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd588bc170>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58434ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd588bccf0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58434ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0.491426...87 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]], device=cpu())\n",
      "        b          = NDArray([[ 0.03157005  0.10869765 -0.01112881  0.10680249  0.23781824 -0.05786078\n",
      "   0.26554906 -0.2618171  -0.0641879...648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584366d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0.491426...87 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]], device=cpu())\n",
      "        b          = NDArray([[ 0.03157005  0.10869765 -0.01112881  0.10680249  0.23781824 -0.05786078\n",
      "   0.26554906 -0.2618171  -0.0641879...648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0.491426...87 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]], device=cpu())\n",
      "other = NDArray([[ 0.03157005  0.10869765 -0.01112881  0.10680249  0.23781824 -0.05786078\n",
      "   0.26554906 -0.2618171  -0.0641879...648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58010e70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd585fc9f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584355f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.03157005  0.10869765 -0.01112881  0.10680249  0.23781824 -0.05786078\n",
      "   0.26554906 -0.2618171  -0.0641879...648  0.27615708 -0.13739628\n",
      "  -0.05960593 -0.25614184  0.08985195  0.0102964   0.11164638  0.10140204]], device=cuda())\n",
      "out        = NDArray([[1.35558992e-19 4.15315626e-05 6.67586619e-07 5.22979111e+22\n",
      "  1.50084232e-19 2.53505594e-09 1.67842515e-07 5...282911e-11 9.10631293e-12 4.24171915e-08\n",
      "  4.29611573e-05 1.30278488e-11 1.35691328e-19 2.74450258e-06]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.14520973 0.32540104 0.4991796  0.87350196 0.37223452 0.8927066\n",
      "  0.03204945 0.15144438 0.98096144 0.491426...87 0.76780653 0.08537307 0.09085617 0.15393125 0.9763252\n",
      "  0.73885345 0.16827026 0.6985919  0.49669456]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 4.05498564e-01  2.01184034e+00 -1.06086254e+00 -7.12554336e-01\n",
      "   -7.36564100e-01 -5.74606299e-01  1... -9.57368195e-01  2.01320958e+00 -6.77953660e-01\n",
      "   -9.55535173e-01 -6.41628683e-01 -2.71424651e-01  9.11887467e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = needle.Tensor([[[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849\n",
      "   -1.9632535   1.5692762  -0.02413426  1....796 -2.3443136  -1.0200772\n",
      "   -0.8360585  -0.858583    0.31500033  0.9848204   0.3909832\n",
      "    0.90799445  0.9882219 ]]])\n",
      "h0         = needle.Tensor([[[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849\n",
      "   -1.9632535   1.5692762  -0.02413426  1....796 -2.3443136  -1.0200772\n",
      "   -0.8360585  -0.858583    0.31500033  0.9848204   0.3909832\n",
      "    0.90799445  0.9882219 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd584b65d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[534., 797.,  62., 725., 295., 952., 924., 608., 335., 524., 835.,\n",
      "        896., 544., 111., 516.],\n",
      "       [855...    [349., 747., 315., 831., 599., 213., 522., 674.,  36., 903., 486.,\n",
      "        888., 322., 682., 399.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[534. 797.  62. 725. 295. 952. 924. 608. 335. 524. 835. 896. 544. 111.\n",
      "  516.]\n",
      " [855. 456. 739. 481. 4...96 -2.3443136  -1.0200772\n",
      "   -0.8360585  -0.858583    0.31500033  0.9848204   0.3909832\n",
      "    0.90799445  0.9882219 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b65d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = needle.Tensor([[[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849\n",
      "   -1.9632535   1.5692762  -0.02413426  1....796 -2.3443136  -1.0200772\n",
      "   -0.8360585  -0.858583    0.31500033  0.9848204   0.3909832\n",
      "    0.90799445  0.9882219 ]]])\n",
      "        self       = <models.LanguageModel object at 0x7fbd584b65d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[534. 797.  62. 725. 295. 952. 924. 608. 335. 524. 835. 896. 544. 111.\n",
      "  516.]\n",
      " [855. 456. 739. 481. 42...343. 361. 437. 206. 891.  11.\n",
      "  271.]\n",
      " [349. 747. 315. 831. 599. 213. 522. 674.  36. 903. 486. 888. 322. 682.\n",
      "  399.]])\n",
      "        x_emb      = needle.Tensor([[[0.29064414 0.72297347 0.5089258  ... 0.77069277 0.01066855 0.01390043]\n",
      "  [0.07823751 0.6805606  0.702...3928 ... 0.5655599  0.26027158 0.5564153 ]\n",
      "  [0.20958175 0.29983646 0.5657022  ... 0.33898422 0.27250051 0.23026699]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.29064414 0.72297347 0.5089258  ... 0.77069277 0.01066855 0.01390043]\n",
      "  [0.07823751 0.6805606  0.70...96 -2.3443136  -1.0200772\n",
      "   -0.8360585  -0.858583    0.31500033  0.9848204   0.3909832\n",
      "    0.90799445  0.9882219 ]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b7350>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.29064414 0.72297347 0.5089258  ... 0.77069277 0.01066855 0.01390043]\n",
      "  [0.07823751 0.6805606  0.702...3928 ... 0.5655599  0.26027158 0.5564153 ]\n",
      "  [0.20958175 0.29983646 0.5657022  ... 0.33898422 0.27250051 0.23026699]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849  -1.9632535\n",
      "   1.5692762  -0.02413426  1.17...457697  -1.5461498  -1.0094585   0.2641804\n",
      "   1.3808496   0.6882752  -0.549989    0.87223077  1.3356982  -1.6566061 ]])\n",
      "        h0         = (needle.Tensor([[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849  -1.9632535\n",
      "   1.5692762  -0.02413426  1.1...601796 -2.3443136  -1.0200772  -0.8360585\n",
      "  -0.858583    0.31500033  0.9848204   0.3909832   0.90799445  0.9882219 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0....23\n",
      "  0.49889648 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]])\n",
      "        inputs     = [needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0...1 8.47778842e-02\n",
      "  6.39910460e-01 7.41582513e-01 4.67612326e-01 6.11942589e-01\n",
      "  9.59341347e-01 6.56336248e-01]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b6450>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584b7350>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0...57697  -1.5461498  -1.0094585   0.2641804\n",
      "   1.3808496   0.6882752  -0.549989    0.87223077  1.3356982  -1.6566061 ]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b6450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0....23\n",
      "  0.49889648 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.22074315 -0.00340295  0.12449351  0.14822713 -0.22598088  0.0493724\n",
      "   0.07753539 -0.24906942  0.26...2449351  0.14822713 -0.22598088  0.0493724\n",
      "   0.07753539 -0.24906942  0.2666927  -0.06582811 -0.26947466  0.0090411 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.08852181 -0.12645681  0.22551382 -0.12468232 -0.2640893  -0.02441657\n",
      "  -0.1426743  -0.10822734  0.0...551382 -0.12468232 -0.2640893  -0.02441657\n",
      "  -0.1426743  -0.10822734  0.01014715 -0.03771657  0.05851641 -0.25551417]])\n",
      "        h          = needle.Tensor([[ 0.3480502  -0.19961198  2.6120615  -0.5183645  -0.6962849  -1.9632535\n",
      "   1.5692762  -0.02413426  1.17...457697  -1.5461498  -1.0094585   0.2641804\n",
      "   1.3808496   0.6882752  -0.549989    0.87223077  1.3356982  -1.6566061 ]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584b6450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.2656104   0.11128387 -0.00870073 -0.24917229 -0.06081274  0.2144835\n",
      "  -0.10978302  0.06455138  0.06...244005 -0.1864458   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]])\n",
      "        self       = needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0....23\n",
      "  0.49889648 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0...44005 -0.1864458   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5874b3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0...44005 -0.1864458   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5874b3d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd55bcf630>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58749390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd55bcd470>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58749390>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0.955090...8 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.2656104   0.11128387 -0.00870073 -0.24917229 -0.06081274  0.2144835\n",
      "  -0.10978302  0.06455138  0.06917778...58   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5874b3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0.955090...8 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.2656104   0.11128387 -0.00870073 -0.24917229 -0.06081274  0.2144835\n",
      "  -0.10978302  0.06455138  0.06917778...58   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0.955090...8 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]], device=cpu())\n",
      "other = NDArray([[ 0.2656104   0.11128387 -0.00870073 -0.24917229 -0.06081274  0.2144835\n",
      "  -0.10978302  0.06455138  0.06917778...58   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58748670>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584b5eb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58748f30>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.2656104   0.11128387 -0.00870073 -0.24917229 -0.06081274  0.2144835\n",
      "  -0.10978302  0.06455138  0.06917778...58   0.11033472  0.12217325\n",
      "   0.22558403 -0.13814923  0.05236086  0.2474497   0.04234186 -0.18952759]], device=cuda())\n",
      "out        = NDArray([[8.83280231e-35 0.00000000e+00 2.32063891e-34 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0...000000e+00 1.20371762e-38 0.00000000e+00\n",
      "  1.24666238e-38 0.00000000e+00 1.20371762e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.29064414 0.72297347 0.5089258  0.2841805  0.8421329  0.8773392\n",
      "  0.43987694 0.40207413 0.11437823 0.955090...8 0.05980913 0.8244872  0.3848692  0.11277599 0.84307104\n",
      "  0.23741576 0.7911765  0.29598325 0.9685188 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.29156]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.03852816]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd561c2250>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[110.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[110.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd561c2250>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd561c2250>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[110.]])\n",
      "        x_emb      = needle.Tensor([[[0.68371975]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.68371975]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd561c0290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.68371975]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.68371975]])\n",
      "        inputs     = [needle.Tensor([[0.68371975]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c3190>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd561c0290>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.68371975]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c3190>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.68371975]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.15380013]])\n",
      "        bias_ih    = needle.Tensor([[-0.43907142]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd561c3190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.0712533]])\n",
      "        self       = needle.Tensor([[0.68371975]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.68371975]]), needle.Tensor([[-0.0712533]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aa210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.68371975]]), needle.Tensor([[-0.0712533]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aa210>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd58092030>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd587a8e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd58092af0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd587a8e10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.68371975]], device=cpu())\n",
      "        b          = NDArray([[-0.0712533]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587aa210>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.68371975]], device=cpu())\n",
      "        b          = NDArray([[-0.0712533]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.68371975]], device=cpu())\n",
      "other = NDArray([[-0.0712533]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587a8630>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd561c1b30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587a92f0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.0712533]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.68371975]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.1452658]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[0.19905986]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd578dedd0>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[528.],\n",
      "       [964.],\n",
      "       [802.],\n",
      "       [225.],\n",
      "       [984.],\n",
      "       [123.],\n",
      "       [834.],\n",
      "       [568.],\n",
      "       [725.],\n",
      "       [ 26.],\n",
      "       [137.],\n",
      "       [692.],\n",
      "       [664.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[528.]\n",
      " [964.]\n",
      " [802.]\n",
      " [225.]\n",
      " [984.]\n",
      " [123.]\n",
      " [834.]\n",
      " [568.]\n",
      " [725.]\n",
      " [ 26.]\n",
      " [137.]\n",
      " [692.]\n",
      " [664.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dedd0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dedd0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[528.]\n",
      " [964.]\n",
      " [802.]\n",
      " [225.]\n",
      " [984.]\n",
      " [123.]\n",
      " [834.]\n",
      " [568.]\n",
      " [725.]\n",
      " [ 26.]\n",
      " [137.]\n",
      " [692.]\n",
      " [664.]])\n",
      "        x_emb      = needle.Tensor([[[0.04757048]]\n",
      "\n",
      " [[0.9533149 ]]\n",
      "\n",
      " [[0.30002967]]\n",
      "\n",
      " [[0.4754348 ]]\n",
      "\n",
      " [[0.92852616]]\n",
      "\n",
      " [[0.71488535]]\n",
      "\n",
      " [[0.07093324]]\n",
      "\n",
      " [[0.6426981 ]]\n",
      "\n",
      " [[0.63552934]]\n",
      "\n",
      " [[0.3267385 ]]\n",
      "\n",
      " [[0.1565886 ]]\n",
      "\n",
      " [[0.2464016 ]]\n",
      "\n",
      " [[0.7426726 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.04757048]]\n",
      "\n",
      " [[0.9533149 ]]\n",
      "\n",
      " [[0.30002967]]\n",
      "\n",
      " [[0.4754348 ]]\n",
      "\n",
      " [[0.92852616]]\n",
      "\n",
      " [[0.71488535]]\n",
      "\n",
      " ...093324]]\n",
      "\n",
      " [[0.6426981 ]]\n",
      "\n",
      " [[0.63552934]]\n",
      "\n",
      " [[0.3267385 ]]\n",
      "\n",
      " [[0.1565886 ]]\n",
      "\n",
      " [[0.2464016 ]]\n",
      "\n",
      " [[0.7426726 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578df950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.04757048]]\n",
      "\n",
      " [[0.9533149 ]]\n",
      "\n",
      " [[0.30002967]]\n",
      "\n",
      " [[0.4754348 ]]\n",
      "\n",
      " [[0.92852616]]\n",
      "\n",
      " [[0.71488535]]\n",
      "\n",
      " [[0.07093324]]\n",
      "\n",
      " [[0.6426981 ]]\n",
      "\n",
      " [[0.63552934]]\n",
      "\n",
      " [[0.3267385 ]]\n",
      "\n",
      " [[0.1565886 ]]\n",
      "\n",
      " [[0.2464016 ]]\n",
      "\n",
      " [[0.7426726 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.04757048]])\n",
      "        inputs     = [needle.Tensor([[0.04757048]]), needle.Tensor([[0.9533149]]), needle.Tensor([[0.30002967]]), needle.Tensor([[0.4754348]]), needle.Tensor([[0.92852616]]), needle.Tensor([[0.71488535]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dca90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578df950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.04757048]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dca90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.04757048]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.7432047]])\n",
      "        bias_ih    = needle.Tensor([[0.46668756]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578dca90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.39250076]])\n",
      "        self       = needle.Tensor([[0.04757048]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.04757048]]), needle.Tensor([[0.39250076]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c1b10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.04757048]]), needle.Tensor([[0.39250076]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c1b10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5787cb70>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd584c1710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5787c7b0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd584c1710>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.04757048]], device=cpu())\n",
      "        b          = NDArray([[0.39250076]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c1b10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.04757048]], device=cpu())\n",
      "        b          = NDArray([[0.39250076]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.04757048]], device=cpu())\n",
      "other = NDArray([[0.39250076]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587d7db0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd578de0f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c1ab0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.39250076]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.04757048]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[0.44762284]]\n",
      "\n",
      " [[0.528235  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.5390664 ]]\n",
      "\n",
      " [[-0.06704277]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd584897d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[179.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[179.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584897d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd584897d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[179.]])\n",
      "        x_emb      = needle.Tensor([[[0.5853621]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5853621]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5848afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5853621]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5853621]])\n",
      "        inputs     = [needle.Tensor([[0.5853621]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58489c10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5848afd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5853621]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58489c10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5853621]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.08455473]])\n",
      "        bias_ih    = needle.Tensor([[-0.08227301]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58489c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.47291243]])\n",
      "        self       = needle.Tensor([[0.5853621]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5853621]]), needle.Tensor([[0.47291243]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588a69d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5853621]]), needle.Tensor([[0.47291243]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588a69d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57258470>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd588a5710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5725aaf0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd588a5710>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5853621]], device=cpu())\n",
      "        b          = NDArray([[0.47291243]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588a69d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5853621]], device=cpu())\n",
      "        b          = NDArray([[0.47291243]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5853621]], device=cpu())\n",
      "other = NDArray([[0.47291243]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd588a6bb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58488970>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd588a66f0>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[0.47291243]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.5853621]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.7201719 ]]\n",
      "\n",
      " [[-0.28930593]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[1.5143028]]\n",
      "\n",
      " [[1.0191448]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58423ad0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[724.],\n",
      "       [507.],\n",
      "       [720.],\n",
      "       [158.],\n",
      "       [230.],\n",
      "       [298.],\n",
      "       [919.],\n",
      "       [286.],\n",
      "       [953.],\n",
      "       [981.],\n",
      "       [794.],\n",
      "       [220.],\n",
      "       [228.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[724.]\n",
      " [507.]\n",
      " [720.]\n",
      " [158.]\n",
      " [230.]\n",
      " [298.]\n",
      " [919.]\n",
      " [286.]\n",
      " [953.]\n",
      " [981.]\n",
      " [794.]\n",
      " [220.]\n",
      " [228.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58423ad0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58423ad0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[724.]\n",
      " [507.]\n",
      " [720.]\n",
      " [158.]\n",
      " [230.]\n",
      " [298.]\n",
      " [919.]\n",
      " [286.]\n",
      " [953.]\n",
      " [981.]\n",
      " [794.]\n",
      " [220.]\n",
      " [228.]])\n",
      "        x_emb      = needle.Tensor([[[0.40487966]]\n",
      "\n",
      " [[0.2063191 ]]\n",
      "\n",
      " [[0.38755053]]\n",
      "\n",
      " [[0.5082088 ]]\n",
      "\n",
      " [[0.66477185]]\n",
      "\n",
      " [[0.8061841 ]]\n",
      "\n",
      " [[0.66765237]]\n",
      "\n",
      " [[0.1607545 ]]\n",
      "\n",
      " [[0.10184377]]\n",
      "\n",
      " [[0.22562988]]\n",
      "\n",
      " [[0.06932586]]\n",
      "\n",
      " [[0.8786003 ]]\n",
      "\n",
      " [[0.02411408]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.40487966]]\n",
      "\n",
      " [[0.2063191 ]]\n",
      "\n",
      " [[0.38755053]]\n",
      "\n",
      " [[0.5082088 ]]\n",
      "\n",
      " [[0.66477185]]\n",
      "\n",
      " [[0.8061841 ]]\n",
      "\n",
      " ...765237]]\n",
      "\n",
      " [[0.1607545 ]]\n",
      "\n",
      " [[0.10184377]]\n",
      "\n",
      " [[0.22562988]]\n",
      "\n",
      " [[0.06932586]]\n",
      "\n",
      " [[0.8786003 ]]\n",
      "\n",
      " [[0.02411408]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584223d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.40487966]]\n",
      "\n",
      " [[0.2063191 ]]\n",
      "\n",
      " [[0.38755053]]\n",
      "\n",
      " [[0.5082088 ]]\n",
      "\n",
      " [[0.66477185]]\n",
      "\n",
      " [[0.8061841 ]]\n",
      "\n",
      " [[0.66765237]]\n",
      "\n",
      " [[0.1607545 ]]\n",
      "\n",
      " [[0.10184377]]\n",
      "\n",
      " [[0.22562988]]\n",
      "\n",
      " [[0.06932586]]\n",
      "\n",
      " [[0.8786003 ]]\n",
      "\n",
      " [[0.02411408]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.40487966]])\n",
      "        inputs     = [needle.Tensor([[0.40487966]]), needle.Tensor([[0.2063191]]), needle.Tensor([[0.38755053]]), needle.Tensor([[0.5082088]]), needle.Tensor([[0.66477185]]), needle.Tensor([[0.8061841]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58422850>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584223d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40487966]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58422850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.40487966]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.7794421]])\n",
      "        bias_ih    = needle.Tensor([[-0.86945736]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58422850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.7428474]])\n",
      "        self       = needle.Tensor([[0.40487966]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.40487966]]), needle.Tensor([[-0.7428474]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55dc79d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.40487966]]), needle.Tensor([[-0.7428474]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55dc79d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57662270>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd55dc4190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57662eb0>, 1, 1, 1') raised in repr()] Tensor object at 0x7fbd55dc4190>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40487966]], device=cpu())\n",
      "        b          = NDArray([[-0.7428474]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55dc79d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.40487966]], device=cpu())\n",
      "        b          = NDArray([[-0.7428474]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.40487966]], device=cpu())\n",
      "other = NDArray([[-0.7428474]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58422c70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58421470>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55dc7030>, 1, 1, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.7428474]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.40487966]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.08332216]\n",
      "  [ 0.49059653]\n",
      "  [-1.9983215 ]\n",
      "  [-1.7583784 ]\n",
      "  [-1.7333328 ]\n",
      "  [-0.23928486]\n",
      "  [-0.14...59 ]\n",
      "  [-0.4778499 ]\n",
      "  [ 0.90958613]\n",
      "  [ 0.5874044 ]\n",
      "  [ 0.8798391 ]\n",
      "  [-1.9149288 ]\n",
      "  [-1.872605  ]\n",
      "  [ 0.90204984]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.69302064]\n",
      "  [-1.24148   ]\n",
      "  [ 1.2315494 ]\n",
      "  [ 1.0599815 ]\n",
      "  [-0.06630231]\n",
      "  [ 0.5452865 ]\n",
      "  [ 1.39...2  ]\n",
      "  [ 0.5510478 ]\n",
      "  [ 0.23321447]\n",
      "  [ 1.4936262 ]\n",
      "  [-0.07250819]\n",
      "  [-0.20581056]\n",
      "  [-0.37804398]\n",
      "  [-0.31190124]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5705d150>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[236., 736., 715., 618., 861.,  82., 786., 485., 473., 392., 188.,\n",
      "        792., 140., 263., 965.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[236. 736. 715. 618. 861.  82. 786. 485. 473. 392. 188. 792. 140. 263.\n",
      "  965.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705d150>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705d150>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[236. 736. 715. 618. 861.  82. 786. 485. 473. 392. 188. 792. 140. 263.\n",
      "  965.]])\n",
      "        x_emb      = needle.Tensor([[[0.35795084]\n",
      "  [0.02467687]\n",
      "  [0.29208365]\n",
      "  [0.453795  ]\n",
      "  [0.7634319 ]\n",
      "  [0.5443821 ]\n",
      "  [0.13864265]...0.3960334 ]\n",
      "  [0.68584806]\n",
      "  [0.58712167]\n",
      "  [0.85839033]\n",
      "  [0.69001967]\n",
      "  [0.6187067 ]\n",
      "  [0.7165415 ]\n",
      "  [0.55588454]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.35795084]\n",
      "  [0.02467687]\n",
      "  [0.29208365]\n",
      "  [0.453795  ]\n",
      "  [0.7634319 ]\n",
      "  [0.5443821 ]\n",
      "  [0.13864265...34 ]\n",
      "  [0.68584806]\n",
      "  [0.58712167]\n",
      "  [0.85839033]\n",
      "  [0.69001967]\n",
      "  [0.6187067 ]\n",
      "  [0.7165415 ]\n",
      "  [0.55588454]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5705d090>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.35795084]\n",
      "  [0.02467687]\n",
      "  [0.29208365]\n",
      "  [0.453795  ]\n",
      "  [0.7634319 ]\n",
      "  [0.5443821 ]\n",
      "  [0.13864265]...0.3960334 ]\n",
      "  [0.68584806]\n",
      "  [0.58712167]\n",
      "  [0.85839033]\n",
      "  [0.69001967]\n",
      "  [0.6187067 ]\n",
      "  [0.7165415 ]\n",
      "  [0.55588454]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]])\n",
      "        inputs     = [needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705ee50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5705d090>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3...55588454]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]\n",
      " [0.2858554]])\n",
      "        bias_ih    = needle.Tensor([[0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]\n",
      " [0.3401941]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5705ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.6804981]])\n",
      "        self       = needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3...6]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]]), needle.Tensor([[0.6804981]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ead0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3...6]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]]), needle.Tensor([[0.6804981]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ead0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56244a70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5705f590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd585fc8b0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd5705f590>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]], device=cpu())\n",
      "        b          = NDArray([[0.6804981]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5705ead0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]], device=cpu())\n",
      "        b          = NDArray([[0.6804981]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]], device=cpu())\n",
      "other = NDArray([[0.6804981]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5705ceb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5705f7b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5705c270>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.6804981]], device=cuda())\n",
      "out        = NDArray([[8.6213918e-35]\n",
      " [0.0000000e+00]\n",
      " [3.2205958e-02]\n",
      " [5.4527509e-01]\n",
      " [3.9520460e-01]\n",
      " [9.4184911e-01]\n",
      " [9.5717...]\n",
      " [1.6260338e-01]\n",
      " [6.8047774e-01]\n",
      " [8.8775295e-01]\n",
      " [4.8092630e-02]\n",
      " [9.7830456e-01]\n",
      " [8.0054009e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.35795084]\n",
      " [0.02467687]\n",
      " [0.29208365]\n",
      " [0.453795  ]\n",
      " [0.7634319 ]\n",
      " [0.5443821 ]\n",
      " [0.13864265]\n",
      " [0.3960334 ]\n",
      " [0.68584806]\n",
      " [0.58712167]\n",
      " [0.85839033]\n",
      " [0.69001967]\n",
      " [0.6187067 ]\n",
      " [0.7165415 ]\n",
      " [0.55588454]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.809449  ]\n",
      "  [-0.79000175]\n",
      "  [ 1.1092323 ]\n",
      "  [ 1.8777678 ]\n",
      "  [-0.7517379 ]\n",
      "  [ 0.11257993]\n",
      "  [-0.77...87 ]\n",
      "  [ 0.66106474]\n",
      "  [-1.4899316 ]\n",
      "  [-0.6880985 ]\n",
      "  [ 0.01790064]\n",
      "  [-1.001839  ]\n",
      "  [-0.5370663 ]\n",
      "  [ 0.70007807]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.39542264]\n",
      "  [ 0.92782027]\n",
      "  [-0.44422334]\n",
      "  [-0.95744145]\n",
      "  [-0.9643201 ]\n",
      "  [ 0.14774947]\n",
      "  [ 1.35...12 ]\n",
      "  [-0.1981466 ]\n",
      "  [-0.34519064]\n",
      "  [ 0.31005654]\n",
      "  [-0.46015236]\n",
      "  [ 0.28909528]\n",
      "  [ 0.59087074]\n",
      "  [ 1.2819638 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd570882d0>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[132., 102.,  62., 414., 652., 515., 590., 128., 309., 802., 506.,\n",
      "        654., 467., 879., 581.],\n",
      "       [160...    [  9., 857., 684., 274., 617., 817., 776., 174., 307.,  80., 126.,\n",
      "        945., 415., 212.,  56.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[132. 102.  62. 414. 652. 515. 590. 128. 309. 802. 506. 654. 467. 879.\n",
      "  581.]\n",
      " [160. 525. 385. 391.  ...7. 671. 763. 417. 273.\n",
      "  908.]\n",
      " [  9. 857. 684. 274. 617. 817. 776. 174. 307.  80. 126. 945. 415. 212.\n",
      "   56.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd570882d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd570882d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[132. 102.  62. 414. 652. 515. 590. 128. 309. 802. 506. 654. 467. 879.\n",
      "  581.]\n",
      " [160. 525. 385. 391.  1...934. 827. 671. 763. 417. 273.\n",
      "  908.]\n",
      " [  9. 857. 684. 274. 617. 817. 776. 174. 307.  80. 126. 945. 415. 212.\n",
      "   56.]])\n",
      "        x_emb      = needle.Tensor([[[0.72570235]\n",
      "  [0.5588478 ]\n",
      "  [0.20841452]\n",
      "  [0.80857927]\n",
      "  [0.35048723]\n",
      "  [0.89811516]\n",
      "  [0.01686855]...0.7975182 ]\n",
      "  [0.22816631]\n",
      "  [0.0783544 ]\n",
      "  [0.20076658]\n",
      "  [0.21715975]\n",
      "  [0.4533092 ]\n",
      "  [0.8804421 ]\n",
      "  [0.942025  ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.72570235]\n",
      "  [0.5588478 ]\n",
      "  [0.20841452]\n",
      "  [0.80857927]\n",
      "  [0.35048723]\n",
      "  [0.89811516]\n",
      "  [0.01686855...82 ]\n",
      "  [0.22816631]\n",
      "  [0.0783544 ]\n",
      "  [0.20076658]\n",
      "  [0.21715975]\n",
      "  [0.4533092 ]\n",
      "  [0.8804421 ]\n",
      "  [0.942025  ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57088190>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.72570235]\n",
      "  [0.5588478 ]\n",
      "  [0.20841452]\n",
      "  [0.80857927]\n",
      "  [0.35048723]\n",
      "  [0.89811516]\n",
      "  [0.01686855]...0.7975182 ]\n",
      "  [0.22816631]\n",
      "  [0.0783544 ]\n",
      "  [0.20076658]\n",
      "  [0.21715975]\n",
      "  [0.4533092 ]\n",
      "  [0.8804421 ]\n",
      "  [0.942025  ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]])\n",
      "        inputs     = [needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.9... [0.02382054]\n",
      " [0.71902543]\n",
      " [0.56857896]\n",
      " [0.9034952 ]\n",
      " [0.443667  ]\n",
      " [0.38351533]\n",
      " [0.27885243]\n",
      " [0.3781206 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708b810>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57088190>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.9...6446899 ]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708b810>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]\n",
      " [0.907014]])\n",
      "        bias_ih    = needle.Tensor([[-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]\n",
      " [-0.2999302]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708b810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.12565207]])\n",
      "        self       = needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.9...]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]]), needle.Tensor([[0.12565207]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c17d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.9...]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]]), needle.Tensor([[0.12565207]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c17d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd561c2a30>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd584c2990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57e8b670>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd584c2990>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]], device=cpu())\n",
      "        b          = NDArray([[0.12565207]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584c17d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]], device=cpu())\n",
      "        b          = NDArray([[0.12565207]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]], device=cpu())\n",
      "other = NDArray([[0.12565207]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd570895b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57088c70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c34f0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.12565207]], device=cuda())\n",
      "out        = NDArray([[2.1510577e-33]\n",
      " [0.0000000e+00]\n",
      " [4.3231097e-01]\n",
      " [6.9804084e-01]\n",
      " [1.6879609e-01]\n",
      " [1.0330481e-01]\n",
      " [4.6484...]\n",
      " [4.0790376e-01]\n",
      " [1.5609495e-01]\n",
      " [7.7432364e-01]\n",
      " [6.2039185e-01]\n",
      " [3.3217031e-01]\n",
      " [1.2975393e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.72570235]\n",
      " [0.5588478 ]\n",
      " [0.20841452]\n",
      " [0.80857927]\n",
      " [0.35048723]\n",
      " [0.89811516]\n",
      " [0.01686855]\n",
      " [0.93217987]\n",
      " [0.21452703]\n",
      " [0.9532878 ]\n",
      " [0.51652664]\n",
      " [0.53766155]\n",
      " [0.7105613 ]\n",
      " [0.23601471]\n",
      " [0.6446899 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.0026866 ]\n",
      "  [-1.7149184 ]\n",
      "  [-0.08288532]\n",
      "  [ 0.7421236 ]\n",
      "  [-1.7423023 ]\n",
      "  [ 0.37186062]\n",
      "  [-0.05...395]\n",
      "  [-1.0409089 ]\n",
      "  [-0.63630223]\n",
      "  [ 0.055886  ]\n",
      "  [-1.5767411 ]\n",
      "  [-0.17250434]\n",
      "  [ 0.11022217]\n",
      "  [ 0.44056866]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.31467584]\n",
      "  [-0.7734718 ]\n",
      "  [-0.17898391]\n",
      "  [ 0.32845923]\n",
      "  [-0.3038459 ]\n",
      "  [ 0.82526535]\n",
      "  [-0.92...945]\n",
      "  [-0.10865212]\n",
      "  [ 0.3571571 ]\n",
      "  [ 1.0513206 ]\n",
      "  [ 0.31563264]\n",
      "  [-0.54189616]\n",
      "  [ 0.19046155]\n",
      "  [ 0.47950506]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5876cc10>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[542., 294., 772., 142., 459., 994., 357., 926., 515., 338., 111.,\n",
      "        253., 961., 589., 427.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[542. 294. 772. 142. 459. 994. 357. 926. 515. 338. 111. 253. 961. 589.\n",
      "  427.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5876cc10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5876cc10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[542. 294. 772. 142. 459. 994. 357. 926. 515. 338. 111. 253. 961. 589.\n",
      "  427.]])\n",
      "        x_emb      = needle.Tensor([[[0.12096925]\n",
      "  [0.96354115]\n",
      "  [0.53971636]\n",
      "  [0.60148084]\n",
      "  [0.3923656 ]\n",
      "  [0.44041798]\n",
      "  [0.11338036]...0.14323722]\n",
      "  [0.51358366]\n",
      "  [0.18810636]\n",
      "  [0.5165731 ]\n",
      "  [0.83195543]\n",
      "  [0.68237966]\n",
      "  [0.17490092]\n",
      "  [0.02926935]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.12096925]\n",
      "  [0.96354115]\n",
      "  [0.53971636]\n",
      "  [0.60148084]\n",
      "  [0.3923656 ]\n",
      "  [0.44041798]\n",
      "  [0.11338036...722]\n",
      "  [0.51358366]\n",
      "  [0.18810636]\n",
      "  [0.5165731 ]\n",
      "  [0.83195543]\n",
      "  [0.68237966]\n",
      "  [0.17490092]\n",
      "  [0.02926935]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5876e5d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.12096925]\n",
      "  [0.96354115]\n",
      "  [0.53971636]\n",
      "  [0.60148084]\n",
      "  [0.3923656 ]\n",
      "  [0.44041798]\n",
      "  [0.11338036]...0.14323722]\n",
      "  [0.51358366]\n",
      "  [0.18810636]\n",
      "  [0.5165731 ]\n",
      "  [0.83195543]\n",
      "  [0.68237966]\n",
      "  [0.17490092]\n",
      "  [0.02926935]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]])\n",
      "        inputs     = [needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5876c6d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5876e5d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.1...02926935]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5876c6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]\n",
      " [-0.24314386]])\n",
      "        bias_ih    = needle.Tensor([[-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]\n",
      " [-0.8174492]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5876c6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[0.8138373]])\n",
      "        self       = needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.1...6]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]]), needle.Tensor([[0.8138373]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584db050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.1...6]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]]), needle.Tensor([[0.8138373]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584db050>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd580efb70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd584d8cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd580ee3f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd584d8cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]], device=cpu())\n",
      "        b          = NDArray([[0.8138373]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584db050>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]], device=cpu())\n",
      "        b          = NDArray([[0.8138373]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]], device=cpu())\n",
      "other = NDArray([[0.8138373]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584db1b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5876eab0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584d84f0>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[0.8138373]], device=cuda())\n",
      "out        = NDArray([[4.8514e-41]\n",
      " [0.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]\n",
      " [1.0000e+00]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.12096925]\n",
      " [0.96354115]\n",
      " [0.53971636]\n",
      " [0.60148084]\n",
      " [0.3923656 ]\n",
      " [0.44041798]\n",
      " [0.11338036]\n",
      " [0.14323722]\n",
      " [0.51358366]\n",
      " [0.18810636]\n",
      " [0.5165731 ]\n",
      " [0.83195543]\n",
      " [0.68237966]\n",
      " [0.17490092]\n",
      " [0.02926935]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.66964006]\n",
      "  [-0.9154622 ]\n",
      "  [-0.20412804]\n",
      "  [-0.9709251 ]\n",
      "  [ 0.6943234 ]\n",
      "  [ 0.7988634 ]\n",
      "  [-0.02...32 ]\n",
      "  [-1.3672918 ]\n",
      "  [-0.11714619]\n",
      "  [ 0.72112995]\n",
      "  [-0.5278377 ]\n",
      "  [-0.67568994]\n",
      "  [ 0.98999804]\n",
      "  [ 0.02622567]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.67962193]\n",
      "  [ 0.12927814]\n",
      "  [ 1.1221071 ]\n",
      "  [ 0.28971374]\n",
      "  [ 1.3253821 ]\n",
      "  [-0.33412802]\n",
      "  [-1.28...9  ]\n",
      "  [-0.01856623]\n",
      "  [ 0.12075179]\n",
      "  [-1.600797  ]\n",
      "  [ 0.23795342]\n",
      "  [-1.327664  ]\n",
      "  [-0.62417585]\n",
      "  [ 1.5067925 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5869ae50>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[602., 336., 914., 336.,  14., 494., 460., 132., 354.,  85., 488.,\n",
      "         70., 874., 828., 163.],\n",
      "       [978...    [523., 671., 282., 411., 986.,  73., 726.,  79.,  86., 274., 966.,\n",
      "        686., 601., 411., 577.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[602. 336. 914. 336.  14. 494. 460. 132. 354.  85. 488.  70. 874. 828.\n",
      "  163.]\n",
      " [978. 235. 382. 523. 5...6. 173. 386. 873. 237.\n",
      "  750.]\n",
      " [523. 671. 282. 411. 986.  73. 726.  79.  86. 274. 966. 686. 601. 411.\n",
      "  577.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5869ae50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5869ae50>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[602. 336. 914. 336.  14. 494. 460. 132. 354.  85. 488.  70. 874. 828.\n",
      "  163.]\n",
      " [978. 235. 382. 523. 52...582.  16. 173. 386. 873. 237.\n",
      "  750.]\n",
      " [523. 671. 282. 411. 986.  73. 726.  79.  86. 274. 966. 686. 601. 411.\n",
      "  577.]])\n",
      "        x_emb      = needle.Tensor([[[0.73471344]\n",
      "  [0.3860495 ]\n",
      "  [0.19326119]\n",
      "  [0.3860495 ]\n",
      "  [0.52377397]\n",
      "  [0.02214923]\n",
      "  [0.36280826]...0.24765871]\n",
      "  [0.12586887]\n",
      "  [0.3411911 ]\n",
      "  [0.37598246]\n",
      "  [0.9682565 ]\n",
      "  [0.5853928 ]\n",
      "  [0.4853901 ]\n",
      "  [0.8680575 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.73471344]\n",
      "  [0.3860495 ]\n",
      "  [0.19326119]\n",
      "  [0.3860495 ]\n",
      "  [0.52377397]\n",
      "  [0.02214923]\n",
      "  [0.36280826...871]\n",
      "  [0.12586887]\n",
      "  [0.3411911 ]\n",
      "  [0.37598246]\n",
      "  [0.9682565 ]\n",
      "  [0.5853928 ]\n",
      "  [0.4853901 ]\n",
      "  [0.8680575 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58698890>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.73471344]\n",
      "  [0.3860495 ]\n",
      "  [0.19326119]\n",
      "  [0.3860495 ]\n",
      "  [0.52377397]\n",
      "  [0.02214923]\n",
      "  [0.36280826]...0.24765871]\n",
      "  [0.12586887]\n",
      "  [0.3411911 ]\n",
      "  [0.37598246]\n",
      "  [0.9682565 ]\n",
      "  [0.5853928 ]\n",
      "  [0.4853901 ]\n",
      "  [0.8680575 ]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]])\n",
      "        inputs     = [needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4... [0.68717027]\n",
      " [0.619761  ]\n",
      " [0.32242626]\n",
      " [0.5368581 ]\n",
      " [0.7047916 ]\n",
      " [0.68060815]\n",
      " [0.04838482]\n",
      " [0.65940213]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5869a6d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58698890>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4...7521944 ]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5869a6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]\n",
      " [0.00076938]])\n",
      "        bias_ih    = needle.Tensor([[-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]\n",
      " [-0.5893189]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5869a6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.94722533]])\n",
      "        self       = needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4...\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]]), needle.Tensor([[-0.94722533]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56057450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4...\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]]), needle.Tensor([[-0.94722533]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56057450>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bf90f0>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd56056110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bf9b70>, 15, 1, 1') raised in repr()] Tensor object at 0x7fbd56056110>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]], device=cpu())\n",
      "        b          = NDArray([[-0.94722533]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56057450>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]], device=cpu())\n",
      "        b          = NDArray([[-0.94722533]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]], device=cpu())\n",
      "other = NDArray([[-0.94722533]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58699930>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5869ba70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56055970>, 15, 1, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.94722533]], device=cuda())\n",
      "out        = NDArray([[5.1694430e-34]\n",
      " [0.0000000e+00]\n",
      " [5.2219731e-01]\n",
      " [1.9297792e-01]\n",
      " [2.6959851e-01]\n",
      " [4.7421378e-01]\n",
      " [2.8871...]\n",
      " [7.3258370e-01]\n",
      " [4.3991014e-01]\n",
      " [6.5626377e-01]\n",
      " [2.8459588e-01]\n",
      " [8.0381083e-01]\n",
      " [8.1036437e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.73471344]\n",
      " [0.3860495 ]\n",
      " [0.19326119]\n",
      " [0.3860495 ]\n",
      " [0.52377397]\n",
      " [0.02214923]\n",
      " [0.36280826]\n",
      " [0.4319768 ]\n",
      " [0.09072176]\n",
      " [0.5233783 ]\n",
      " [0.04111104]\n",
      " [0.5935551 ]\n",
      " [0.13762197]\n",
      " [0.18124981]\n",
      " [0.7521944 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.2246162]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[1.2971741]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58561790>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[977.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[977.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58561790>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58561790>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[977.]])\n",
      "        x_emb      = needle.Tensor([[[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "   0.59746593 0.4952519  0.8550094 ...   0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "   0.57984185 0.640425   0.14615029 0.7549915 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "   0.59746593 0.4952519  0.8550094...317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "   0.57984185 0.640425   0.14615029 0.7549915 ]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58560d50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "   0.59746593 0.4952519  0.8550094 ...   0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "   0.57984185 0.640425   0.14615029 0.7549915 ]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0...27\n",
      "  0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]])\n",
      "        inputs     = [needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  ...7\n",
      "  0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58563f10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58560d50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  ...9  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58563f10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0...27\n",
      "  0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.94229925]])\n",
      "        bias_ih    = needle.Tensor([[-0.22283131]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58563f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.67889464]\n",
      " [-0.6933408 ]\n",
      " [-0.12318426]\n",
      " [-0.6338224 ]\n",
      " [-0.19247991]\n",
      " [ 0.16270602]\n",
      " [-0.658291  ]... 0.9143348 ]\n",
      " [-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]])\n",
      "        self       = needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0...27\n",
      "  0.57317793 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  ...0.9143348 ]\n",
      " [-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588b7790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  ...0.9143348 ]\n",
      " [-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588b7790>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56545630>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd588b7210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56547e70>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd588b7210>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0.73829...3 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.67889464]\n",
      " [-0.6933408 ]\n",
      " [-0.12318426]\n",
      " [-0.6338224 ]\n",
      " [-0.19247991]\n",
      " [ 0.16270602]\n",
      " [-0.658291  ]\n",
      " [ 0....-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd588b7790>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0.73829...3 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]], device=cpu())\n",
      "        b          = NDArray([[ 0.67889464]\n",
      " [-0.6933408 ]\n",
      " [-0.12318426]\n",
      " [-0.6338224 ]\n",
      " [-0.19247991]\n",
      " [ 0.16270602]\n",
      " [-0.658291  ]\n",
      " [ 0....-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0.73829...3 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]], device=cpu())\n",
      "other = NDArray([[ 0.67889464]\n",
      " [-0.6933408 ]\n",
      " [-0.12318426]\n",
      " [-0.6338224 ]\n",
      " [-0.19247991]\n",
      " [ 0.16270602]\n",
      " [-0.658291  ]\n",
      " [ 0....-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58563ab0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58560870>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd588b6e70>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.67889464]\n",
      " [-0.6933408 ]\n",
      " [-0.12318426]\n",
      " [-0.6338224 ]\n",
      " [-0.19247991]\n",
      " [ 0.16270602]\n",
      " [-0.658291  ]\n",
      " [ 0....-0.21687973]\n",
      " [ 0.98379564]\n",
      " [ 0.9777684 ]\n",
      " [-0.44277048]\n",
      " [-0.88767326]\n",
      " [ 0.62740636]\n",
      " [ 0.64931357]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.6921422  0.958491   0.47470626 0.7799768  0.43275425 0.76243955\n",
      "  0.59746593 0.4952519  0.8550094  0.73829...3 0.7287139  0.40937042 0.36602142 0.02466103 0.18738644\n",
      "  0.57984185 0.640425   0.14615029 0.7549915 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[1.6390197]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.6064572]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd55a46590>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[592.],\n",
      "       [808.],\n",
      "       [688.],\n",
      "       [203.],\n",
      "       [434.],\n",
      "       [696.],\n",
      "       [783.],\n",
      "       [400.],\n",
      "       [429.],\n",
      "       [426.],\n",
      "       [773.],\n",
      "       [900.],\n",
      "       [292.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[592.]\n",
      " [808.]\n",
      " [688.]\n",
      " [203.]\n",
      " [434.]\n",
      " [696.]\n",
      " [783.]\n",
      " [400.]\n",
      " [429.]\n",
      " [426.]\n",
      " [773.]\n",
      " [900.]\n",
      " [292.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a46590>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a46590>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[592.]\n",
      " [808.]\n",
      " [688.]\n",
      " [203.]\n",
      " [434.]\n",
      " [696.]\n",
      " [783.]\n",
      " [400.]\n",
      " [429.]\n",
      " [426.]\n",
      " [773.]\n",
      " [900.]\n",
      " [292.]])\n",
      "        x_emb      = needle.Tensor([[[9.88839269e-01 9.65746716e-02 9.01257038e-01 5.50566256e-01\n",
      "   5.29692531e-01 7.04168081e-01 8.683761...e-01 6.26879454e-01\n",
      "   2.68227667e-01 6.16117775e-01 1.45777641e-02 5.16916275e-01\n",
      "   5.20518720e-01 6.37879193e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[9.88839269e-01 9.65746716e-02 9.01257038e-01 5.50566256e-01\n",
      "   5.29692531e-01 7.04168081e-01 8.68376...26879454e-01\n",
      "   2.68227667e-01 6.16117775e-01 1.45777641e-02 5.16916275e-01\n",
      "   5.20518720e-01 6.37879193e-01]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a47a10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[9.88839269e-01 9.65746716e-02 9.01257038e-01 5.50566256e-01\n",
      "   5.29692531e-01 7.04168081e-01 8.683761...e-01 6.26879454e-01\n",
      "   2.68227667e-01 6.16117775e-01 1.45777641e-02 5.16916275e-01\n",
      "   5.20518720e-01 6.37879193e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0....94\n",
      "  0.85093296 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]])\n",
      "        inputs     = [needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0....46981376 0.57716537 0.00595579 0.38471258 0.56472933 0.27506396\n",
      "  0.61240304 0.72888696 0.20862734 0.65286946]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a473d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a47a10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0...48 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a473d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0....94\n",
      "  0.85093296 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.7162807]])\n",
      "        bias_ih    = needle.Tensor([[-0.25686562]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a473d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.9688169 ]\n",
      " [ 0.5384761 ]\n",
      " [-0.73570263]\n",
      " [-0.29783612]\n",
      " [ 0.45993066]\n",
      " [ 0.85172415]\n",
      " [ 0.31847668]...-0.2730049 ]\n",
      " [-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]])\n",
      "        self       = needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0....94\n",
      "  0.85093296 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0...0.2730049 ]\n",
      " [-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ea0590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0...0.2730049 ]\n",
      " [-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ea0590>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56244bf0>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56ea1c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56247770>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56ea1c50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0.164368...6 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]], device=cpu())\n",
      "        b          = NDArray([[ 0.9688169 ]\n",
      " [ 0.5384761 ]\n",
      " [-0.73570263]\n",
      " [-0.29783612]\n",
      " [ 0.45993066]\n",
      " [ 0.85172415]\n",
      " [ 0.31847668]\n",
      " [-0....-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56ea0590>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0.164368...6 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]], device=cpu())\n",
      "        b          = NDArray([[ 0.9688169 ]\n",
      " [ 0.5384761 ]\n",
      " [-0.73570263]\n",
      " [-0.29783612]\n",
      " [ 0.45993066]\n",
      " [ 0.85172415]\n",
      " [ 0.31847668]\n",
      " [-0....-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0.164368...6 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]], device=cpu())\n",
      "other = NDArray([[ 0.9688169 ]\n",
      " [ 0.5384761 ]\n",
      " [-0.73570263]\n",
      " [-0.29783612]\n",
      " [ 0.45993066]\n",
      " [ 0.85172415]\n",
      " [ 0.31847668]\n",
      " [-0....-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a44370>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55a44170>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56ea11f0>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.9688169 ]\n",
      " [ 0.5384761 ]\n",
      " [-0.73570263]\n",
      " [-0.29783612]\n",
      " [ 0.45993066]\n",
      " [ 0.85172415]\n",
      " [ 0.31847668]\n",
      " [-0....-0.7980449 ]\n",
      " [-0.84695804]\n",
      " [ 0.6820111 ]\n",
      " [-0.40152073]\n",
      " [-0.84529424]\n",
      " [-0.50106007]\n",
      " [-0.40356857]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.98883927 0.09657467 0.90125704 0.55056626 0.52969253 0.7041681\n",
      "  0.8683762  0.22217649 0.4478506  0.164368...6 0.25617248 0.48012128 0.77144325 0.43227372 0.27437466\n",
      "  0.6354425  0.24642324 0.5774552  0.21017751]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 0.00439571]]\n",
      "\n",
      " [[-0.10746338]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.45738477]]\n",
      "\n",
      " [[ 0.15482926]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd55bcec50>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[452.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[452.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55bcec50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd55bcec50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[452.]])\n",
      "        x_emb      = needle.Tensor([[[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "   0.9490208  0.64298594 0.61995435...   0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "   0.9492572  0.4939278  0.0427278  0.33877218]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "   0.9490208  0.64298594 0.6199543...37782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "   0.9492572  0.4939278  0.0427278  0.33877218]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b7e8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "   0.9490208  0.64298594 0.61995435...   0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "   0.9492572  0.4939278  0.0427278  0.33877218]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0...84\n",
      "  0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]])\n",
      "        inputs     = [needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 ...4\n",
      "  0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b7c650>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56b7e8d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 ...28 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b7c650>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0...84\n",
      "  0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.48482966]])\n",
      "        bias_ih    = needle.Tensor([[-0.81962466]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56b7c650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.9282111 ]\n",
      " [-0.21397603]\n",
      " [ 0.3001126 ]\n",
      " [-0.20856947]\n",
      " [ 0.1396265 ]\n",
      " [-0.96230835]\n",
      " [ 0.3877201 ]... 0.6720518 ]\n",
      " [ 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]])\n",
      "        self       = needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0...84\n",
      "  0.1737782  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 ...0.6720518 ]\n",
      " [ 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b7d490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 ...0.6720518 ]\n",
      " [ 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b7d490>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f2d030>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56b7ee90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56f2e070>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd56b7ee90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0.71046...  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]], device=cpu())\n",
      "        b          = NDArray([[ 0.9282111 ]\n",
      " [-0.21397603]\n",
      " [ 0.3001126 ]\n",
      " [-0.20856947]\n",
      " [ 0.1396265 ]\n",
      " [-0.96230835]\n",
      " [ 0.3877201 ]\n",
      " [ 0.... 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b7d490>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0.71046...  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]], device=cpu())\n",
      "        b          = NDArray([[ 0.9282111 ]\n",
      " [-0.21397603]\n",
      " [ 0.3001126 ]\n",
      " [-0.20856947]\n",
      " [ 0.1396265 ]\n",
      " [-0.96230835]\n",
      " [ 0.3877201 ]\n",
      " [ 0.... 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0.71046...  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]], device=cpu())\n",
      "other = NDArray([[ 0.9282111 ]\n",
      " [-0.21397603]\n",
      " [ 0.3001126 ]\n",
      " [-0.20856947]\n",
      " [ 0.1396265 ]\n",
      " [-0.96230835]\n",
      " [ 0.3877201 ]\n",
      " [ 0.... 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b7c330>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56b7d430>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b7f770>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.9282111 ]\n",
      " [-0.21397603]\n",
      " [ 0.3001126 ]\n",
      " [-0.20856947]\n",
      " [ 0.1396265 ]\n",
      " [-0.96230835]\n",
      " [ 0.3877201 ]\n",
      " [ 0.... 0.49090874]\n",
      " [ 0.1011858 ]\n",
      " [ 0.00196087]\n",
      " [-0.04450828]\n",
      " [ 0.20495152]\n",
      " [-0.81519216]\n",
      " [ 0.3411678 ]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.8374713  0.4973128  0.9429316  0.35421675 0.6887656  0.72342324\n",
      "  0.9490208  0.64298594 0.61995435 0.71046...  0.31156528 0.28274894 0.2824621  0.8309726  0.14772534\n",
      "  0.9492572  0.4939278  0.0427278  0.33877218]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.9447454 ]]\n",
      "\n",
      " [[-0.31467927]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.7097622]]\n",
      "\n",
      " [[-1.3907409]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57e44690>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[886.],\n",
      "       [588.],\n",
      "       [112.],\n",
      "       [434.],\n",
      "       [306.],\n",
      "       [228.],\n",
      "       [119.],\n",
      "       [298.],\n",
      "       [724.],\n",
      "       [886.],\n",
      "       [688.],\n",
      "       [470.],\n",
      "       [ 19.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[886.]\n",
      " [588.]\n",
      " [112.]\n",
      " [434.]\n",
      " [306.]\n",
      " [228.]\n",
      " [119.]\n",
      " [298.]\n",
      " [724.]\n",
      " [886.]\n",
      " [688.]\n",
      " [470.]\n",
      " [ 19.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e44690>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57e44690>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[886.]\n",
      " [588.]\n",
      " [112.]\n",
      " [434.]\n",
      " [306.]\n",
      " [228.]\n",
      " [119.]\n",
      " [298.]\n",
      " [724.]\n",
      " [886.]\n",
      " [688.]\n",
      " [470.]\n",
      " [ 19.]])\n",
      "        x_emb      = needle.Tensor([[[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "   0.7432981  0.5186758  0.5680323 ...   0.17345521 0.73472846 0.7885748  0.5878209  0.6287262  0.01643471\n",
      "   0.9340525  0.6674605  0.6903384  0.03865733]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "   0.7432981  0.5186758  0.5680323...345521 0.73472846 0.7885748  0.5878209  0.6287262  0.01643471\n",
      "   0.9340525  0.6674605  0.6903384  0.03865733]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e47050>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "   0.7432981  0.5186758  0.5680323 ...   0.17345521 0.73472846 0.7885748  0.5878209  0.6287262  0.01643471\n",
      "   0.9340525  0.6674605  0.6903384  0.03865733]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        h0         = [needle.Tensor([[0.]]), needle.Tensor([[0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0...87\n",
      "  0.87704545 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]])\n",
      "        inputs     = [needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  ....48128018 0.5760443  0.47614652 0.12351523 0.33043292 0.41019937\n",
      "  0.9580558  0.35073882 0.8700022  0.82205415]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e441d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57e47050>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  ...7  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]]), needle.Tensor([[0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e441d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0...87\n",
      "  0.87704545 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[0.51159215]])\n",
      "        bias_ih    = needle.Tensor([[0.77179456]])\n",
      "        h          = needle.Tensor([[0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e441d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.6837152 ]\n",
      " [ 0.63306594]\n",
      " [ 0.18351603]\n",
      " [-0.22483599]\n",
      " [ 0.51724935]\n",
      " [ 0.2858292 ]\n",
      " [-0.7660695 ]... 0.7115432 ]\n",
      " [ 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]])\n",
      "        self       = needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0...87\n",
      "  0.87704545 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  ...0.7115432 ]\n",
      " [ 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830e010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  ...0.7115432 ]\n",
      " [ 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830e010>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57641430>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd5830f090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57641870>, 1, 34, 1') raised in repr()] Tensor object at 0x7fbd5830f090>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0.01996...5 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]], device=cpu())\n",
      "        b          = NDArray([[ 0.6837152 ]\n",
      " [ 0.63306594]\n",
      " [ 0.18351603]\n",
      " [-0.22483599]\n",
      " [ 0.51724935]\n",
      " [ 0.2858292 ]\n",
      " [-0.7660695 ]\n",
      " [-0.... 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5830e010>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0.01996...5 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]], device=cpu())\n",
      "        b          = NDArray([[ 0.6837152 ]\n",
      " [ 0.63306594]\n",
      " [ 0.18351603]\n",
      " [-0.22483599]\n",
      " [ 0.51724935]\n",
      " [ 0.2858292 ]\n",
      " [-0.7660695 ]\n",
      " [-0.... 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0.01996...5 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]], device=cpu())\n",
      "other = NDArray([[ 0.6837152 ]\n",
      " [ 0.63306594]\n",
      " [ 0.18351603]\n",
      " [-0.22483599]\n",
      " [ 0.51724935]\n",
      " [ 0.2858292 ]\n",
      " [-0.7660695 ]\n",
      " [-0.... 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5889ffb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57e46070>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5830ff70>, 1, 34, 1\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.6837152 ]\n",
      " [ 0.63306594]\n",
      " [ 0.18351603]\n",
      " [-0.22483599]\n",
      " [ 0.51724935]\n",
      " [ 0.2858292 ]\n",
      " [-0.7660695 ]\n",
      " [-0.... 0.29897773]\n",
      " [-0.32030225]\n",
      " [ 0.63079286]\n",
      " [ 0.15608442]\n",
      " [ 0.38872623]\n",
      " [-0.2301113 ]\n",
      " [-0.67812467]], device=cuda())\n",
      "out        = NDArray([[8.177487e+35]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.545761   0.7070672  0.9560082  0.15743816 0.57380193 0.83187854\n",
      "  0.7432981  0.5186758  0.5680323  0.01996...5 0.6271237  0.05646462 0.6704482  0.146      0.43342328\n",
      "  0.91563493 0.624407   0.9203305  0.82310563]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.49826416]\n",
      "  [-0.00420918]\n",
      "  [-0.778765  ]\n",
      "  [ 0.5665142 ]\n",
      "  [ 0.5129477 ]\n",
      "  [-0.6588042 ]\n",
      "  [ 0.52...057]\n",
      "  [-0.6275853 ]\n",
      "  [-0.2751018 ]\n",
      "  [-2.2332745 ]\n",
      "  [ 0.5045306 ]\n",
      "  [-2.09388   ]\n",
      "  [-0.8698966 ]\n",
      "  [-1.8622564 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.5932234 ]\n",
      "  [-0.29791418]\n",
      "  [-0.15485376]\n",
      "  [ 0.42473754]\n",
      "  [-0.30823445]\n",
      "  [ 0.49682674]\n",
      "  [ 1.59...203]\n",
      "  [-0.4263854 ]\n",
      "  [ 0.8258535 ]\n",
      "  [-1.321409  ]\n",
      "  [ 0.20110615]\n",
      "  [-0.02190047]\n",
      "  [ 0.5276992 ]\n",
      "  [ 0.35086665]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57dd9e10>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[  0., 748., 916., 842., 890., 375.,  75., 719., 645., 580., 261.,\n",
      "        855., 807., 584., 649.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[  0. 748. 916. 842. 890. 375.  75. 719. 645. 580. 261. 855. 807. 584.\n",
      "  649.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57dd9e10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57dd9e10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[  0. 748. 916. 842. 890. 375.  75. 719. 645. 580. 261. 855. 807. 584.\n",
      "  649.]])\n",
      "        x_emb      = needle.Tensor([[[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "   3.85846585e-01 9.03731287e-01 8.670153...e-02 1.49668247e-01\n",
      "   2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "   8.47862899e-01 3.47099930e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "   3.85846585e-01 9.03731287e-01 8.67015...49668247e-01\n",
      "   2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "   8.47862899e-01 3.47099930e-01]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57dd9f10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "   3.85846585e-01 9.03731287e-01 8.670153...e-02 1.49668247e-01\n",
      "   2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "   8.47862899e-01 3.47099930e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302...133e-02 1.49668247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]])\n",
      "        inputs     = [needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.6701530...33e-02 1.49668247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dda190>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57dd9f10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.6701530...9930e-01]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dda190>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302...133e-02 1.49668247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]\n",
      " [-0.64221907]])\n",
      "        bias_ih    = needle.Tensor([[0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]\n",
      " [0.01238322]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57dda190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.545146  ]\n",
      " [ 0.12241244]\n",
      " [-0.5968161 ]\n",
      " [-0.6030937 ]\n",
      " [-0.09278905]\n",
      " [ 0.20080829]\n",
      " [-0.96513164]... 0.63296807]\n",
      " [ 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]])\n",
      "        self       = needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302...133e-02 1.49668247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.6701530...0.63296807]\n",
      " [ 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58035850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.6701530...0.63296807]\n",
      " [ 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58035850>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56d6f570>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58034b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56d6e330>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd58034b90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302e-01 8...8247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]], device=cpu())\n",
      "        b          = NDArray([[ 0.545146  ]\n",
      " [ 0.12241244]\n",
      " [-0.5968161 ]\n",
      " [-0.6030937 ]\n",
      " [-0.09278905]\n",
      " [ 0.20080829]\n",
      " [-0.96513164]\n",
      " [ 0.... 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58035850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302e-01 8...8247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]], device=cpu())\n",
      "        b          = NDArray([[ 0.545146  ]\n",
      " [ 0.12241244]\n",
      " [-0.5968161 ]\n",
      " [-0.6030937 ]\n",
      " [-0.09278905]\n",
      " [ 0.20080829]\n",
      " [-0.96513164]\n",
      " [ 0.... 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302e-01 8...8247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]], device=cpu())\n",
      "other = NDArray([[ 0.545146  ]\n",
      " [ 0.12241244]\n",
      " [-0.5968161 ]\n",
      " [-0.6030937 ]\n",
      " [-0.09278905]\n",
      " [ 0.20080829]\n",
      " [-0.96513164]\n",
      " [ 0.... 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580354b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57dd88f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580378f0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[ 0.545146  ]\n",
      " [ 0.12241244]\n",
      " [-0.5968161 ]\n",
      " [-0.6030937 ]\n",
      " [-0.09278905]\n",
      " [ 0.20080829]\n",
      " [-0.96513164]\n",
      " [ 0.... 0.12685871]\n",
      " [ 0.597178  ]\n",
      " [ 0.6114609 ]\n",
      " [ 0.00754774]\n",
      " [ 0.06252706]\n",
      " [-0.18431407]\n",
      " [-0.8263594 ]], device=cuda())\n",
      "out        = NDArray([[3.7951010e-34]\n",
      " [0.0000000e+00]\n",
      " [4.2160907e-01]\n",
      " [5.9826690e-01]\n",
      " [6.9493920e-01]\n",
      " [8.1588525e-01]\n",
      " [2.1604...]\n",
      " [4.5993984e-01]\n",
      " [2.1139106e-02]\n",
      " [9.9421275e-01]\n",
      " [2.2011951e-01]\n",
      " [2.2627149e-02]\n",
      " [7.8336215e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[8.40656757e-01 6.36002183e-01 4.21586990e-01 7.41328955e-01\n",
      "  3.85846585e-01 9.03731287e-01 8.67015302e-01 8...8247e-01\n",
      "  2.80131906e-01 3.49508166e-01 7.74256885e-01 2.02679992e-01\n",
      "  8.47862899e-01 3.47099930e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.73320764]\n",
      "  [-1.372612  ]\n",
      "  [ 0.58127284]\n",
      "  [ 0.52594596]\n",
      "  [ 0.0513191 ]\n",
      "  [-2.0963879 ]\n",
      "  [ 0.19...572]\n",
      "  [-0.3307994 ]\n",
      "  [ 0.40039995]\n",
      "  [-0.1797395 ]\n",
      "  [ 0.9901929 ]\n",
      "  [ 0.8515809 ]\n",
      "  [ 0.48189518]\n",
      "  [ 0.14024316]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.745905  ]\n",
      "  [ 0.30511874]\n",
      "  [ 0.4134695 ]\n",
      "  [-0.37755433]\n",
      "  [ 0.25637156]\n",
      "  [-0.8235092 ]\n",
      "  [ 0.58...397]\n",
      "  [-0.7004886 ]\n",
      "  [-1.1323057 ]\n",
      "  [ 1.8091922 ]\n",
      "  [-1.2975111 ]\n",
      "  [-0.84208244]\n",
      "  [-0.27386376]\n",
      "  [ 0.03769422]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5623f150>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[203., 847., 656., 888., 146., 938., 811., 779., 792., 415., 238.,\n",
      "        974., 361., 564., 316.],\n",
      "       [  1...    [292., 378., 980., 520., 756., 353., 980., 287.,  34., 104., 218.,\n",
      "        397., 682., 817.,  42.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[203. 847. 656. 888. 146. 938. 811. 779. 792. 415. 238. 974. 361. 564.\n",
      "  316.]\n",
      " [  1. 432. 524. 118. 7...0. 953. 874. 104.  78.\n",
      "  743.]\n",
      " [292. 378. 980. 520. 756. 353. 980. 287.  34. 104. 218. 397. 682. 817.\n",
      "   42.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5623f150>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5623f150>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[203. 847. 656. 888. 146. 938. 811. 779. 792. 415. 238. 974. 361. 564.\n",
      "  316.]\n",
      " [  1. 432. 524. 118. 71...962. 480. 953. 874. 104.  78.\n",
      "  743.]\n",
      " [292. 378. 980. 520. 756. 353. 980. 287.  34. 104. 218. 397. 682. 817.\n",
      "   42.]])\n",
      "        x_emb      = needle.Tensor([[[0.16515838 0.2763946  0.1829169  ... 0.70800894 0.83600765 0.09504323]\n",
      "  [0.95113456 0.15222381 0.749...976  ... 0.26441666 0.6756832  0.49011096]\n",
      "  [0.13958928 0.48786587 0.5105826  ... 0.8685303  0.33283216 0.67984307]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.16515838 0.2763946  0.1829169  ... 0.70800894 0.83600765 0.09504323]\n",
      "  [0.95113456 0.15222381 0.74.... 0.26441666 0.6756832  0.49011096]\n",
      "  [0.13958928 0.48786587 0.5105826  ... 0.8685303  0.33283216 0.67984307]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5623e850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.16515838 0.2763946  0.1829169  ... 0.70800894 0.83600765 0.09504323]\n",
      "  [0.95113456 0.15222381 0.749...976  ... 0.26441666 0.6756832  0.49011096]\n",
      "  [0.13958928 0.48786587 0.5105826  ... 0.8685303  0.33283216 0.67984307]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0...592\n",
      "  0.5085038  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]])\n",
      "        inputs     = [needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 ...1 9.73727703e-01\n",
      "  4.64226872e-01 2.12620407e-01 7.00745225e-01 1.64258108e-01\n",
      "  6.74676716e-01 7.26768255e-01]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623ed50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5623e850>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 ...03469574]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623ed50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0...592\n",
      "  0.5085038  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]\n",
      " [0.45365465]])\n",
      "        bias_ih    = needle.Tensor([[-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]\n",
      " [-0.27880555]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5623ed50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.88703966]\n",
      " [ 0.2878877 ]\n",
      " [ 0.67704725]\n",
      " [-0.45074695]\n",
      " [ 0.24426913]\n",
      " [ 0.7879833 ]\n",
      " [ 0.6582043 ]...-0.6986202 ]\n",
      " [-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]])\n",
      "        self       = needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0...592\n",
      "  0.5085038  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 ...0.6986202 ]\n",
      " [-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa52d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 ...0.6986202 ]\n",
      " [-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa52d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd587a9570>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57aa7250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd587aa670>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57aa7250>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0.11859...8  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]], device=cpu())\n",
      "        b          = NDArray([[-0.88703966]\n",
      " [ 0.2878877 ]\n",
      " [ 0.67704725]\n",
      " [-0.45074695]\n",
      " [ 0.24426913]\n",
      " [ 0.7879833 ]\n",
      " [ 0.6582043 ]\n",
      " [ 0....-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57aa52d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0.11859...8  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]], device=cpu())\n",
      "        b          = NDArray([[-0.88703966]\n",
      " [ 0.2878877 ]\n",
      " [ 0.67704725]\n",
      " [-0.45074695]\n",
      " [ 0.24426913]\n",
      " [ 0.7879833 ]\n",
      " [ 0.6582043 ]\n",
      " [ 0....-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0.11859...8  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]], device=cpu())\n",
      "other = NDArray([[-0.88703966]\n",
      " [ 0.2878877 ]\n",
      " [ 0.67704725]\n",
      " [-0.45074695]\n",
      " [ 0.24426913]\n",
      " [ 0.7879833 ]\n",
      " [ 0.6582043 ]\n",
      " [ 0....-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5623e2f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5623d030>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57aa4cb0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.88703966]\n",
      " [ 0.2878877 ]\n",
      " [ 0.67704725]\n",
      " [-0.45074695]\n",
      " [ 0.24426913]\n",
      " [ 0.7879833 ]\n",
      " [ 0.6582043 ]\n",
      " [ 0....-0.8139016 ]\n",
      " [ 0.63785315]\n",
      " [-0.38456607]\n",
      " [ 0.13552988]\n",
      " [ 0.5896238 ]\n",
      " [-0.2644632 ]\n",
      " [ 0.511323  ]], device=cuda())\n",
      "out        = NDArray([[9.6185216e-35]\n",
      " [0.0000000e+00]\n",
      " [2.7165475e-01]\n",
      " [7.0758218e-01]\n",
      " [5.9994102e-01]\n",
      " [8.4636468e-01]\n",
      " [5.6711...]\n",
      " [5.9791684e-01]\n",
      " [3.7449092e-01]\n",
      " [2.2123802e-01]\n",
      " [6.9990313e-01]\n",
      " [7.6802599e-01]\n",
      " [4.4628793e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.16515838 0.2763946  0.1829169  0.8183867  0.01992628 0.07445184\n",
      "  0.65685576 0.61492    0.11803348 0.11859...8  0.04303359 0.16844627 0.879708   0.25471318 0.7574668\n",
      "  0.95612687 0.6096199  0.6276778  0.03469574]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.8851777 ]\n",
      "  [ 0.24307893]\n",
      "  [ 0.68847287]\n",
      "  [-1.3799838 ]\n",
      "  [-1.33685   ]\n",
      "  [ 1.1902821 ]\n",
      "  [ 1.03...8  ]\n",
      "  [ 1.4142705 ]\n",
      "  [-0.03709865]\n",
      "  [-1.1814404 ]\n",
      "  [-1.001545  ]\n",
      "  [-1.1155485 ]\n",
      "  [ 1.5290588 ]\n",
      "  [ 1.3629605 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.3051119 ]\n",
      "  [-0.61699307]\n",
      "  [ 0.4330364 ]\n",
      "  [ 1.957986  ]\n",
      "  [-0.69622445]\n",
      "  [-1.5396078 ]\n",
      "  [-0.30...71 ]\n",
      "  [ 0.62624824]\n",
      "  [ 1.1121117 ]\n",
      "  [-0.5874289 ]\n",
      "  [ 2.606711  ]\n",
      "  [ 0.59429485]\n",
      "  [ 1.5611916 ]\n",
      "  [-0.35100096]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58259610>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[748., 174., 406., 175., 388., 281., 582., 985.,  64., 475., 259.,\n",
      "        398., 866.,  93., 155.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[748. 174. 406. 175. 388. 281. 582. 985.  64. 475. 259. 398. 866.  93.\n",
      "  155.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58259610>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58259610>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[748. 174. 406. 175. 388. 281. 582. 985.  64. 475. 259. 398. 866.  93.\n",
      "  155.]])\n",
      "        x_emb      = needle.Tensor([[[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "   9.28050637e-01 9.20076072e-01 3.106396...e-01 9.73913193e-01\n",
      "   2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "   8.41440082e-01 3.69355008e-02]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "   9.28050637e-01 9.20076072e-01 3.10639...73913193e-01\n",
      "   2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "   8.41440082e-01 3.69355008e-02]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5825a850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "   9.28050637e-01 9.20076072e-01 3.106396...e-01 9.73913193e-01\n",
      "   2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "   8.41440082e-01 3.69355008e-02]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650...334e-01 9.73913193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]])\n",
      "        inputs     = [needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.1063965...34e-01 9.73913193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5825bc90>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5825a850>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.1063965...5008e-02]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5825bc90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650...334e-01 9.73913193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]\n",
      " [0.81978726]])\n",
      "        bias_ih    = needle.Tensor([[-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]\n",
      " [-0.926972]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5825bc90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.7226416 ]\n",
      " [ 0.5262544 ]\n",
      " [ 0.18669629]\n",
      " [-0.5665683 ]\n",
      " [ 0.55775285]\n",
      " [ 0.3181436 ]\n",
      " [ 0.85676515]... 0.09992349]\n",
      " [ 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]])\n",
      "        self       = needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650...334e-01 9.73913193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.1063965...0.09992349]\n",
      " [ 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e45090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.1063965...0.09992349]\n",
      " [ 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e45090>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd5716f530>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57e44410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd580df9b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd57e44410>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650e-01 1...3193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]], device=cpu())\n",
      "        b          = NDArray([[-0.7226416 ]\n",
      " [ 0.5262544 ]\n",
      " [ 0.18669629]\n",
      " [-0.5665683 ]\n",
      " [ 0.55775285]\n",
      " [ 0.3181436 ]\n",
      " [ 0.85676515]\n",
      " [-0.... 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e45090>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650e-01 1...3193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]], device=cpu())\n",
      "        b          = NDArray([[-0.7226416 ]\n",
      " [ 0.5262544 ]\n",
      " [ 0.18669629]\n",
      " [-0.5665683 ]\n",
      " [ 0.55775285]\n",
      " [ 0.3181436 ]\n",
      " [ 0.85676515]\n",
      " [-0.... 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650e-01 1...3193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]], device=cpu())\n",
      "other = NDArray([[-0.7226416 ]\n",
      " [ 0.5262544 ]\n",
      " [ 0.18669629]\n",
      " [-0.5665683 ]\n",
      " [ 0.55775285]\n",
      " [ 0.3181436 ]\n",
      " [ 0.85676515]\n",
      " [-0.... 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5825aeb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58258630>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57e44ff0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.7226416 ]\n",
      " [ 0.5262544 ]\n",
      " [ 0.18669629]\n",
      " [-0.5665683 ]\n",
      " [ 0.55775285]\n",
      " [ 0.3181436 ]\n",
      " [ 0.85676515]\n",
      " [-0.... 0.60159373]\n",
      " [ 0.8076695 ]\n",
      " [-0.5789407 ]\n",
      " [ 0.62698555]\n",
      " [-0.7843939 ]\n",
      " [ 0.83323526]\n",
      " [-0.10978711]], device=cuda())\n",
      "out        = NDArray([[6.52694558e-35]\n",
      " [0.00000000e+00]\n",
      " [9.59946290e-02]\n",
      " [9.93686318e-02]\n",
      " [9.26303506e-01]\n",
      " [8.25074673e-01]\n",
      " [...70038682e-01]\n",
      " [9.77409422e-01]\n",
      " [1.13549136e-01]\n",
      " [9.98282194e-01]\n",
      " [5.39432704e-01]\n",
      " [3.37528616e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[4.58708555e-01 9.80726361e-01 4.03687835e-01 7.99291074e-01\n",
      "  9.28050637e-01 9.20076072e-01 3.10639650e-01 1...3193e-01\n",
      "  2.04601720e-01 8.84620011e-01 7.34929025e-01 1.14214778e-01\n",
      "  8.41440082e-01 3.69355008e-02]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.6789131 ]\n",
      "  [ 0.97778374]\n",
      "  [ 0.88697517]\n",
      "  [-1.8629597 ]\n",
      "  [ 0.8379305 ]\n",
      "  [ 0.7792777 ]\n",
      "  [ 0.63...417]\n",
      "  [-1.7869924 ]\n",
      "  [-0.0246618 ]\n",
      "  [-0.18643798]\n",
      "  [ 1.1133202 ]\n",
      "  [ 0.31281495]\n",
      "  [-0.39757648]\n",
      "  [-0.7859053 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.1653954 ]\n",
      "  [-0.09795082]\n",
      "  [ 0.20096497]\n",
      "  [ 1.0120441 ]\n",
      "  [ 0.40973842]\n",
      "  [ 1.5537851 ]\n",
      "  [ 0.11...63 ]\n",
      "  [-0.2144897 ]\n",
      "  [ 0.01634782]\n",
      "  [-0.80789953]\n",
      "  [ 0.26280165]\n",
      "  [-0.4075866 ]\n",
      "  [ 0.14226228]\n",
      "  [-0.20532407]]])\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd576fd090>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[387., 560., 825., 921., 221., 754., 978., 657., 555.,  87., 346.,\n",
      "        624., 701., 981., 545.],\n",
      "       [586...    [538., 740., 345., 686., 510., 142., 211., 283., 368., 738., 281.,\n",
      "        410., 863., 313., 764.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[387. 560. 825. 921. 221. 754. 978. 657. 555.  87. 346. 624. 701. 981.\n",
      "  545.]\n",
      " [586. 457. 773. 743. 3...2. 606. 265. 948. 472.\n",
      "  867.]\n",
      " [538. 740. 345. 686. 510. 142. 211. 283. 368. 738. 281. 410. 863. 313.\n",
      "  764.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd576fd090>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd576fd090>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[387. 560. 825. 921. 221. 754. 978. 657. 555.  87. 346. 624. 701. 981.\n",
      "  545.]\n",
      " [586. 457. 773. 743. 37...143. 482. 606. 265. 948. 472.\n",
      "  867.]\n",
      " [538. 740. 345. 686. 510. 142. 211. 283. 368. 738. 281. 410. 863. 313.\n",
      "  764.]])\n",
      "        x_emb      = needle.Tensor([[[4.24933314e-01 6.13215506e-01 2.43263945e-01 ... 9.80434269e-02\n",
      "   2.94689059e-01 3.30931455e-01]\n",
      "  [...5.40416658e-01]\n",
      "  [5.92850626e-01 2.74357527e-01 5.15462935e-01 ... 6.12852812e-01\n",
      "   2.20203668e-01 9.91820514e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[4.24933314e-01 6.13215506e-01 2.43263945e-01 ... 9.80434269e-02\n",
      "   2.94689059e-01 3.30931455e-01]\n",
      "  ...658e-01]\n",
      "  [5.92850626e-01 2.74357527e-01 5.15462935e-01 ... 6.12852812e-01\n",
      "   2.20203668e-01 9.91820514e-01]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576fd250>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[4.24933314e-01 6.13215506e-01 2.43263945e-01 ... 9.80434269e-02\n",
      "   2.94689059e-01 3.30931455e-01]\n",
      "  [...5.40416658e-01]\n",
      "  [5.92850626e-01 2.74357527e-01 5.15462935e-01 ... 6.12852812e-01\n",
      "   2.20203668e-01 9.91820514e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        h0         = [needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0....18\n",
      "  0.6747528  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]])\n",
      "        inputs     = [needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0...1 4.15192582e-02\n",
      "  9.76991951e-01 2.93078154e-01 9.81905997e-01 5.49620688e-01\n",
      "  7.79234529e-01 5.92769086e-01]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576fed50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd576fd250>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0...04843394]]), needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576fed50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0....18\n",
      "  0.6747528  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]\n",
      " [-0.23650438]])\n",
      "        bias_ih    = needle.Tensor([[-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]\n",
      " [-0.08680618]])\n",
      "        h          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd576fed50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.2528183 ]\n",
      " [-0.2707976 ]\n",
      " [-0.03433478]\n",
      " [ 0.0519855 ]\n",
      " [-0.85751724]\n",
      " [-0.61985385]\n",
      " [ 0.01277566]... 0.23779488]\n",
      " [-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]])\n",
      "        self       = needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0....18\n",
      "  0.6747528  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0...0.23779488]\n",
      " [-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bd7050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0...0.23779488]\n",
      " [-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bd7050>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b58b0>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56bd78d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b5e30>, 15, 34, 1') raised in repr()] Tensor object at 0x7fbd56bd78d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0.193209...  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]], device=cpu())\n",
      "        b          = NDArray([[-0.2528183 ]\n",
      " [-0.2707976 ]\n",
      " [-0.03433478]\n",
      " [ 0.0519855 ]\n",
      " [-0.85751724]\n",
      " [-0.61985385]\n",
      " [ 0.01277566]\n",
      " [ 0....-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56bd7050>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0.193209...  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]], device=cpu())\n",
      "        b          = NDArray([[-0.2528183 ]\n",
      " [-0.2707976 ]\n",
      " [-0.03433478]\n",
      " [ 0.0519855 ]\n",
      " [-0.85751724]\n",
      " [-0.61985385]\n",
      " [ 0.01277566]\n",
      " [ 0....-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0.193209...  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]], device=cpu())\n",
      "other = NDArray([[-0.2528183 ]\n",
      " [-0.2707976 ]\n",
      " [-0.03433478]\n",
      " [ 0.0519855 ]\n",
      " [-0.85751724]\n",
      " [-0.61985385]\n",
      " [ 0.01277566]\n",
      " [ 0....-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd576fc030>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd576fcff0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56bd4fb0>, 15, 34, 1\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.2528183 ]\n",
      " [-0.2707976 ]\n",
      " [-0.03433478]\n",
      " [ 0.0519855 ]\n",
      " [-0.85751724]\n",
      " [-0.61985385]\n",
      " [ 0.01277566]\n",
      " [ 0....-0.0575099 ]\n",
      " [ 0.4005586 ]\n",
      " [-0.9910094 ]\n",
      " [-0.7558068 ]\n",
      " [-0.00825447]\n",
      " [ 0.15922725]\n",
      " [ 0.25681996]], device=cuda())\n",
      "out        = NDArray([[8.7457457e-35]\n",
      " [0.0000000e+00]\n",
      " [2.7876458e-04]\n",
      " [5.3069737e-02]\n",
      " [3.7081307e-01]\n",
      " [5.1831824e-01]\n",
      " [9.1813...]\n",
      " [1.8486996e-01]\n",
      " [5.0748122e-01]\n",
      " [1.5568469e-01]\n",
      " [1.8117563e-01]\n",
      " [6.9667596e-01]\n",
      " [2.5049013e-01]], device=cpu())\n",
      "p          = 1\n",
      "self       = NDArray([[0.4249333  0.6132155  0.24326394 0.1560639  0.06218843 0.5541164\n",
      "  0.5366623  0.7400256  0.19034265 0.193209...  0.21891965 0.47301948 0.67572147 0.8488977  0.13172118\n",
      "  0.6000578  0.20159799 0.9454406  0.04843394]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.06094273 -0.2912912   0.47195283  0.45639506  2.4895782\n",
      "    0.7864952  -0.48835593  0.57420105  0.66476667 -0.03622772\n",
      "   -1.5440154  -0.82229316]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-1.2642894   1.3878957   1.3517418   0.10421322 -0.08914617\n",
      "   -0.5841063  -0.64045036 -0.5942677   1.1878549  -0.72258824\n",
      "    0.4520281  -0.06479085]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5855ad50>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[217.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[217.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5855ad50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5855ad50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[217.]])\n",
      "        x_emb      = needle.Tensor([[[0.22563384]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.22563384]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd588a4950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.22563384]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.22563384]])\n",
      "        inputs     = [needle.Tensor([[0.22563384]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd588a7490>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd588a4950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22563384]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd588a7490>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.22563384]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.20291972 -0.00862303  0.2331248   0.27534795 -0.25320184  0.26487142\n",
      "  -0.25457639 -0.14434187 -0.18837938 -0.11914508 -0.18329468  0.218669  ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.21244341  0.2743253   0.12960067  0.2019004   0.24068892 -0.168783\n",
      "   0.18382272 -0.23983079  0.0671708  -0.17929432 -0.12541288 -0.11249349]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd588a7490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]])\n",
      "        self       = needle.Tensor([[0.22563384]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.22563384]]), needle.Tensor([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56946910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.22563384]]), needle.Tensor([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56946910>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56e1dc70>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56947b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56e1f5b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56947b50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22563384]], device=cpu())\n",
      "        b          = NDArray([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56946910>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.22563384]], device=cpu())\n",
      "        b          = NDArray([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.22563384]], device=cpu())\n",
      "other = NDArray([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd588a7a30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd588a7170>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56946d70>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.20578334  0.05069852  0.10156259  0.21568668  0.17706314 -0.07344154\n",
      "  -0.25213236  0.09246284 -0.00270635  0.17544767  0.19026685 -0.21250647]], device=cuda())\n",
      "out        = NDArray([[7.412989e-35 0.000000e+00 7.412888e-35 0.000000e+00 1.000000e+00\n",
      "  1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00 1.000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.22563384]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.0100939  -1.8342807   0.90794927  0.7957788   0.7806254\n",
      "   -0.12405317 -0.5231924  -0.638828    0.08892103 -1.3035232\n",
      "   -0.07482408 -0.655063  ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.23184603 -0.7649522   0.84586996 -0.41872254 -0.40268555\n",
      "   -0.5080196  -0.8270608   1.3414518  -0.5040363  -1.2470251\n",
      "   -0.32250673  0.25304204]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58732f50>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[403.],\n",
      "       [547.],\n",
      "       [664.],\n",
      "       [721.],\n",
      "       [774.],\n",
      "       [ 80.],\n",
      "       [ 30.],\n",
      "       [526.],\n",
      "       [355.],\n",
      "       [387.],\n",
      "       [627.],\n",
      "       [ 14.],\n",
      "       [851.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[403.]\n",
      " [547.]\n",
      " [664.]\n",
      " [721.]\n",
      " [774.]\n",
      " [ 80.]\n",
      " [ 30.]\n",
      " [526.]\n",
      " [355.]\n",
      " [387.]\n",
      " [627.]\n",
      " [ 14.]\n",
      " [851.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58732f50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58732f50>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[403.]\n",
      " [547.]\n",
      " [664.]\n",
      " [721.]\n",
      " [774.]\n",
      " [ 80.]\n",
      " [ 30.]\n",
      " [526.]\n",
      " [355.]\n",
      " [387.]\n",
      " [627.]\n",
      " [ 14.]\n",
      " [851.]])\n",
      "        x_emb      = needle.Tensor([[[0.95364356]]\n",
      "\n",
      " [[0.31119615]]\n",
      "\n",
      " [[0.29442245]]\n",
      "\n",
      " [[0.00582019]]\n",
      "\n",
      " [[0.40635252]]\n",
      "\n",
      " [[0.8249471 ]]\n",
      "\n",
      " [[0.79981947]]\n",
      "\n",
      " [[0.7303009 ]]\n",
      "\n",
      " [[0.8305974 ]]\n",
      "\n",
      " [[0.6809103 ]]\n",
      "\n",
      " [[0.93557715]]\n",
      "\n",
      " [[0.2756178 ]]\n",
      "\n",
      " [[0.55552936]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.95364356]]\n",
      "\n",
      " [[0.31119615]]\n",
      "\n",
      " [[0.29442245]]\n",
      "\n",
      " [[0.00582019]]\n",
      "\n",
      " [[0.40635252]]\n",
      "\n",
      " [[0.8249471 ]]\n",
      "\n",
      " ...981947]]\n",
      "\n",
      " [[0.7303009 ]]\n",
      "\n",
      " [[0.8305974 ]]\n",
      "\n",
      " [[0.6809103 ]]\n",
      "\n",
      " [[0.93557715]]\n",
      "\n",
      " [[0.2756178 ]]\n",
      "\n",
      " [[0.55552936]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58730c90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.95364356]]\n",
      "\n",
      " [[0.31119615]]\n",
      "\n",
      " [[0.29442245]]\n",
      "\n",
      " [[0.00582019]]\n",
      "\n",
      " [[0.40635252]]\n",
      "\n",
      " [[0.8249471 ]]\n",
      "\n",
      " [[0.79981947]]\n",
      "\n",
      " [[0.7303009 ]]\n",
      "\n",
      " [[0.8305974 ]]\n",
      "\n",
      " [[0.6809103 ]]\n",
      "\n",
      " [[0.93557715]]\n",
      "\n",
      " [[0.2756178 ]]\n",
      "\n",
      " [[0.55552936]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.95364356]])\n",
      "        inputs     = [needle.Tensor([[0.95364356]]), needle.Tensor([[0.31119615]]), needle.Tensor([[0.29442245]]), needle.Tensor([[0.00582019]]), needle.Tensor([[0.40635252]]), needle.Tensor([[0.8249471]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58731e10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58730c90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.95364356]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58731e10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.95364356]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.1432189  -0.09056491 -0.28253937  0.13917273  0.15938494 -0.08297607\n",
      "  -0.21802187  0.10006556  0.02086979  0.23347169  0.17145792  0.20983791]])\n",
      "        bias_ih    = needle.Tensor([[-0.13965762  0.25020534  0.06547746  0.13977545  0.18390235  0.26641768\n",
      "  -0.2468932  -0.2625882   0.19725806  0.01912621 -0.14343606  0.04498962]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58731e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]])\n",
      "        self       = needle.Tensor([[0.95364356]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.95364356]]), needle.Tensor([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.95364356]]), needle.Tensor([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58850>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5696a770>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56c5ab50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5696b1b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd56c5ab50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.95364356]], device=cpu())\n",
      "        b          = NDArray([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56c58850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.95364356]], device=cpu())\n",
      "        b          = NDArray([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.95364356]], device=cpu())\n",
      "other = NDArray([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58732af0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58732ff0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd582c2970>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.16837028 -0.01255947 -0.17596418 -0.117236   -0.15784602  0.1319921\n",
      "   0.11565629  0.13301623 -0.16587994  0.1729039   0.13123822  0.01823559]], device=cuda())\n",
      "out        = NDArray([[ 4.2123252e-34  0.0000000e+00  8.8908423e-35  0.0000000e+00\n",
      "  -6.5727085e-01 -1.3530481e-01 -1.0524237e-01 -5.3471643e-01\n",
      "   4.3867093e-01  5.9546047e-01 -4.7715247e-01  7.0699769e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.95364356]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.4927492   1.0738329  -0.5847047   2.3507056  -0.2914285\n",
      "   -1.5120255   0.12093759  0.212005    0....48   0.2852864   0.7564696\n",
      "    0.67528206 -0.6105782  -0.5171637   1.6805729  -1.8133804\n",
      "   -0.13153002 -1.4402889 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-1.693517    1.0558939   1.499117    0.92392313  0.5884734\n",
      "   -2.6618416  -0.67038393  0.979228   -0....91  -0.05240346  0.6208196\n",
      "    0.4301835   1.9835514   0.050872   -0.84541184 -1.6577004\n",
      "    1.4181603  -0.40932828]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5608a3d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[346.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[346.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5608a3d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5608a3d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[346.]])\n",
      "        x_emb      = needle.Tensor([[[0.45388496]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.45388496]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56088950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.45388496]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.45388496]])\n",
      "        inputs     = [needle.Tensor([[0.45388496]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089e50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56088950>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.45388496]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.45388496]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.11275917 -0.09381337  0.00211972 -0.17753914  0.23762757 -0.28548005\n",
      "   0.22710276 -0.13236693  0.27753282  0.25523782  0.12753016  0.2592799 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.15752995  0.19486395  0.01359591  0.03081438  0.13766399  0.2106688\n",
      "   0.12691963 -0.20742261 -0.23673543  0.21993709 -0.21477777 -0.03517053]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56089e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]])\n",
      "        self       = needle.Tensor([[0.45388496]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.45388496]]), needle.Tensor([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58819e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.45388496]]), needle.Tensor([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58819e90>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bf96f0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd5881be50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd55bfa630>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd5881be50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.45388496]], device=cpu())\n",
      "        b          = NDArray([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58819e90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.45388496]], device=cpu())\n",
      "        b          = NDArray([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.45388496]], device=cpu())\n",
      "other = NDArray([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5881a5f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd572b31b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58818830>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.04812986  0.13862959 -0.2863622  -0.09085059  0.00974396  0.2821564\n",
      "   0.20663482  0.10980645 -0.08542915 -0.25016513 -0.00310004  0.02716467]], device=cuda())\n",
      "out        = NDArray([[8.0686024e-35 0.0000000e+00 1.6593003e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.45388496]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.37422645 -2.1027002   1.5578986   1.4829252  -0.5746417\n",
      "   -0.16645075 -1.2293239   0.41871655  0....65   0.9428025  -1.0796804\n",
      "   -0.5668289  -0.65678835 -0.13883242 -0.8099121  -2.0361485\n",
      "   -0.27928758  1.1186019 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.63687295  0.62343216 -0.03808499  0.02250149 -0.07469893\n",
      "   -0.95130837 -0.39611277 -0.73823464 -0...54 -2.0208943   1.4049278\n",
      "   -0.20516728  1.0575551  -0.07090794  0.8115138   0.28139722\n",
      "    1.0086657  -0.08092995]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58191b10>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[345.],\n",
      "       [755.],\n",
      "       [634.],\n",
      "       [179.],\n",
      "       [  9.],\n",
      "       [892.],\n",
      "       [325.],\n",
      "       [349.],\n",
      "       [970.],\n",
      "       [449.],\n",
      "       [206.],\n",
      "       [727.],\n",
      "       [156.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[345.]\n",
      " [755.]\n",
      " [634.]\n",
      " [179.]\n",
      " [  9.]\n",
      " [892.]\n",
      " [325.]\n",
      " [349.]\n",
      " [970.]\n",
      " [449.]\n",
      " [206.]\n",
      " [727.]\n",
      " [156.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58191b10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58191b10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[345.]\n",
      " [755.]\n",
      " [634.]\n",
      " [179.]\n",
      " [  9.]\n",
      " [892.]\n",
      " [325.]\n",
      " [349.]\n",
      " [970.]\n",
      " [449.]\n",
      " [206.]\n",
      " [727.]\n",
      " [156.]])\n",
      "        x_emb      = needle.Tensor([[[0.21841441]]\n",
      "\n",
      " [[0.39660332]]\n",
      "\n",
      " [[0.18399093]]\n",
      "\n",
      " [[0.2538044 ]]\n",
      "\n",
      " [[0.9388862 ]]\n",
      "\n",
      " [[0.9004953 ]]\n",
      "\n",
      " [[0.6410169 ]]\n",
      "\n",
      " [[0.7978644 ]]\n",
      "\n",
      " [[0.6963012 ]]\n",
      "\n",
      " [[0.7800451 ]]\n",
      "\n",
      " [[0.34736884]]\n",
      "\n",
      " [[0.8124861 ]]\n",
      "\n",
      " [[0.83043534]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.21841441]]\n",
      "\n",
      " [[0.39660332]]\n",
      "\n",
      " [[0.18399093]]\n",
      "\n",
      " [[0.2538044 ]]\n",
      "\n",
      " [[0.9388862 ]]\n",
      "\n",
      " [[0.9004953 ]]\n",
      "\n",
      " ...10169 ]]\n",
      "\n",
      " [[0.7978644 ]]\n",
      "\n",
      " [[0.6963012 ]]\n",
      "\n",
      " [[0.7800451 ]]\n",
      "\n",
      " [[0.34736884]]\n",
      "\n",
      " [[0.8124861 ]]\n",
      "\n",
      " [[0.83043534]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd581920d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.21841441]]\n",
      "\n",
      " [[0.39660332]]\n",
      "\n",
      " [[0.18399093]]\n",
      "\n",
      " [[0.2538044 ]]\n",
      "\n",
      " [[0.9388862 ]]\n",
      "\n",
      " [[0.9004953 ]]\n",
      "\n",
      " [[0.6410169 ]]\n",
      "\n",
      " [[0.7978644 ]]\n",
      "\n",
      " [[0.6963012 ]]\n",
      "\n",
      " [[0.7800451 ]]\n",
      "\n",
      " [[0.34736884]]\n",
      "\n",
      " [[0.8124861 ]]\n",
      "\n",
      " [[0.83043534]]])\n",
      "        _          = 1\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.21841441]])\n",
      "        inputs     = [needle.Tensor([[0.21841441]]), needle.Tensor([[0.39660332]]), needle.Tensor([[0.18399093]]), needle.Tensor([[0.2538044]]), needle.Tensor([[0.9388862]]), needle.Tensor([[0.9004953]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58191c50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd581920d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.21841441]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58191c50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.21841441]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.2875798  -0.16726139 -0.18010014  0.17653498  0.01685232  0.19817388\n",
      "  -0.2884908   0.02565828  0.05335024 -0.26363096  0.20675534 -0.2646805 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.00600639  0.10258552 -0.20301615  0.21548027  0.04904389 -0.10459961\n",
      "  -0.12612303 -0.08700752  0.2271058  -0.06298673 -0.25595474  0.00480112]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58191c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]])\n",
      "        self       = needle.Tensor([[0.21841441]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.21841441]]), needle.Tensor([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58117e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.21841441]]), needle.Tensor([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58117e50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd58259470>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58115690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd582586b0>, 1, 1, 12') raised in repr()] Tensor object at 0x7fbd58115690>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.21841441]], device=cpu())\n",
      "        b          = NDArray([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58117e50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.21841441]], device=cpu())\n",
      "        b          = NDArray([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.21841441]], device=cpu())\n",
      "other = NDArray([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd581919b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd581927b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58114e70>, 1, 1, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.08860561 -0.08464819  0.04091012  0.27081704  0.16372451 -0.09664825\n",
      "   0.22826046  0.266473    0.13946235 -0.14839648 -0.04644972 -0.10934825]], device=cuda())\n",
      "out        = NDArray([[2.5841776e-34 0.0000000e+00 1.0585474e-34 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.21841441]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.6529373   0.784479   -1.0365573   1.7040498  -0.45860344\n",
      "    0.7808374   0.5573944   0.736387    1...1  -0.85554415 -0.63016796\n",
      "    0.63188714  0.6756163  -1.9380916   0.7006979  -0.6074646\n",
      "   -0.7132241   0.28125632]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-2.1809816  -2.1443248   0.70290107  0.21448895 -2.0746908\n",
      "   -0.12313911 -0.47080153 -2.070478   -1....51  -0.5812214   0.2837071\n",
      "    0.24319576 -1.074323   -0.06838111  1.7244427   1.2657721\n",
      "    0.0578196  -0.61160886]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58080610>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[ 11., 522., 667., 395., 872., 770., 613., 704.,  46., 407., 133.,\n",
      "        109., 721., 361.,  28.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 11. 522. 667. 395. 872. 770. 613. 704.  46. 407. 133. 109. 721. 361.\n",
      "   28.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58080610>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58080610>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[ 11. 522. 667. 395. 872. 770. 613. 704.  46. 407. 133. 109. 721. 361.\n",
      "   28.]])\n",
      "        x_emb      = needle.Tensor([[[0.56848377]\n",
      "  [0.25079224]\n",
      "  [0.08997117]\n",
      "  [0.6898776 ]\n",
      "  [0.19813699]\n",
      "  [0.80763274]\n",
      "  [0.13928409]...0.75501955]\n",
      "  [0.96482915]\n",
      "  [0.41716006]\n",
      "  [0.35711232]\n",
      "  [0.34594694]\n",
      "  [0.8895979 ]\n",
      "  [0.30194327]\n",
      "  [0.82452935]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.56848377]\n",
      "  [0.25079224]\n",
      "  [0.08997117]\n",
      "  [0.6898776 ]\n",
      "  [0.19813699]\n",
      "  [0.80763274]\n",
      "  [0.13928409...955]\n",
      "  [0.96482915]\n",
      "  [0.41716006]\n",
      "  [0.35711232]\n",
      "  [0.34594694]\n",
      "  [0.8895979 ]\n",
      "  [0.30194327]\n",
      "  [0.82452935]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58080ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.56848377]\n",
      "  [0.25079224]\n",
      "  [0.08997117]\n",
      "  [0.6898776 ]\n",
      "  [0.19813699]\n",
      "  [0.80763274]\n",
      "  [0.13928409]...0.75501955]\n",
      "  [0.96482915]\n",
      "  [0.41716006]\n",
      "  [0.35711232]\n",
      "  [0.34594694]\n",
      "  [0.8895979 ]\n",
      "  [0.30194327]\n",
      "  [0.82452935]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]])\n",
      "        inputs     = [needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e7dd10>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58080ed0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.7... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e7dd10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.171518   -0.21135819  0.09138951  0.1520451  -0.2393161  -0.22260228\n",
      "  -0.2874662  -0.20018461 -0.2...138951  0.1520451  -0.2393161  -0.22260228\n",
      "  -0.2874662  -0.20018461 -0.23245856  0.15996066  0.03781438 -0.2575012 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.27171245 -0.00667885  0.25894308  0.2094535   0.09251276  0.0257405\n",
      "  -0.10815576  0.14269695  0.21...5894308  0.2094535   0.09251276  0.0257405\n",
      "  -0.10815576  0.14269695  0.21509981  0.26659358 -0.0113318   0.27875406]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57e7dd10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.15006602  0.20056415  0.11499402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]])\n",
      "        self       = needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.7...99402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e7f410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.7...99402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e7f410>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58270570>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584b2d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58561cb0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584b2d90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]], device=cpu())\n",
      "        b          = NDArray([[-0.15006602  0.20056415  0.11499402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e7f410>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]], device=cpu())\n",
      "        b          = NDArray([[-0.15006602  0.20056415  0.11499402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]], device=cpu())\n",
      "other = NDArray([[-0.15006602  0.20056415  0.11499402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57e7db70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57e7c2f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b06b0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.15006602  0.20056415  0.11499402 -0.13675456 -0.25277862  0.22920728\n",
      "  -0.19741017  0.20386797  0.13878104  0.15326607  0.10021451  0.12903073]], device=cuda())\n",
      "out        = NDArray([[1.21405808e-38 0.00000000e+00 1.21096850e-38 0.00000000e+00\n",
      "  1.21096850e-38 0.00000000e+00 1.21405808e-38 0...000000e+00 1.21405808e-38 0.00000000e+00\n",
      "  1.05274158e+15 4.58238611e-41 1.21405808e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.56848377]\n",
      " [0.25079224]\n",
      " [0.08997117]\n",
      " [0.6898776 ]\n",
      " [0.19813699]\n",
      " [0.80763274]\n",
      " [0.13928409]\n",
      " [0.75501955]\n",
      " [0.96482915]\n",
      " [0.41716006]\n",
      " [0.35711232]\n",
      " [0.34594694]\n",
      " [0.8895979 ]\n",
      " [0.30194327]\n",
      " [0.82452935]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.9657911   0.4343072   0.6634394   1.041693    0.1594316\n",
      "   -0.23861064 -0.13604018  0.19335435  1....47   0.15441547  0.2991522\n",
      "    0.16499965  1.1235275  -0.36847025 -0.88383406  1.2387148\n",
      "   -0.23988482 -0.98669636]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[-0.9265298  -1.4885046  -0.7659133   0.91792053 -0.9787618\n",
      "    0.8025577   0.6954594  -0.9803014   0....   0.0442691   0.19802223\n",
      "   -2.9541178  -0.5611552  -1.0720967  -0.64659035  0.01296541\n",
      "    1.2821381  -0.47267523]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56326810>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[841.,  37., 935., 669., 159., 977., 925., 280., 629., 839., 799.,\n",
      "        420., 157., 892.,  27.],\n",
      "       [318...    [699., 786., 305., 743., 286., 153., 236., 380., 814., 947., 146.,\n",
      "        928.,  81., 897., 685.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[841.  37. 935. 669. 159. 977. 925. 280. 629. 839. 799. 420. 157. 892.\n",
      "   27.]\n",
      " [318. 991.  79.  19. 6...9. 114. 218. 492. 843.\n",
      "  381.]\n",
      " [699. 786. 305. 743. 286. 153. 236. 380. 814. 947. 146. 928.  81. 897.\n",
      "  685.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56326810>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56326810>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[841.  37. 935. 669. 159. 977. 925. 280. 629. 839. 799. 420. 157. 892.\n",
      "   27.]\n",
      " [318. 991.  79.  19. 64...408. 239. 114. 218. 492. 843.\n",
      "  381.]\n",
      " [699. 786. 305. 743. 286. 153. 236. 380. 814. 947. 146. 928.  81. 897.\n",
      "  685.]])\n",
      "        x_emb      = needle.Tensor([[[0.7091008 ]\n",
      "  [0.9165898 ]\n",
      "  [0.05104895]\n",
      "  [0.57645077]\n",
      "  [0.8155708 ]\n",
      "  [0.9185634 ]\n",
      "  [0.24644805]...0.58133996]\n",
      "  [0.7235039 ]\n",
      "  [0.74278533]\n",
      "  [0.4492805 ]\n",
      "  [0.12556371]\n",
      "  [0.11065772]\n",
      "  [0.13557692]\n",
      "  [0.69656485]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7091008 ]\n",
      "  [0.9165898 ]\n",
      "  [0.05104895]\n",
      "  [0.57645077]\n",
      "  [0.8155708 ]\n",
      "  [0.9185634 ]\n",
      "  [0.24644805...996]\n",
      "  [0.7235039 ]\n",
      "  [0.74278533]\n",
      "  [0.4492805 ]\n",
      "  [0.12556371]\n",
      "  [0.11065772]\n",
      "  [0.13557692]\n",
      "  [0.69656485]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56327290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.7091008 ]\n",
      "  [0.9165898 ]\n",
      "  [0.05104895]\n",
      "  [0.57645077]\n",
      "  [0.8155708 ]\n",
      "  [0.9185634 ]\n",
      "  [0.24644805]...0.58133996]\n",
      "  [0.7235039 ]\n",
      "  [0.74278533]\n",
      "  [0.4492805 ]\n",
      "  [0.12556371]\n",
      "  [0.11065772]\n",
      "  [0.13557692]\n",
      "  [0.69656485]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]])\n",
      "        inputs     = [needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.0... [0.7284366 ]\n",
      " [0.50605005]\n",
      " [0.46074632]\n",
      " [0.12464966]\n",
      " [0.08439182]\n",
      " [0.37090498]\n",
      " [0.21619873]\n",
      " [0.15380207]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563267d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56327290>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.0... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563267d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.01104146  0.20992824  0.25345403  0.03282139  0.2635724  -0.25888264\n",
      "   0.04203391 -0.10051203  0.1...345403  0.03282139  0.2635724  -0.25888264\n",
      "   0.04203391 -0.10051203  0.19344807 -0.11399111 -0.00472987 -0.283494  ]])\n",
      "        bias_ih    = needle.Tensor([[-0.0256221  -0.06923848  0.28080767 -0.11209202 -0.10064204 -0.2825508\n",
      "  -0.02205497 -0.08380894 -0.23...8080767 -0.11209202 -0.10064204 -0.2825508\n",
      "  -0.02205497 -0.08380894 -0.23510157 -0.09696993  0.11670882 -0.0422754 ]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd563267d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.27656853 -0.02434269  0.05300307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]])\n",
      "        self       = needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.0...00307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd582c0810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.0...00307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd582c0810>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58435070>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd582c3150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584372f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd582c3150>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]], device=cpu())\n",
      "        b          = NDArray([[ 0.27656853 -0.02434269  0.05300307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd582c0810>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]], device=cpu())\n",
      "        b          = NDArray([[ 0.27656853 -0.02434269  0.05300307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]], device=cpu())\n",
      "other = NDArray([[ 0.27656853 -0.02434269  0.05300307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56327670>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56325930>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd582c2d30>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.27656853 -0.02434269  0.05300307 -0.28233013 -0.20165804  0.07358751\n",
      "  -0.15219855 -0.03596696 -0.14677364  0.09405145 -0.08312143 -0.0779258 ]], device=cuda())\n",
      "out        = NDArray([[1.2140581e-38 0.0000000e+00 1.2109685e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2140581e-38 0.000000...\n",
      "  0.0000000e+00 1.2140581e-38 0.0000000e+00 4.1950556e+14 4.5823861e-41\n",
      "  1.2140581e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.7091008 ]\n",
      " [0.9165898 ]\n",
      " [0.05104895]\n",
      " [0.57645077]\n",
      " [0.8155708 ]\n",
      " [0.9185634 ]\n",
      " [0.24644805]\n",
      " [0.03694064]\n",
      " [0.07692453]\n",
      " [0.9340859 ]\n",
      " [0.48605964]\n",
      " [0.05511854]\n",
      " [0.13097347]\n",
      " [0.8206021 ]\n",
      " [0.15058847]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 8.41249049e-01  1.30034730e-01  1.84593320e+00 -1.30570218e-01\n",
      "   -4.16284621e-01 -7.97761083e-01  3...  1.43867284e-01 -6.28248677e-02 -6.76072598e-01\n",
      "   -4.40811031e-02 -5.85437473e-03  8.32548916e-01 -9.87846136e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 7.31395483e-01 -1.09852254e+00  4.46003489e-02  5.31286061e-01\n",
      "   -4.95305389e-01  1.53432816e-01 -2... -1.25884688e+00  1.33263314e+00  1.81643093e+00\n",
      "    9.93318737e-01 -2.40002441e+00  1.18105328e+00  8.92512918e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd55a26110>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[801., 480.,  78., 786., 413., 460., 783.,  61., 158., 499., 458.,\n",
      "        701., 266., 516., 265.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[801. 480.  78. 786. 413. 460. 783.  61. 158. 499. 458. 701. 266. 516.\n",
      "  265.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a26110>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a26110>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[801. 480.  78. 786. 413. 460. 783.  61. 158. 499. 458. 701. 266. 516.\n",
      "  265.]])\n",
      "        x_emb      = needle.Tensor([[[0.4463476 ]\n",
      "  [0.5747412 ]\n",
      "  [0.7498288 ]\n",
      "  [0.1729072 ]\n",
      "  [0.6079759 ]\n",
      "  [0.0559682 ]\n",
      "  [0.7965205 ]...0.77786356]\n",
      "  [0.16973595]\n",
      "  [0.73852533]\n",
      "  [0.79920655]\n",
      "  [0.7346389 ]\n",
      "  [0.30771324]\n",
      "  [0.9519826 ]\n",
      "  [0.17759407]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.4463476 ]\n",
      "  [0.5747412 ]\n",
      "  [0.7498288 ]\n",
      "  [0.1729072 ]\n",
      "  [0.6079759 ]\n",
      "  [0.0559682 ]\n",
      "  [0.7965205 ...356]\n",
      "  [0.16973595]\n",
      "  [0.73852533]\n",
      "  [0.79920655]\n",
      "  [0.7346389 ]\n",
      "  [0.30771324]\n",
      "  [0.9519826 ]\n",
      "  [0.17759407]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a25590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.4463476 ]\n",
      "  [0.5747412 ]\n",
      "  [0.7498288 ]\n",
      "  [0.1729072 ]\n",
      "  [0.6079759 ]\n",
      "  [0.0559682 ]\n",
      "  [0.7965205 ]...0.77786356]\n",
      "  [0.16973595]\n",
      "  [0.73852533]\n",
      "  [0.79920655]\n",
      "  [0.7346389 ]\n",
      "  [0.30771324]\n",
      "  [0.9519826 ]\n",
      "  [0.17759407]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]])\n",
      "        inputs     = [needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a25f50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd55a25590>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.7... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a25f50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.01310688 -0.20978421 -0.02339852 -0.00323275  0.12148759  0.1264962\n",
      "  -0.07109837 -0.13103461 -0.17...2339852 -0.00323275  0.12148759  0.1264962\n",
      "  -0.07109837 -0.13103461 -0.17555988  0.25923538  0.13237613  0.18908998]])\n",
      "        bias_ih    = needle.Tensor([[ 0.11972472 -0.16826281  0.28113455 -0.2771013   0.17404091 -0.10844497\n",
      "   0.25755668 -0.04412985  0.0...113455 -0.2771013   0.17404091 -0.10844497\n",
      "   0.25755668 -0.04412985  0.00452712 -0.11519852 -0.15309519  0.23163474]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd55a25f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.14225198 -0.26958323  0.26033783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]])\n",
      "        self       = needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.7...33783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b1a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.7...33783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b1a10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ab78f0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584b2850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd57ab5270>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd584b2850>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]], device=cpu())\n",
      "        b          = NDArray([[-0.14225198 -0.26958323  0.26033783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd584b1a10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]], device=cpu())\n",
      "        b          = NDArray([[-0.14225198 -0.26958323  0.26033783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]], device=cpu())\n",
      "other = NDArray([[-0.14225198 -0.26958323  0.26033783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a26d30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55a245b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584b13b0>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.14225198 -0.26958323  0.26033783  0.2572325  -0.06109346  0.16743213\n",
      "  -0.15053044  0.22654516  0.04391289 -0.26487535  0.15299195  0.04663873]], device=cuda())\n",
      "out        = NDArray([[8.4313251e-35 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.4463476 ]\n",
      " [0.5747412 ]\n",
      " [0.7498288 ]\n",
      " [0.1729072 ]\n",
      " [0.6079759 ]\n",
      " [0.0559682 ]\n",
      " [0.7965205 ]\n",
      " [0.77786356]\n",
      " [0.16973595]\n",
      " [0.73852533]\n",
      " [0.79920655]\n",
      " [0.7346389 ]\n",
      " [0.30771324]\n",
      " [0.9519826 ]\n",
      " [0.17759407]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.9857164  -1.4072149  -1.3245409  -0.31455618 -0.81979847\n",
      "    0.9915193  -0.21089609  0.06625701 -1...706 -0.643148    2.1999104\n",
      "    1.1869496  -2.2887297  -0.23545507  0.75721514 -0.3946516\n",
      "    0.40011615  0.50540686]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h0         = needle.Tensor([[[ 0.04161346 -0.44636875 -0.8084623  -0.59341174  0.71132475\n",
      "   -2.0629437   0.03117322  1.7869121  -1...  -0.43791315  0.27802244\n",
      "   -0.18152861 -0.18496019 -0.6541486   0.27615142  0.54808825\n",
      "    0.62673837  0.9674578 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57640510>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[943., 549., 798., 352., 790., 496., 456., 136., 714., 757.,  96.,\n",
      "         68., 353., 353., 770.],\n",
      "       [952...    [338., 464., 255., 825.,  99., 455.,   0., 870., 903., 884., 656.,\n",
      "        202., 907., 107., 545.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[943. 549. 798. 352. 790. 496. 456. 136. 714. 757.  96.  68. 353. 353.\n",
      "  770.]\n",
      " [952. 130. 274.  79. 1...2. 125. 900. 778. 938.\n",
      "  247.]\n",
      " [338. 464. 255. 825.  99. 455.   0. 870. 903. 884. 656. 202. 907. 107.\n",
      "  545.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57640510>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57640510>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[943. 549. 798. 352. 790. 496. 456. 136. 714. 757.  96.  68. 353. 353.\n",
      "  770.]\n",
      " [952. 130. 274.  79. 16...354. 282. 125. 900. 778. 938.\n",
      "  247.]\n",
      " [338. 464. 255. 825.  99. 455.   0. 870. 903. 884. 656. 202. 907. 107.\n",
      "  545.]])\n",
      "        x_emb      = needle.Tensor([[[0.5164573 ]\n",
      "  [0.01369656]\n",
      "  [0.37937513]\n",
      "  [0.44868097]\n",
      "  [0.47281215]\n",
      "  [0.8075077 ]\n",
      "  [0.11607508]...0.54515314]\n",
      "  [0.00468773]\n",
      "  [0.8242474 ]\n",
      "  [0.20564833]\n",
      "  [0.5421426 ]\n",
      "  [0.8674557 ]\n",
      "  [0.7214115 ]\n",
      "  [0.06594747]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5164573 ]\n",
      "  [0.01369656]\n",
      "  [0.37937513]\n",
      "  [0.44868097]\n",
      "  [0.47281215]\n",
      "  [0.8075077 ]\n",
      "  [0.11607508...314]\n",
      "  [0.00468773]\n",
      "  [0.8242474 ]\n",
      "  [0.20564833]\n",
      "  [0.5421426 ]\n",
      "  [0.8674557 ]\n",
      "  [0.7214115 ]\n",
      "  [0.06594747]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57643510>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5164573 ]\n",
      "  [0.01369656]\n",
      "  [0.37937513]\n",
      "  [0.44868097]\n",
      "  [0.47281215]\n",
      "  [0.8075077 ]\n",
      "  [0.11607508]...0.54515314]\n",
      "  [0.00468773]\n",
      "  [0.8242474 ]\n",
      "  [0.20564833]\n",
      "  [0.5421426 ]\n",
      "  [0.8674557 ]\n",
      "  [0.7214115 ]\n",
      "  [0.06594747]]])\n",
      "        _          = 1\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]])\n",
      "        inputs     = [needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7... [0.71771747]\n",
      " [0.40845072]\n",
      " [0.15893441]\n",
      " [0.7643234 ]\n",
      " [0.37078518]\n",
      " [0.6838623 ]\n",
      " [0.5088763 ]\n",
      " [0.79046446]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57643790>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57643510>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57643790>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.12802747 -0.17783493  0.16444016 -0.09585494  0.08589992  0.12917447\n",
      "   0.11306253 -0.24207667  0.1...444016 -0.09585494  0.08589992  0.12917447\n",
      "   0.11306253 -0.24207667  0.16764829 -0.19579583  0.20651594  0.18622085]])\n",
      "        bias_ih    = needle.Tensor([[-0.1799924  -0.04268326 -0.15196481  0.1405431  -0.0991589   0.2604863\n",
      "  -0.28272492 -0.08371884 -0.07...5196481  0.1405431  -0.0991589   0.2604863\n",
      "  -0.28272492 -0.08371884 -0.07871357  0.1633397   0.14888981  0.21224374]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57643790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.01192898  0.19526997  0.04805416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]])\n",
      "        self       = needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7...05416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7...05416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9710>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b60b0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd57eda410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd571b5bb0>, 15, 1, 12') raised in repr()] Tensor object at 0x7fbd57eda410>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]], device=cpu())\n",
      "        b          = NDArray([[-0.01192898  0.19526997  0.04805416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57ed9710>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]], device=cpu())\n",
      "        b          = NDArray([[-0.01192898  0.19526997  0.04805416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]], device=cpu())\n",
      "other = NDArray([[-0.01192898  0.19526997  0.04805416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58503230>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd576403b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57edae30>, 15, 1, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.01192898  0.19526997  0.04805416  0.12775037  0.23261595  0.00716364\n",
      "  -0.23713417  0.0173763   0.0338158   0.02389812 -0.19687222  0.1131956 ]], device=cuda())\n",
      "out        = NDArray([[1.2109685e-38 0.0000000e+00 1.2140581e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2109685e-38 0.000000...\n",
      "  0.0000000e+00 1.2450940e-38 0.0000000e+00 1.2468776e-38 0.0000000e+00\n",
      "  1.2450940e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.5164573 ]\n",
      " [0.01369656]\n",
      " [0.37937513]\n",
      " [0.44868097]\n",
      " [0.47281215]\n",
      " [0.8075077 ]\n",
      " [0.11607508]\n",
      " [0.7694798 ]\n",
      " [0.09883767]\n",
      " [0.6187207 ]\n",
      " [0.36593357]\n",
      " [0.65522707]\n",
      " [0.89316106]\n",
      " [0.89316106]\n",
      " [0.73264724]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-2.5702515  -1.1093854  -0.08054388  0.6366082   0.7291181\n",
      "    0.21818805 -0.04308267  0.01194411  1.529407    0.68997693\n",
      "   -0.4365011  -0.6016364 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.5745015  -2.6477354  -1.2730317   0.06559725  0.16291381\n",
      "    1.7840117  -2.2708147   0.3897374  -1.0200932  -0.30743486\n",
      "    0.19204007  0.44711494]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5708b790>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[92.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[92.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5708b790>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5708b790>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[92.]])\n",
      "        x_emb      = needle.Tensor([[[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "   3.1651470e-01 5.4307371e-01 ... 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "   7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "   3.1651470e-01 5.4307371e-01...466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "   7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5708be50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "   3.1651470e-01 5.4307371e-01 ... 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "   7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5....01 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]])\n",
      "        inputs     = [needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5...1 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a6d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5708be50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5...1\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5....01 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.14919986 -0.12549369 -0.10751981  0.05968738 -0.06696093 -0.17335516\n",
      "  -0.14606646 -0.10883027  0.23791605 -0.14124389  0.13951987  0.05591136]])\n",
      "        bias_ih    = needle.Tensor([[-0.26449326 -0.25809044  0.15188062 -0.03169885 -0.14248967  0.08944359\n",
      "  -0.26856345  0.04734856 -0.03689826 -0.00157183  0.19072789  0.01794609]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5708a6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.67535746e-01  2.30480134e-01 -2.76091158e-01  6.32291436e-02\n",
      "   1.67282999e-01  5.28896451e-02 -1.1...02  5.12677133e-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]])\n",
      "        self       = needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5....01 9.4848466e-01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5...2  5.12677133e-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708bf10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5...2  5.12677133e-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708bf10>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58272cb0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5708b610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58273030>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5708b610>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5.696814...01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]], device=cpu())\n",
      "        b          = NDArray([[-2.67535746e-01  2.30480134e-01 -2.76091158e-01  6.32291436e-02\n",
      "   1.67282999e-01  5.28896451e-02 -1.1806684...-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5708bf10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5.696814...01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]], device=cpu())\n",
      "        b          = NDArray([[-2.67535746e-01  2.30480134e-01 -2.76091158e-01  6.32291436e-02\n",
      "   1.67282999e-01  5.28896451e-02 -1.1806684...-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5.696814...01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]], device=cpu())\n",
      "other = NDArray([[-2.67535746e-01  2.30480134e-01 -2.76091158e-01  6.32291436e-02\n",
      "   1.67282999e-01  5.28896451e-02 -1.1806684...-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5708b830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5708a270>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd584c06b0>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-2.67535746e-01  2.30480134e-01 -2.76091158e-01  6.32291436e-02\n",
      "   1.67282999e-01  5.28896451e-02 -1.1806684...-02 -2.52687335e-02 -1.24628246e-01\n",
      "  -2.00429723e-01 -1.91717342e-01 -1.63345560e-01  1.63888246e-01]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 8.9669097e-35 0.0000000e+00 9.1227901e-01\n",
      "  1.7232065e-01 7.9711920e-01 9.6228647e-01 9.1431010e-01 3.5284108e-01\n",
      "  1.0699658e-01 4.3240532e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[5.0303251e-01 5.5226696e-01 9.7321272e-01 8.7624204e-01 3.3693793e-01\n",
      "  3.1651470e-01 5.4307371e-01 5.696814...01 8.4087497e-01 1.7063601e-04 8.8192618e-01\n",
      "  7.0203853e-01 5.8854759e-01 8.0949581e-01 5.1421344e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.2200531  -0.11552986 -0.06380731 -0.21786034  0.30333057\n",
      "   -0.9824154   0.97140414 -0.67362916  0.04640769 -0.9309051\n",
      "    1.0245584   0.88978183]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 1.5349013   0.30363205 -0.64775467 -0.8240963   0.40242866\n",
      "    0.5826656   0.46073237 -0.5074976   0.5949419  -0.4317588\n",
      "   -0.37188005  0.12913622]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd584a49d0>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[728.],\n",
      "       [122.],\n",
      "       [377.],\n",
      "       [381.],\n",
      "       [669.],\n",
      "       [260.],\n",
      "       [558.],\n",
      "       [ 71.],\n",
      "       [935.],\n",
      "       [360.],\n",
      "       [507.],\n",
      "       [580.],\n",
      "       [139.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[728.]\n",
      " [122.]\n",
      " [377.]\n",
      " [381.]\n",
      " [669.]\n",
      " [260.]\n",
      " [558.]\n",
      " [ 71.]\n",
      " [935.]\n",
      " [360.]\n",
      " [507.]\n",
      " [580.]\n",
      " [139.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a49d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd584a49d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[728.]\n",
      " [122.]\n",
      " [377.]\n",
      " [381.]\n",
      " [669.]\n",
      " [260.]\n",
      " [558.]\n",
      " [ 71.]\n",
      " [935.]\n",
      " [360.]\n",
      " [507.]\n",
      " [580.]\n",
      " [139.]])\n",
      "        x_emb      = needle.Tensor([[[1.00154981e-01 2.36809582e-01 9.37628746e-01 2.02225104e-01\n",
      "   4.33614701e-01 4.36327487e-01 8.128396...e-01 7.21458614e-01\n",
      "   6.95243031e-02 5.11457801e-01 3.67895007e-01 3.11272353e-01\n",
      "   1.16703436e-01 5.59511006e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.00154981e-01 2.36809582e-01 9.37628746e-01 2.02225104e-01\n",
      "   4.33614701e-01 4.36327487e-01 8.12839...21458614e-01\n",
      "   6.95243031e-02 5.11457801e-01 3.67895007e-01 3.11272353e-01\n",
      "   1.16703436e-01 5.59511006e-01]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a7e10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[1.00154981e-01 2.36809582e-01 9.37628746e-01 2.02225104e-01\n",
      "   4.33614701e-01 4.36327487e-01 8.128396...e-01 7.21458614e-01\n",
      "   6.95243031e-02 5.11457801e-01 3.67895007e-01 3.11272353e-01\n",
      "   1.16703436e-01 5.59511006e-01]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0....371\n",
      "  0.9340388  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]])\n",
      "        inputs     = [needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0....5359666  0.07450743 0.6439199  0.5090853  0.40823877 0.99178725\n",
      "  0.52768147 0.5908868  0.607239   0.4672134 ]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a64d0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd584a7e10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0...61  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a64d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0....371\n",
      "  0.9340388  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.04623644  0.24769098 -0.21474499 -0.11367053  0.14786586 -0.15363403\n",
      "  -0.01860154 -0.10662083 -0.15676425  0.15915322 -0.12931848 -0.20183243]])\n",
      "        bias_ih    = needle.Tensor([[-0.22148398 -0.01022753 -0.04716933  0.26719862 -0.0228751   0.19907749\n",
      "  -0.00468308 -0.1448974   0.19819677  0.05785885 -0.22339949 -0.13744916]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd584a64d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.04310988  0.1044513  -0.0491966  -0.22807176  0.19253632  0.22839797\n",
      "   0.06404176  0.089304   -0.0...588833  0.13426173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]])\n",
      "        self       = needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0....371\n",
      "  0.9340388  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0...88833  0.13426173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55c23090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0...88833  0.13426173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55c23090>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56f2f1b0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd55c20b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd56f2d670>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd55c20b90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0.465639...8  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]], device=cpu())\n",
      "        b          = NDArray([[-0.04310988  0.1044513  -0.0491966  -0.22807176  0.19253632  0.22839797\n",
      "   0.06404176  0.089304   -0.0128570...173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55c23090>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0.465639...8  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]], device=cpu())\n",
      "        b          = NDArray([[-0.04310988  0.1044513  -0.0491966  -0.22807176  0.19253632  0.22839797\n",
      "   0.06404176  0.089304   -0.0128570...173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0.465639...8  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]], device=cpu())\n",
      "other = NDArray([[-0.04310988  0.1044513  -0.0491966  -0.22807176  0.19253632  0.22839797\n",
      "   0.06404176  0.089304   -0.0128570...173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56056c70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd584a5e30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55c23930>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.04310988  0.1044513  -0.0491966  -0.22807176  0.19253632  0.22839797\n",
      "   0.06404176  0.089304   -0.0128570...173 -0.08151425 -0.14182077\n",
      "   0.09455818  0.14872816 -0.28327113  0.10976568 -0.26921728 -0.17729685]], device=cuda())\n",
      "out        = NDArray([[9.4223495e-35 0.0000000e+00 9.4222484e-35 0.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.10015498 0.23680958 0.93762875 0.2022251  0.4336147  0.4363275\n",
      "  0.8128396  0.14302176 0.02834754 0.465639...8  0.3733388  0.7438388  0.07458409 0.9596961  0.3487426\n",
      "  0.63556737 0.4795327  0.37785766 0.93393975]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-4.7961646e-01  5.4387057e-01 -2.4960670e-01 -9.3406951e-01\n",
      "    1.0530591e-02 -8.2402486e-01  3.64342...150e-01 -1.8664579e+00 -6.6215825e-01  9.2733407e-01\n",
      "   -2.1984471e-02  2.5198099e-01  9.8829043e-01 -1.5979717e-03]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[ 0.99896085 -1.012679    2.2920346   1.1338369  -0.5119552\n",
      "    0.26965064 -1.3815185   2.037746    0....25 -0.03427726  0.0971919\n",
      "    1.1841774   2.02316     0.38383517  0.67897224 -0.47902957\n",
      "    0.36378044  0.44175833]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd58695d50>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[997.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[997.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd58695d50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd58695d50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[997.]])\n",
      "        x_emb      = needle.Tensor([[[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "   0.88433117 0.07248611 0.7811833 ...\n",
      "   0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "   0.64228356 0.70771176 0.76221895 0.42488134]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "   0.88433117 0.07248611 0.7811833...586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "   0.64228356 0.70771176 0.76221895 0.42488134]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58696310>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "   0.88433117 0.07248611 0.7811833 ...\n",
      "   0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "   0.64228356 0.70771176 0.76221895 0.42488134]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0...006\n",
      "  0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]])\n",
      "        inputs     = [needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  ...06\n",
      "  0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58695e50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd58696310>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  ...213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58695e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0...006\n",
      "  0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.161985    0.05341494  0.24218094  0.15821326  0.25403517 -0.22518283\n",
      "   0.14116812  0.1874663   0.11755306 -0.23551375 -0.08522694  0.2652697 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.11445948  0.26107818  0.08654043 -0.28353003  0.14753875 -0.24936847\n",
      "   0.14508158  0.06696764 -0.02783033 -0.13254566 -0.22777411 -0.17704663]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd58695e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.170317    0.03879997  0.03441069  0.13120964 -0.2552169   0.04786175\n",
      "  -0.22723942 -0.21060371 -0.2...2843305 -0.27498788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]])\n",
      "        self       = needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0...006\n",
      "  0.6586461  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  ...843305 -0.27498788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58488090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  ...843305 -0.27498788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58488090>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd58073530>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd584891d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd580814f0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd584891d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0.41290...1  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]], device=cpu())\n",
      "        b          = NDArray([[-0.170317    0.03879997  0.03441069  0.13120964 -0.2552169   0.04786175\n",
      "  -0.22723942 -0.21060371 -0.2126188...8788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58488090>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0.41290...1  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]], device=cpu())\n",
      "        b          = NDArray([[-0.170317    0.03879997  0.03441069  0.13120964 -0.2552169   0.04786175\n",
      "  -0.22723942 -0.21060371 -0.2126188...8788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0.41290...1  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]], device=cpu())\n",
      "other = NDArray([[-0.170317    0.03879997  0.03441069  0.13120964 -0.2552169   0.04786175\n",
      "  -0.22723942 -0.21060371 -0.2126188...8788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5848b3b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd58694d30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58489b30>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.170317    0.03879997  0.03441069  0.13120964 -0.2552169   0.04786175\n",
      "  -0.22723942 -0.21060371 -0.2126188...8788  0.02417821  0.2617216\n",
      "   0.08465576 -0.17653435 -0.09789409  0.13017723  0.2751509  -0.03060502]], device=cuda())\n",
      "out        = NDArray([[ 8.1775126e+35  4.5823861e-41  5.7463968e-34  0.0000000e+00\n",
      "  -4.4787395e-01 -6.0556531e-02  4.0392548e-01  1.4879751e-01\n",
      "   7.0536315e-02 -2.2423795e-01  6.3689172e-02 -6.3659197e-01]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.42354122 0.13665256 0.0648214  0.10510194 0.02292803 0.34210604\n",
      "  0.88433117 0.07248611 0.7811833  0.41290...1  0.47309333 0.7171715  0.7233271  0.17395213 0.6708723\n",
      "  0.64228356 0.70771176 0.76221895 0.42488134]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-0.45892107 -1.1581941   2.2567523  -0.18175684 -0.70572084\n",
      "   -0.40437225  0.2652279   0.63516116 -1...25  -1.22183    -0.5526288\n",
      "    0.36139953  0.29621884  0.74512315  0.70947975 -0.5617089\n",
      "    0.42183658  1.0937697 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.84063065 -1.4891317   2.1152456  -0.4042638   1.5414956\n",
      "    1.0798551  -0.42928034 -0.5202508  -0....   0.6390137   0.20792693\n",
      "    1.1253275   0.754625    0.6451527   0.6128646  -0.75303525\n",
      "    1.9384854   0.9849163 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd57190510>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[649.],\n",
      "       [727.],\n",
      "       [711.],\n",
      "       [952.],\n",
      "       [732.],\n",
      "       [550.],\n",
      "       [417.],\n",
      "       [189.],\n",
      "       [712.],\n",
      "       [ 88.],\n",
      "       [879.],\n",
      "       [631.],\n",
      "       [474.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[649.]\n",
      " [727.]\n",
      " [711.]\n",
      " [952.]\n",
      " [732.]\n",
      " [550.]\n",
      " [417.]\n",
      " [189.]\n",
      " [712.]\n",
      " [ 88.]\n",
      " [879.]\n",
      " [631.]\n",
      " [474.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57190510>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd57190510>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[649.]\n",
      " [727.]\n",
      " [711.]\n",
      " [952.]\n",
      " [732.]\n",
      " [550.]\n",
      " [417.]\n",
      " [189.]\n",
      " [712.]\n",
      " [ 88.]\n",
      " [879.]\n",
      " [631.]\n",
      " [474.]])\n",
      "        x_emb      = needle.Tensor([[[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "   0.4762388  0.69575906 0.5130842 ...   0.19727653 0.11099589 0.7250892  0.5814219  0.05509925 0.17046838\n",
      "   0.06625327 0.29123315 0.22539546 0.69602835]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "   0.4762388  0.69575906 0.5130842...727653 0.11099589 0.7250892  0.5814219  0.05509925 0.17046838\n",
      "   0.06625327 0.29123315 0.22539546 0.69602835]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57192110>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "   0.4762388  0.69575906 0.5130842 ...   0.19727653 0.11099589 0.7250892  0.5814219  0.05509925 0.17046838\n",
      "   0.06625327 0.29123315 0.22539546 0.69602835]]])\n",
      "        _          = 34\n",
      "        batch_size = 1\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0...04\n",
      "  0.94874775 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]])\n",
      "        inputs     = [needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  ....62843406 0.6395442  0.6625407  0.41725296 0.32598472 0.09647448\n",
      "  0.43770888 0.9805687  0.43464237 0.81465334]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57191ed0>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd57192110>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  ...64 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]]), needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57191ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0...04\n",
      "  0.94874775 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.08032027 -0.14554556  0.18016452 -0.01525387 -0.09916121  0.276915\n",
      "   0.19672248  0.28566617  0.2832625   0.08742937  0.25115114 -0.1813819 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.23774952 -0.19287734 -0.01007059  0.19995674  0.05511272 -0.04991287\n",
      "   0.22529238 -0.16991024  0.16528198  0.24698812  0.18351525 -0.13510607]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd57191ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.02681735 -0.1387479   0.12995702 -0.09275365 -0.23964426  0.01387161\n",
      "   0.0848912  -0.10870798 -0.1...269433  0.26481068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]])\n",
      "        self       = needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0...04\n",
      "  0.94874775 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  ...69433  0.26481068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725b110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  ...69433  0.26481068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725b110>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584bb7b0>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5725b150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ray.ndarray_backend_cpu.Array object at 0x7fbd584ba370>, 1, 34, 12') raised in repr()] Tensor object at 0x7fbd5725b150>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0.43780...5 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]], device=cpu())\n",
      "        b          = NDArray([[-0.02681735 -0.1387479   0.12995702 -0.09275365 -0.23964426  0.01387161\n",
      "   0.0848912  -0.10870798 -0.1743614...068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5725b110>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0.43780...5 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]], device=cpu())\n",
      "        b          = NDArray([[-0.02681735 -0.1387479   0.12995702 -0.09275365 -0.23964426  0.01387161\n",
      "   0.0848912  -0.10870798 -0.1743614...068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0.43780...5 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]], device=cpu())\n",
      "other = NDArray([[-0.02681735 -0.1387479   0.12995702 -0.09275365 -0.23964426  0.01387161\n",
      "   0.0848912  -0.10870798 -0.1743614...068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd569471f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57192bb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57259430>, 1, 34, 12\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 34\n",
      "other      = NDArray([[-0.02681735 -0.1387479   0.12995702 -0.09275365 -0.23964426  0.01387161\n",
      "   0.0848912  -0.10870798 -0.1743614...068  0.06576142 -0.09712383\n",
      "   0.12937641  0.20436707  0.2121228   0.01111829  0.07708094  0.11078876]], device=cuda())\n",
      "out        = NDArray([[8.1775126e+35 4.5823861e-41 1.5551478e-34 0.0000000e+00 1.3081290e+00\n",
      "  1.1638604e+00 1.0180688e+00 1.3663578e+00 6.4611465e-01 9.2358273e-01\n",
      "  9.0477723e-01 3.2055959e-02]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.03304723 0.7302288  0.6217143  0.43786728 0.04997312 0.08003444\n",
      "  0.4762388  0.69575906 0.5130842  0.43780...5 0.4195611  0.6291979  0.55667216 0.97559464 0.11436573\n",
      "  0.67876524 0.62430245 0.5003295  0.7500509 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 6.8809944e-01 -5.3276682e-01  3.5119069e-01 -1.0099984e-01\n",
      "   -7.8163123e-01 -1.0080031e+00 -2.69508...655e-02 -1.1222547e+00 -3.4964848e-01 -2.6753631e-01\n",
      "   -2.0374437e-01  6.6540945e-01 -5.0283539e-01 -7.5652355e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-1.36007488e+00  6.41722500e-01  9.29798260e-02  6.17499828e-01\n",
      "   -6.43436432e-01 -4.87289339e-01 -3... -4.74847525e-01  1.99553773e-01 -1.69611776e+00\n",
      "    3.98368090e-01  5.38506031e-01 -2.83701032e-01  2.49536961e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd578dc950>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[798., 387., 363., 971., 207., 161., 857., 984., 856., 436., 700.,\n",
      "        124., 808., 357., 822.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[798. 387. 363. 971. 207. 161. 857. 984. 856. 436. 700. 124. 808. 357.\n",
      "  822.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dc950>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd578dc950>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[798. 387. 363. 971. 207. 161. 857. 984. 856. 436. 700. 124. 808. 357.\n",
      "  822.]])\n",
      "        x_emb      = needle.Tensor([[[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "   0.36138692 0.45929706 0.3309472 ...\n",
      "   0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "   0.7895689  0.3314353  0.4442106  0.13361989]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "   0.36138692 0.45929706 0.3309472...201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "   0.7895689  0.3314353  0.4442106  0.13361989]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578dc250>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "   0.36138692 0.45929706 0.3309472 ...\n",
      "   0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "   0.7895689  0.3314353  0.4442106  0.13361989]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0...774\n",
      "  0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]])\n",
      "        inputs     = [needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  ...74\n",
      "  0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578df190>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd578dc250>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578df190>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0...774\n",
      "  0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.02940848 -0.1106697   0.2205655   0.18890968 -0.08881265 -0.13063593\n",
      "  -0.12577772  0.10976601  0.2...05655   0.18890968 -0.08881265 -0.13063593\n",
      "  -0.12577772  0.10976601  0.25461042 -0.10156424 -0.22410735  0.1299065 ]])\n",
      "        bias_ih    = needle.Tensor([[-0.15772715  0.09283969  0.00819439  0.2837698  -0.08141726 -0.13275437\n",
      "  -0.00578633  0.14153025 -0.1...819439  0.2837698  -0.08141726 -0.13275437\n",
      "  -0.00578633  0.14153025 -0.19889511 -0.12474617  0.14214289 -0.20708601]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd578df190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.21240509  0.03034613 -0.1040694   0.07840303  0.090242    0.1090669\n",
      "   0.18079218  0.00244227  0.00...8275138 -0.22251481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]])\n",
      "        self       = needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0...774\n",
      "  0.7201518  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  ...275138 -0.22251481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55eedbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  ...275138 -0.22251481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55eedbd0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd56f102b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd55eee4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd56f122b0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd55eee4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0.24680...8  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]], device=cpu())\n",
      "        b          = NDArray([[-0.21240509  0.03034613 -0.1040694   0.07840303  0.090242    0.1090669\n",
      "   0.18079218  0.00244227  0.00230446...1481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55eedbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0.24680...8  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]], device=cpu())\n",
      "        b          = NDArray([[-0.21240509  0.03034613 -0.1040694   0.07840303  0.090242    0.1090669\n",
      "   0.18079218  0.00244227  0.00230446...1481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0.24680...8  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]], device=cpu())\n",
      "other = NDArray([[-0.21240509  0.03034613 -0.1040694   0.07840303  0.090242    0.1090669\n",
      "   0.18079218  0.00244227  0.00230446...1481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55eefdb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd578dcb70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55eeeeb0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.21240509  0.03034613 -0.1040694   0.07840303  0.090242    0.1090669\n",
      "   0.18079218  0.00244227  0.00230446...1481  0.11899972  0.2488324\n",
      "  -0.08760922  0.2858675  -0.2883719   0.10805798 -0.26392597  0.01737583]], device=cuda())\n",
      "out        = NDArray([[1.21096850e-38 0.00000000e+00 1.21405808e-38 0.00000000e+00\n",
      "  1.21096850e-38 0.00000000e+00 1.21096850e-38 0...000000e+00 1.24509404e-38 0.00000000e+00\n",
      "  1.24687762e-38 0.00000000e+00 1.24509404e-38 0.00000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.28533104 0.4271746  0.5077383  0.20579392 0.50520915 0.92354757\n",
      "  0.36138692 0.45929706 0.3309472  0.24680...8  0.22997777 0.4826081  0.84845185 0.39915827 0.9201632\n",
      "  0.7895689  0.3314353  0.4442106  0.13361989]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.5157613   0.1182226  -0.35070798 -0.36896637 -0.03714681\n",
      "    1.647425   -0.83036774  0.35847738  0...6 -0.279328    0.18975295\n",
      "   -0.8529633  -0.3037605  -2.3746557  -1.0319636   0.59790295\n",
      "   -1.4895359  -1.2531475 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-2.93983519e-01 -1.12304516e-01  1.08849578e-01 -3.31277132e-01\n",
      "    2.60328919e-01  3.74556959e-01  8...  6.46672666e-01 -7.84070432e-01  1.11544180e+00\n",
      "   -8.35281074e-01  6.11072898e-01 -8.65667611e-02  9.16545033e-01]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd5725ab50>\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[289.,  76., 816., 210., 625., 887., 329.,  30., 643., 353.,  96.,\n",
      "         98.,  32.,  34., 323.],\n",
      "       [153...    [435.,  53., 943., 931., 205., 890., 744.,  57., 819., 147., 917.,\n",
      "        215.,  68.,  79., 829.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[289.  76. 816. 210. 625. 887. 329.  30. 643. 353.  96.  98.  32.  34.\n",
      "  323.]\n",
      " [153. 983. 894. 599. 1...6. 616. 981. 744. 144.\n",
      "  465.]\n",
      " [435.  53. 943. 931. 205. 890. 744.  57. 819. 147. 917. 215.  68.  79.\n",
      "  829.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5725ab50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd5725ab50>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[289.  76. 816. 210. 625. 887. 329.  30. 643. 353.  96.  98.  32.  34.\n",
      "  323.]\n",
      " [153. 983. 894. 599. 17...466. 516. 616. 981. 744. 144.\n",
      "  465.]\n",
      " [435.  53. 943. 931. 205. 890. 744.  57. 819. 147. 917. 215.  68.  79.\n",
      "  829.]])\n",
      "        x_emb      = needle.Tensor([[[0.5095273  0.6682178  0.8585364  ... 0.6234576  0.8632534  0.7362159 ]\n",
      "  [0.59040123 0.44047442 0.400...769  ... 0.02442428 0.922451   0.9537211 ]\n",
      "  [0.33487844 0.80269974 0.35768178 ... 0.10135488 0.96084636 0.03050059]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5095273  0.6682178  0.8585364  ... 0.6234576  0.8632534  0.7362159 ]\n",
      "  [0.59040123 0.44047442 0.40.... 0.02442428 0.922451   0.9537211 ]\n",
      "  [0.33487844 0.80269974 0.35768178 ... 0.10135488 0.96084636 0.03050059]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5725a710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5095273  0.6682178  0.8585364  ... 0.6234576  0.8632534  0.7362159 ]\n",
      "  [0.59040123 0.44047442 0.400...769  ... 0.02442428 0.922451   0.9537211 ]\n",
      "  [0.33487844 0.80269974 0.35768178 ... 0.10135488 0.96084636 0.03050059]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050...356e-01 7.72042513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]])\n",
      "        inputs     = [needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.7649405....16903652 0.8153301  0.7558556  0.14081995 0.5208536  0.50418705\n",
      "  0.9288093  0.44803327 0.34108305 0.15431996]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725a250>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd5725a710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.7649405... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725a250>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050...356e-01 7.72042513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.01239675  0.2840513   0.09091872 -0.20084882  0.01748082  0.04467887\n",
      "   0.26087886 -0.1620224  -0.1...091872 -0.20084882  0.01748082  0.04467887\n",
      "   0.26087886 -0.1620224  -0.17290229 -0.11919998  0.03220093 -0.26895893]])\n",
      "        bias_ih    = needle.Tensor([[-0.19508067  0.02707142  0.21440661 -0.10161953  0.18304524  0.20147079\n",
      "  -0.25170124  0.22320223 -0.0...440661 -0.10161953  0.18304524  0.20147079\n",
      "  -0.25170124  0.22320223 -0.05908415 -0.20750846 -0.18410486 -0.25557777]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd5725a250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.09002361  0.22126818 -0.09955145 -0.24704923  0.09935823  0.07254395\n",
      "   0.03904751  0.12799403  0.0...420845  0.06531456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]])\n",
      "        self       = needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050...356e-01 7.72042513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.7649405...20845  0.06531456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580dd650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.7649405...20845  0.06531456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580dd650>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd585066f0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd580dd310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd585043f0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd580dd310>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050e-01 8...2513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.09002361  0.22126818 -0.09955145 -0.24704923  0.09935823  0.07254395\n",
      "   0.03904751  0.12799403  0.0477298...456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580dd650>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050e-01 8...2513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.09002361  0.22126818 -0.09955145 -0.24704923  0.09935823  0.07254395\n",
      "   0.03904751  0.12799403  0.0477298...456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050e-01 8...2513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]], device=cpu())\n",
      "other = NDArray([[-0.09002361  0.22126818 -0.09955145 -0.24704923  0.09935823  0.07254395\n",
      "   0.03904751  0.12799403  0.0477298...456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580dd7b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd572593b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd580ddbf0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.09002361  0.22126818 -0.09955145 -0.24704923  0.09935823  0.07254395\n",
      "   0.03904751  0.12799403  0.0477298...456  0.00588924 -0.19943231\n",
      "   0.19775397 -0.15650475 -0.22715273 -0.28474563  0.02702942 -0.11847711]], device=cuda())\n",
      "out        = NDArray([[2.0456658e-33 0.0000000e+00 2.3206389e-34 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.000000...\n",
      "  0.0000000e+00 1.2037176e-38 0.0000000e+00 1.2466624e-38 0.0000000e+00\n",
      "  1.2037176e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[5.09527326e-01 6.68217778e-01 8.58536422e-01 6.63568854e-01\n",
      "  1.27532646e-01 9.87473607e-01 8.76494050e-01 8...2513e-01\n",
      "  2.24044949e-01 3.58527273e-01 4.18880224e-01 1.20040156e-01\n",
      "  8.25537980e-01 1.07007757e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 6.4424711e-01 -3.7973210e-01 -2.1596572e+00  1.6015436e-01\n",
      "   -3.0833864e-01  8.7346542e-01  4.12931...651e+00 -3.3164114e-01 -8.9968437e-01  2.9858023e-01\n",
      "    2.0025804e+00 -1.1443319e+00  4.5453954e-01  7.9671639e-01]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.29863474  0.17636162  1.4772437   0.4009905   0.87826014\n",
      "   -0.43669498 -0.03644678 -0.0906073   1...01 -0.5639666   0.5001358\n",
      "    2.25947     0.2692351  -0.7670907  -1.9139432   0.88797396\n",
      "    0.27828276  0.9909687 ]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd573a54d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[233., 106., 641., 286., 816., 258., 530., 798., 650., 975., 209.,\n",
      "        441., 376., 272., 990.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[233. 106. 641. 286. 816. 258. 530. 798. 650. 975. 209. 441. 376. 272.\n",
      "  990.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd573a54d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd573a54d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[233. 106. 641. 286. 816. 258. 530. 798. 650. 975. 209. 441. 376. 272.\n",
      "  990.]])\n",
      "        x_emb      = needle.Tensor([[[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "   0.92661875 0.64512324 0.06580926...   0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "   0.77904546 0.47401217 0.98744094 0.06434116]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "   0.92661875 0.64512324 0.0658092...73357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "   0.77904546 0.47401217 0.98744094 0.06434116]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd573a4b90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "   0.92661875 0.64512324 0.06580926...   0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "   0.77904546 0.47401217 0.98744094 0.06434116]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0...34\n",
      "  0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]])\n",
      "        inputs     = [needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 ...4\n",
      "  0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]])]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7e50>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd573a4b90>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7e50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0...34\n",
      "  0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.220557   -0.00478601  0.16194132  0.17809087 -0.15700077 -0.17552125\n",
      "  -0.05122392  0.2063779  -0.0...194132  0.17809087 -0.15700077 -0.17552125\n",
      "  -0.05122392  0.2063779  -0.09141019 -0.1422137  -0.2379719  -0.09365895]])\n",
      "        bias_ih    = needle.Tensor([[-0.06615548  0.12015197 -0.10709481 -0.20777225 -0.2367292  -0.01002476\n",
      "   0.24417067 -0.21476915 -0.1...709481 -0.20777225 -0.2367292  -0.01002476\n",
      "   0.24417067 -0.21476915 -0.16927513  0.14518082  0.03275153  0.22407293]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd573a7e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-2.80383199e-01  2.04179794e-01 -1.72872514e-01 -4.35468554e-03\n",
      "   2.32807875e-01 -1.91780508e-01 -1.7...02  1.45601034e-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]])\n",
      "        self       = needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0...34\n",
      "  0.6973357  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 ...2  1.45601034e-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806c110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 ...2  1.45601034e-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806c110>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd56abea30>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5806d050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd56abee30>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd5806d050>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0.65374...  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]], device=cpu())\n",
      "        b          = NDArray([[-2.80383199e-01  2.04179794e-01 -1.72872514e-01 -4.35468554e-03\n",
      "   2.32807875e-01 -1.91780508e-01 -1.7172859...-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5806c110>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0.65374...  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]], device=cpu())\n",
      "        b          = NDArray([[-2.80383199e-01  2.04179794e-01 -1.72872514e-01 -4.35468554e-03\n",
      "   2.32807875e-01 -1.91780508e-01 -1.7172859...-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0.65374...  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]], device=cpu())\n",
      "other = NDArray([[-2.80383199e-01  2.04179794e-01 -1.72872514e-01 -4.35468554e-03\n",
      "   2.32807875e-01 -1.91780508e-01 -1.7172859...-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5806d470>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd573a5df0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5806f9f0>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-2.80383199e-01  2.04179794e-01 -1.72872514e-01 -4.35468554e-03\n",
      "   2.32807875e-01 -1.91780508e-01 -1.7172859...-02  1.03697091e-01  2.43338645e-01\n",
      "  -2.17016935e-01 -1.17452368e-01 -2.08382130e-01  1.12015069e-01]], device=cuda())\n",
      "out        = NDArray([[1.2109685e-38 0.0000000e+00 1.2140581e-38 0.0000000e+00 1.2109685e-38\n",
      "  0.0000000e+00 1.2109685e-38 0.000000...\n",
      "  0.0000000e+00 1.2450940e-38 0.0000000e+00 1.2468776e-38 0.0000000e+00\n",
      "  1.2450940e-38 0.0000000e+00]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[0.5243513  0.8251076  0.707283   0.28806698 0.16531287 0.27724755\n",
      "  0.92661875 0.64512324 0.06580926 0.65374...  0.32255456 0.39744517 0.69299906 0.5563173  0.31409177\n",
      "  0.77904546 0.47401217 0.98744094 0.06434116]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-1.1866902  -0.4204378   0.79942477 -0.2342561  -1.8716384\n",
      "    0.3492206   1.5994061  -0.56739753 -1....6  0.14480346 -0.80761296\n",
      "    0.77569544 -1.0344185  -0.4331339  -0.47072315 -0.51644164\n",
      "   -0.40470454 -1.6563864 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = needle.Tensor([[[-0.38964587 -0.885625    0.7285119  -0.04539206 -0.9265443\n",
      "   -0.6938091   0.90982246  0.8177766  -0....6   1.3154659  -0.40216345\n",
      "    0.1293266   1.3719076   0.36415225  1.4589462   1.6872588\n",
      "    1.6399732  -0.32406744]]])\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fbd56ea22d0>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[269., 216., 288., 178.,  28., 101., 241.,  93., 263., 440., 977.,\n",
      "        177., 409., 796., 258.],\n",
      "       [524...    [196., 948.,  16., 586., 511., 481., 539., 139., 702., 297., 514.,\n",
      "        995., 119., 619., 809.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[269. 216. 288. 178.  28. 101. 241.  93. 263. 440. 977. 177. 409. 796.\n",
      "  258.]\n",
      " [524. 864. 110. 399. 8...8.  46. 460. 912. 943.\n",
      "  795.]\n",
      " [196. 948.  16. 586. 511. 481. 539. 139. 702. 297. 514. 995. 119. 619.\n",
      "  809.]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56ea22d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = None\n",
      "        self       = <models.LanguageModel object at 0x7fbd56ea22d0>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[269. 216. 288. 178.  28. 101. 241.  93. 263. 440. 977. 177. 409. 796.\n",
      "  258.]\n",
      " [524. 864. 110. 399. 85...455. 268.  46. 460. 912. 943.\n",
      "  795.]\n",
      " [196. 948.  16. 586. 511. 481. 539. 139. 702. 297. 514. 995. 119. 619.\n",
      "  809.]])\n",
      "        x_emb      = needle.Tensor([[[3.8960746e-01 7.8327000e-01 6.7667055e-01 ... 2.5771081e-01\n",
      "   2.8014807e-02 1.7025553e-01]\n",
      "  [5.1745...31e-01 7.1327436e-01]\n",
      "  [1.9275087e-01 5.8387154e-01 5.6503254e-01 ... 8.9523292e-01\n",
      "   8.8339126e-01 4.7159344e-02]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[3.8960746e-01 7.8327000e-01 6.7667055e-01 ... 2.5771081e-01\n",
      "   2.8014807e-02 1.7025553e-01]\n",
      "  [5.174...7.1327436e-01]\n",
      "  [1.9275087e-01 5.8387154e-01 5.6503254e-01 ... 8.9523292e-01\n",
      "   8.8339126e-01 4.7159344e-02]]]), None)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56ea0a50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:155: in forward\n",
      "    h = layer(\u001b[96minput\u001b[39;49;00m, h)\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[3.8960746e-01 7.8327000e-01 6.7667055e-01 ... 2.5771081e-01\n",
      "   2.8014807e-02 1.7025553e-01]\n",
      "  [5.1745...31e-01 7.1327436e-01]\n",
      "  [1.9275087e-01 5.8387154e-01 5.6503254e-01 ... 8.9523292e-01\n",
      "   8.8339126e-01 4.7159344e-02]]])\n",
      "        _          = 34\n",
      "        batch_size = 15\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        h0         = [needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])]\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4....01 9.6598989e-01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]])\n",
      "        inputs     = [needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4....8573552  0.30639294 0.8609641  0.01222658 0.13596182 0.38819465\n",
      "  0.8319768  0.5714483  0.42865622 0.80120105]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ea2610>\n",
      "        self       = <needle.nn.nn_sequence.RNN object at 0x7fbd56ea0a50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ea2610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:79: in forward\n",
      "    out = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4....01 9.6598989e-01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.00520536  0.03330833 -0.00169179 -0.1999741  -0.009123    0.0687435\n",
      "   0.15855294  0.19230989  0.21...0169179 -0.1999741  -0.009123    0.0687435\n",
      "   0.15855294  0.19230989  0.21814287 -0.06829582 -0.16514307  0.22817707]])\n",
      "        bias_ih    = needle.Tensor([[ 0.21261317 -0.2033234  -0.258407   -0.1324253   0.06354132  0.27534014\n",
      "  -0.17637448 -0.15969905 -0.2...8407   -0.1324253   0.06354132  0.27534014\n",
      "  -0.17637448 -0.15969905 -0.24102908 -0.06759882 -0.26651084 -0.1026939 ]])\n",
      "        h          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. ...\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        self       = <needle.nn.nn_sequence.RNNCell object at 0x7fbd56ea2610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.05833784 -0.23946847 -0.20796815  0.05087879 -0.12357938 -0.07266672\n",
      "  -0.12291496 -0.16318016 -0.1...426334 -0.05488998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]])\n",
      "        self       = needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4....01 9.6598989e-01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4...26334 -0.05488998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580133d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4...26334 -0.05488998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580133d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd585a16f0>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58012310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...ay.ndarray_backend_cpu.Array object at 0x7fbd585a1170>, 15, 34, 12') raised in repr()] Tensor object at 0x7fbd58012310>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4.355164...01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.05833784 -0.23946847 -0.20796815  0.05087879 -0.12357938 -0.07266672\n",
      "  -0.12291496 -0.16318016 -0.191628 ...998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd580133d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4.355164...01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]], device=cpu())\n",
      "        b          = NDArray([[-0.05833784 -0.23946847 -0.20796815  0.05087879 -0.12357938 -0.07266672\n",
      "  -0.12291496 -0.16318016 -0.191628 ...998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4.355164...01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]], device=cpu())\n",
      "other = NDArray([[-0.05833784 -0.23946847 -0.20796815  0.05087879 -0.12357938 -0.07266672\n",
      "  -0.12291496 -0.16318016 -0.191628 ...998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56ea2370>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56ea14b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58011730>, 15, 34, 12\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 34\n",
      "other      = NDArray([[-0.05833784 -0.23946847 -0.20796815  0.05087879 -0.12357938 -0.07266672\n",
      "  -0.12291496 -0.16318016 -0.191628 ...998  0.03772816 -0.13139199\n",
      "   0.05648336  0.06428277 -0.18160081 -0.19951454 -0.22474262 -0.2243585 ]], device=cuda())\n",
      "out        = NDArray([[1.35559018e-19 4.05581204e-08 1.05035015e-05 5.34854337e+22\n",
      "  1.50084232e-19 1.66122525e-04 6.70172418e-10 5...294005e-11 1.35688019e-19 2.57831467e-09\n",
      "  1.73755790e-04 1.30278436e-11 1.35688019e-19 6.40940689e-10]], device=cpu())\n",
      "p          = 12\n",
      "self       = NDArray([[3.8960746e-01 7.8327000e-01 6.7667055e-01 5.0337374e-01 5.2779871e-01\n",
      "  1.5752603e-01 5.5815655e-01 4.355164...01 9.3137968e-01 4.0533754e-01 8.2195294e-01\n",
      "  1.3454835e-01 9.2604430e-03 2.8492847e-01 9.7231841e-01]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[1.4827842]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[0.99624133]]]), needle.Tensor([[[1.4827842]]]))\n",
      "h0         = needle.Tensor([[[0.99624133]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd55a01150>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), (needle.Tensor([[[0.99624133]]]), needle.Tensor([[[1.4827842]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a01150>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = (needle.Tensor([[[0.99624133]]]), needle.Tensor([[[1.4827842]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd55a01150>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.24162677]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.24162677]]]), (needle.Tensor([[[0.99624133]]]), needle.Tensor([[[1.4827842]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd55a02290>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.24162677]]])\n",
      "        batch_size = 1\n",
      "        c          = needle.Tensor([[1.4827842]])\n",
      "        c0         = (needle.Tensor([[1.4827842]]),)\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[0.99624133]])\n",
      "        h0         = (needle.Tensor([[0.99624133]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.24162677]])\n",
      "        inputs     = [needle.Tensor([[0.24162677]])]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd55a02b10>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd55a02290>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.24162677]]), (needle.Tensor([[0.99624133]]), needle.Tensor([[1.4827842]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd55a02b10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.24162677]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.3416937 -0.5359626  0.809392  -0.8373274]])\n",
      "        bias_ih    = needle.Tensor([[ 0.6931579  -0.9929867   0.10260677 -0.87338746]])\n",
      "        c0         = needle.Tensor([[1.4827842]])\n",
      "        h          = (needle.Tensor([[0.99624133]]), needle.Tensor([[1.4827842]]))\n",
      "        h0         = needle.Tensor([[0.99624133]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd55a02b10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.7390611   0.59085524  0.17062414 -0.05478823]])\n",
      "        self       = needle.Tensor([[0.24162677]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.24162677]]), needle.Tensor([[ 0.7390611   0.59085524  0.17062414 -0.05478823]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55a01d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.24162677]]), needle.Tensor([[ 0.7390611   0.59085524  0.17062414 -0.05478823]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55a01d50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd5708a4b0>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd55a03110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd57088030>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd55a03110>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.24162677]], device=cpu())\n",
      "        b          = NDArray([[ 0.7390611   0.59085524  0.17062414 -0.05478823]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd55a01d50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.24162677]], device=cpu())\n",
      "        b          = NDArray([[ 0.7390611   0.59085524  0.17062414 -0.05478823]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.24162677]], device=cpu())\n",
      "other = NDArray([[ 0.7390611   0.59085524  0.17062414 -0.05478823]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a00870>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd55a02830>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd55a03cb0>, 1, 1, 4\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.7390611   0.59085524  0.17062414 -0.05478823]], device=cuda())\n",
      "out        = NDArray([[8.1774873e+35 4.5823861e-41 3.4455282e-34 0.0000000e+00]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.24162677]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-1.7463825]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[0.05384694]]]), needle.Tensor([[[-1.7463825]]]))\n",
      "h0         = needle.Tensor([[[0.05384694]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd573f2910>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), (needle.Tensor([[[0.05384694]]]), needle.Tensor([[[-1.7463825]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd573f2910>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = (needle.Tensor([[[0.05384694]]]), needle.Tensor([[[-1.7463825]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd573f2910>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " ...[[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]]), (needle.Tensor([[[0.05384694]]]), needle.Tensor([[[-1.7463825]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd573f1b50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]\n",
      "\n",
      " [[0.52542573]]])\n",
      "        batch_size = 1\n",
      "        c          = needle.Tensor([[-1.7463825]])\n",
      "        c0         = (needle.Tensor([[-1.7463825]]),)\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[0.05384694]])\n",
      "        h0         = (needle.Tensor([[0.05384694]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.52542573]])\n",
      "        inputs     = [needle.Tensor([[0.52542573]]), needle.Tensor([[0.52542573]]), needle.Tensor([[0.52542573]]), needle.Tensor([[0.52542573]]), needle.Tensor([[0.52542573]]), needle.Tensor([[0.52542573]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd573f23d0>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd573f1b50>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.52542573]]), (needle.Tensor([[0.05384694]]), needle.Tensor([[-1.7463825]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd573f23d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.52542573]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.2803998  -0.20755541  0.6799967   0.4199729 ]])\n",
      "        bias_ih    = needle.Tensor([[ 0.6647601  -0.11484122 -0.8420445  -0.0678001 ]])\n",
      "        c0         = needle.Tensor([[-1.7463825]])\n",
      "        h          = (needle.Tensor([[0.05384694]]), needle.Tensor([[-1.7463825]]))\n",
      "        h0         = needle.Tensor([[0.05384694]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd573f23d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.12479311  0.24111402  0.11076188  0.08783853]])\n",
      "        self       = needle.Tensor([[0.52542573]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.52542573]]), needle.Tensor([[-0.12479311  0.24111402  0.11076188  0.08783853]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd558743d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.52542573]]), needle.Tensor([[-0.12479311  0.24111402  0.11076188  0.08783853]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd558743d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd573a63f0>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd55875210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd573a7f70>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd55875210>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.52542573]], device=cpu())\n",
      "        b          = NDArray([[-0.12479311  0.24111402  0.11076188  0.08783853]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd558743d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.52542573]], device=cpu())\n",
      "        b          = NDArray([[-0.12479311  0.24111402  0.11076188  0.08783853]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.52542573]], device=cpu())\n",
      "other = NDArray([[-0.12479311  0.24111402  0.11076188  0.08783853]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573f29f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd573f2db0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd558765f0>, 1, 1, 4\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.12479311  0.24111402  0.11076188  0.08783853]], device=cuda())\n",
      "out        = NDArray([[8.1774873e+35 4.5823861e-41 1.9034986e-33 0.0000000e+00]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.52542573]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[ 1.1487337]]\n",
      "\n",
      " [[-1.382791 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[-1.6864676]]\n",
      "\n",
      " [[-1.8682607]]]), needle.Tensor([[[ 1.1487337]]\n",
      "\n",
      " [[-1.382791 ]]]))\n",
      "h0         = needle.Tensor([[[-1.6864676]]\n",
      "\n",
      " [[-1.8682607]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5809ef50>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), (needle.Tensor([[[-1.6864676]]\n",
      "\n",
      " [[-1.8682607]]]), needle.Tensor([[[ 1.1487337]]\n",
      "\n",
      " [[-1.382791 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5809ef50>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = (needle.Tensor([[[-1.6864676]]\n",
      "\n",
      " [[-1.8682607]]]), needle.Tensor([[[ 1.1487337]]\n",
      "\n",
      " [[-1.382791 ]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd5809ef50>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.63137454]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.63137454]]]), (needle.Tensor([[[-1.6864676]]\n",
      "\n",
      " [[-1.8682607]]]), needle.Tensor([[[ 1.1487337]]\n",
      "\n",
      " [[-1.382791 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd5809f750>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.63137454]]])\n",
      "        batch_size = 1\n",
      "        c          = needle.Tensor([[1.1487337]])\n",
      "        c0         = (needle.Tensor([[1.1487337]]), needle.Tensor([[-1.382791]]))\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[-1.6864676]])\n",
      "        h0         = (needle.Tensor([[-1.6864676]]), needle.Tensor([[-1.8682607]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.63137454]])\n",
      "        inputs     = [needle.Tensor([[0.63137454]])]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd5809f850>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd5809f750>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.63137454]]), (needle.Tensor([[-1.6864676]]), needle.Tensor([[1.1487337]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd5809f850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.63137454]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[-0.8478203   0.127419    0.9502183  -0.53979254]])\n",
      "        bias_ih    = needle.Tensor([[ 0.03896391 -0.35921192 -0.42777723 -0.3971476 ]])\n",
      "        c0         = needle.Tensor([[1.1487337]])\n",
      "        h          = (needle.Tensor([[-1.6864676]]), needle.Tensor([[1.1487337]]))\n",
      "        h0         = needle.Tensor([[-1.6864676]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd5809f850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]])\n",
      "        self       = needle.Tensor([[0.63137454]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.63137454]]), needle.Tensor([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587d7f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.63137454]]), needle.Tensor([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587d7f50>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd584a6730>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd587d77d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd584a40f0>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd587d77d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.63137454]], device=cpu())\n",
      "        b          = NDArray([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd587d7f50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.63137454]], device=cpu())\n",
      "        b          = NDArray([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.63137454]], device=cpu())\n",
      "other = NDArray([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5809ea30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd5809d670>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd587d6af0>, 1, 1, 4\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.4615385  -0.36377412 -0.62469447 -0.84097135]], device=cuda())\n",
      "out        = NDArray([[8.1774873e+35 4.5823861e-41 1.3243501e-33 0.0000000e+00]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.63137454]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[-2.218828  ]]\n",
      "\n",
      " [[ 0.75063545]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[-0.14004616]]\n",
      "\n",
      " [[ 0.11606378]]]), needle.Tensor([[[-2.218828  ]]\n",
      "\n",
      " [[ 0.75063545]]]))\n",
      "h0         = needle.Tensor([[[-0.14004616]]\n",
      "\n",
      " [[ 0.11606378]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd581abd10>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]), (needle.Tensor([[[-0.14004616]]\n",
      "\n",
      " [[ 0.11606378]]]), needle.Tensor([[[-2.218828  ]]\n",
      "\n",
      " [[ 0.75063545]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd581abd10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = (needle.Tensor([[[-0.14004616]]\n",
      "\n",
      " [[ 0.11606378]]]), needle.Tensor([[[-2.218828  ]]\n",
      "\n",
      " [[ 0.75063545]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd581abd10>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]])\n",
      "        x_emb      = needle.Tensor([[[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " ...86452496]]]), (needle.Tensor([[[-0.14004616]]\n",
      "\n",
      " [[ 0.11606378]]]), needle.Tensor([[[-2.218828  ]]\n",
      "\n",
      " [[ 0.75063545]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd581aa710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]\n",
      "\n",
      " [[0.86452496]]])\n",
      "        batch_size = 1\n",
      "        c          = needle.Tensor([[-2.218828]])\n",
      "        c0         = (needle.Tensor([[-2.218828]]), needle.Tensor([[0.75063545]]))\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[-0.14004616]])\n",
      "        h0         = (needle.Tensor([[-0.14004616]]), needle.Tensor([[0.11606378]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.86452496]])\n",
      "        inputs     = [needle.Tensor([[0.86452496]]), needle.Tensor([[0.86452496]]), needle.Tensor([[0.86452496]]), needle.Tensor([[0.86452496]]), needle.Tensor([[0.86452496]]), needle.Tensor([[0.86452496]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd581aac90>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd581aa710>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.86452496]]), (needle.Tensor([[-0.14004616]]), needle.Tensor([[-2.218828]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd581aac90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.86452496]])\n",
      "        batch_size = 1\n",
      "        bias_hh    = needle.Tensor([[ 0.26921666 -0.9947709   0.41780388  0.47721684]])\n",
      "        bias_ih    = needle.Tensor([[ 0.17853212  0.11196792  0.5466856  -0.30966455]])\n",
      "        c0         = needle.Tensor([[-2.218828]])\n",
      "        h          = (needle.Tensor([[-0.14004616]]), needle.Tensor([[-2.218828]]))\n",
      "        h0         = needle.Tensor([[-0.14004616]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd581aac90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]])\n",
      "        self       = needle.Tensor([[0.86452496]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.86452496]]), needle.Tensor([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b49750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.86452496]]), needle.Tensor([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b49750>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd55906cb0>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd56b4ac10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...array.ndarray_backend_cpu.Array object at 0x7fbd55906b30>, 1, 1, 4') raised in repr()] Tensor object at 0x7fbd56b4ac10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.86452496]], device=cpu())\n",
      "        b          = NDArray([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd56b49750>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.86452496]], device=cpu())\n",
      "        b          = NDArray([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.86452496]], device=cpu())\n",
      "other = NDArray([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd576d82f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd581aacb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56b48730>, 1, 1, 4\u001b[0m\n",
      "\n",
      "m          = 1\n",
      "n          = 1\n",
      "other      = NDArray([[-0.9695863   0.28617895 -0.7378682   0.3698498 ]], device=cuda())\n",
      "out        = NDArray([[8.1774873e+35 4.5823861e-41 7.1680084e-35 0.0000000e+00]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.86452496]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.29828703]\n",
      "  [-0.41966695]\n",
      "  [-0.19168583]\n",
      "  [ 0.69464827]\n",
      "  [-0.21766703]\n",
      "  [-0.05655547]\n",
      "  [ 0.16...03 ]\n",
      "  [-0.9819632 ]\n",
      "  [ 0.0552718 ]\n",
      "  [ 0.4562415 ]\n",
      "  [ 0.89517653]\n",
      "  [ 1.8880291 ]\n",
      "  [-0.9942215 ]\n",
      "  [-0.7296312 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[ 1.0390903 ]\n",
      "  [ 0.5546989 ]\n",
      "  [ 0.14940491]\n",
      "  [ 0.82119006]\n",
      "  [-0.4427576 ]\n",
      "  [ 1.0112054 ]\n",
      "  [-0.5...3 ]\n",
      "  [-0.9819632 ]\n",
      "  [ 0.0552718 ]\n",
      "  [ 0.4562415 ]\n",
      "  [ 0.89517653]\n",
      "  [ 1.8880291 ]\n",
      "  [-0.9942215 ]\n",
      "  [-0.7296312 ]]]))\n",
      "h0         = needle.Tensor([[[ 1.0390903 ]\n",
      "  [ 0.5546989 ]\n",
      "  [ 0.14940491]\n",
      "  [ 0.82119006]\n",
      "  [-0.4427576 ]\n",
      "  [ 1.0112054 ]\n",
      "  [-0.52...686]\n",
      "  [-1.2766007 ]\n",
      "  [ 1.8904608 ]\n",
      "  [-0.2178374 ]\n",
      "  [ 0.58123356]\n",
      "  [-0.5521808 ]\n",
      "  [ 1.03074   ]\n",
      "  [ 0.7007253 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56c8fc10>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), (needle.Tensor([[[ 1.0390903 ]\n",
      "  [ 0.5546989 ]\n",
      "  [ 0... ]\n",
      "  [-0.9819632 ]\n",
      "  [ 0.0552718 ]\n",
      "  [ 0.4562415 ]\n",
      "  [ 0.89517653]\n",
      "  [ 1.8880291 ]\n",
      "  [-0.9942215 ]\n",
      "  [-0.7296312 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56c8fc10>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = (needle.Tensor([[[ 1.0390903 ]\n",
      "  [ 0.5546989 ]\n",
      "  [ 0.14940491]\n",
      "  [ 0.82119006]\n",
      "  [-0.4427576 ]\n",
      "  [ 1.0112054 ]\n",
      "  [-0.5...3 ]\n",
      "  [-0.9819632 ]\n",
      "  [ 0.0552718 ]\n",
      "  [ 0.4562415 ]\n",
      "  [ 0.89517653]\n",
      "  [ 1.8880291 ]\n",
      "  [-0.9942215 ]\n",
      "  [-0.7296312 ]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd56c8fc10>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.... ]\n",
      "  [-0.9819632 ]\n",
      "  [ 0.0552718 ]\n",
      "  [ 0.4562415 ]\n",
      "  [ 0.89517653]\n",
      "  [ 1.8880291 ]\n",
      "  [-0.9942215 ]\n",
      "  [-0.7296312 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd56c8f990>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]\n",
      "  [0.9300626]]])\n",
      "        batch_size = 15\n",
      "        c          = needle.Tensor([[-0.29828703]\n",
      " [-0.41966695]\n",
      " [-0.19168583]\n",
      " [ 0.69464827]\n",
      " [-0.21766703]\n",
      " [-0.05655547]\n",
      " [ 0.16055906]\n",
      " [ 1.1181103 ]\n",
      " [-0.9819632 ]\n",
      " [ 0.0552718 ]\n",
      " [ 0.4562415 ]\n",
      " [ 0.89517653]\n",
      " [ 1.8880291 ]\n",
      " [-0.9942215 ]\n",
      " [-0.7296312 ]])\n",
      "        c0         = (needle.Tensor([[-0.29828703]\n",
      " [-0.41966695]\n",
      " [-0.19168583]\n",
      " [ 0.69464827]\n",
      " [-0.21766703]\n",
      " [-0.05655547]\n",
      " [ 0.16055906....1181103 ]\n",
      " [-0.9819632 ]\n",
      " [ 0.0552718 ]\n",
      " [ 0.4562415 ]\n",
      " [ 0.89517653]\n",
      " [ 1.8880291 ]\n",
      " [-0.9942215 ]\n",
      " [-0.7296312 ]]),)\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[ 1.0390903 ]\n",
      " [ 0.5546989 ]\n",
      " [ 0.14940491]\n",
      " [ 0.82119006]\n",
      " [-0.4427576 ]\n",
      " [ 1.0112054 ]\n",
      " [-0.5268701 ]\n",
      " [-0.37665686]\n",
      " [-1.2766007 ]\n",
      " [ 1.8904608 ]\n",
      " [-0.2178374 ]\n",
      " [ 0.58123356]\n",
      " [-0.5521808 ]\n",
      " [ 1.03074   ]\n",
      " [ 0.7007253 ]])\n",
      "        h0         = (needle.Tensor([[ 1.0390903 ]\n",
      " [ 0.5546989 ]\n",
      " [ 0.14940491]\n",
      " [ 0.82119006]\n",
      " [-0.4427576 ]\n",
      " [ 1.0112054 ]\n",
      " [-0.5268701 ....37665686]\n",
      " [-1.2766007 ]\n",
      " [ 1.8904608 ]\n",
      " [-0.2178374 ]\n",
      " [ 0.58123356]\n",
      " [-0.5521808 ]\n",
      " [ 1.03074   ]\n",
      " [ 0.7007253 ]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]])\n",
      "        inputs     = [needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]])]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56c8ee50>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd56c8f990>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]....1181103 ]\n",
      " [-0.9819632 ]\n",
      " [ 0.0552718 ]\n",
      " [ 0.4562415 ]\n",
      " [ 0.89517653]\n",
      " [ 1.8880291 ]\n",
      " [-0.9942215 ]\n",
      " [-0.7296312 ]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56c8ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[0.5939821  0.3505062  0.7566422  0.56678855]\n",
      " [0.5939821  0.3505062  0.7566422  0.56678855]\n",
      " [0.593982... 0.7566422  0.56678855]\n",
      " [0.5939821  0.3505062  0.7566422  0.56678855]\n",
      " [0.5939821  0.3505062  0.7566422  0.56678855]])\n",
      "        bias_ih    = needle.Tensor([[-0.25704396 -0.05986923 -0.9702532   0.7522054 ]\n",
      " [-0.25704396 -0.05986923 -0.9702532   0.7522054 ]\n",
      " [...2   0.7522054 ]\n",
      " [-0.25704396 -0.05986923 -0.9702532   0.7522054 ]\n",
      " [-0.25704396 -0.05986923 -0.9702532   0.7522054 ]])\n",
      "        c0         = needle.Tensor([[-0.29828703]\n",
      " [-0.41966695]\n",
      " [-0.19168583]\n",
      " [ 0.69464827]\n",
      " [-0.21766703]\n",
      " [-0.05655547]\n",
      " [ 0.16055906]\n",
      " [ 1.1181103 ]\n",
      " [-0.9819632 ]\n",
      " [ 0.0552718 ]\n",
      " [ 0.4562415 ]\n",
      " [ 0.89517653]\n",
      " [ 1.8880291 ]\n",
      " [-0.9942215 ]\n",
      " [-0.7296312 ]])\n",
      "        h          = (needle.Tensor([[ 1.0390903 ]\n",
      " [ 0.5546989 ]\n",
      " [ 0.14940491]\n",
      " [ 0.82119006]\n",
      " [-0.4427576 ]\n",
      " [ 1.0112054 ]\n",
      " [-0.5268701 ...1.1181103 ]\n",
      " [-0.9819632 ]\n",
      " [ 0.0552718 ]\n",
      " [ 0.4562415 ]\n",
      " [ 0.89517653]\n",
      " [ 1.8880291 ]\n",
      " [-0.9942215 ]\n",
      " [-0.7296312 ]]))\n",
      "        h0         = needle.Tensor([[ 1.0390903 ]\n",
      " [ 0.5546989 ]\n",
      " [ 0.14940491]\n",
      " [ 0.82119006]\n",
      " [-0.4427576 ]\n",
      " [ 1.0112054 ]\n",
      " [-0.5268701 ]\n",
      " [-0.37665686]\n",
      " [-1.2766007 ]\n",
      " [ 1.8904608 ]\n",
      " [-0.2178374 ]\n",
      " [ 0.58123356]\n",
      " [-0.5521808 ]\n",
      " [ 1.03074   ]\n",
      " [ 0.7007253 ]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56c8ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]])\n",
      "        self       = needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]....9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]]), needle.Tensor([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58504810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]....9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]]), needle.Tensor([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58504810>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57ed89f0>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd58504d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57eda470>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd58504d90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]], device=cpu())\n",
      "        b          = NDArray([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58504810>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]], device=cpu())\n",
      "        b          = NDArray([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]], device=cpu())\n",
      "other = NDArray([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56c8cb70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56c8c330>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd585045b0>, 15, 1, 4\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.3497461  -0.95560455 -0.31561637 -0.1622442 ]], device=cuda())\n",
      "out        = NDArray([[5.8337157e-10 4.3445434e-05 1.7375208e-04 1.1719867e-19]\n",
      " [4.0046504e-11 1.7154964e-07 1.5010421e-19 4.50473...6 4.0556348e-08 2.7151091e-06 8.0248859e+17]\n",
      " [1.2455823e-11 1.6897242e-04 1.7376315e-04 1.0650768e-32]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]\n",
      " [0.9300626]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-2.000615  ]\n",
      "  [-0.12239981]\n",
      "  [-1.2275628 ]\n",
      "  [ 1.342174  ]\n",
      "  [ 0.1939382 ]\n",
      "  [ 0.6552325 ]\n",
      "  [-0.72...438]\n",
      "  [ 0.3360021 ]\n",
      "  [-0.25781852]\n",
      "  [ 0.84998065]\n",
      "  [ 0.19699948]\n",
      "  [-0.14878936]\n",
      "  [ 0.29517597]\n",
      "  [ 0.7892822 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[ 1.1356641 ]\n",
      "  [ 0.0884271 ]\n",
      "  [ 0.33876866]\n",
      "  [-1.2498374 ]\n",
      "  [ 1.6761293 ]\n",
      "  [-1.3367964 ]\n",
      "  [ 1.1...38]\n",
      "  [ 0.3360021 ]\n",
      "  [-0.25781852]\n",
      "  [ 0.84998065]\n",
      "  [ 0.19699948]\n",
      "  [-0.14878936]\n",
      "  [ 0.29517597]\n",
      "  [ 0.7892822 ]]]))\n",
      "h0         = needle.Tensor([[[ 1.1356641 ]\n",
      "  [ 0.0884271 ]\n",
      "  [ 0.33876866]\n",
      "  [-1.2498374 ]\n",
      "  [ 1.6761293 ]\n",
      "  [-1.3367964 ]\n",
      "  [ 1.18...734]\n",
      "  [ 1.6079314 ]\n",
      "  [ 0.3353808 ]\n",
      "  [-1.1454853 ]\n",
      "  [ 0.44609627]\n",
      "  [-0.21087201]\n",
      "  [ 0.98961145]\n",
      "  [ 0.26444343]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd56386190>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0....8]\n",
      "  [ 0.3360021 ]\n",
      "  [-0.25781852]\n",
      "  [ 0.84998065]\n",
      "  [ 0.19699948]\n",
      "  [-0.14878936]\n",
      "  [ 0.29517597]\n",
      "  [ 0.7892822 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd56386190>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = (needle.Tensor([[[ 1.1356641 ]\n",
      "  [ 0.0884271 ]\n",
      "  [ 0.33876866]\n",
      "  [-1.2498374 ]\n",
      "  [ 1.6761293 ]\n",
      "  [-1.3367964 ]\n",
      "  [ 1.1...38]\n",
      "  [ 0.3360021 ]\n",
      "  [-0.25781852]\n",
      "  [ 0.84998065]\n",
      "  [ 0.19699948]\n",
      "  [-0.14878936]\n",
      "  [ 0.29517597]\n",
      "  [ 0.7892822 ]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd56386190>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]...0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275...8]\n",
      "  [ 0.3360021 ]\n",
      "  [-0.25781852]\n",
      "  [ 0.84998065]\n",
      "  [ 0.19699948]\n",
      "  [-0.14878936]\n",
      "  [ 0.29517597]\n",
      "  [ 0.7892822 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd56385110>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]...0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]\n",
      "  [0.26232275]]])\n",
      "        batch_size = 15\n",
      "        c          = needle.Tensor([[-2.000615  ]\n",
      " [-0.12239981]\n",
      " [-1.2275628 ]\n",
      " [ 1.342174  ]\n",
      " [ 0.1939382 ]\n",
      " [ 0.6552325 ]\n",
      " [-0.721788  ]\n",
      " [ 0.48014438]\n",
      " [ 0.3360021 ]\n",
      " [-0.25781852]\n",
      " [ 0.84998065]\n",
      " [ 0.19699948]\n",
      " [-0.14878936]\n",
      " [ 0.29517597]\n",
      " [ 0.7892822 ]])\n",
      "        c0         = (needle.Tensor([[-2.000615  ]\n",
      " [-0.12239981]\n",
      " [-1.2275628 ]\n",
      " [ 1.342174  ]\n",
      " [ 0.1939382 ]\n",
      " [ 0.6552325 ]\n",
      " [-0.721788  ....48014438]\n",
      " [ 0.3360021 ]\n",
      " [-0.25781852]\n",
      " [ 0.84998065]\n",
      " [ 0.19699948]\n",
      " [-0.14878936]\n",
      " [ 0.29517597]\n",
      " [ 0.7892822 ]]),)\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[ 1.1356641 ]\n",
      " [ 0.0884271 ]\n",
      " [ 0.33876866]\n",
      " [-1.2498374 ]\n",
      " [ 1.6761293 ]\n",
      " [-1.3367964 ]\n",
      " [ 1.1831032 ]\n",
      " [-0.29080734]\n",
      " [ 1.6079314 ]\n",
      " [ 0.3353808 ]\n",
      " [-1.1454853 ]\n",
      " [ 0.44609627]\n",
      " [-0.21087201]\n",
      " [ 0.98961145]\n",
      " [ 0.26444343]])\n",
      "        h0         = (needle.Tensor([[ 1.1356641 ]\n",
      " [ 0.0884271 ]\n",
      " [ 0.33876866]\n",
      " [-1.2498374 ]\n",
      " [ 1.6761293 ]\n",
      " [-1.3367964 ]\n",
      " [ 1.1831032 ....29080734]\n",
      " [ 1.6079314 ]\n",
      " [ 0.3353808 ]\n",
      " [-1.1454853 ]\n",
      " [ 0.44609627]\n",
      " [-0.21087201]\n",
      " [ 0.98961145]\n",
      " [ 0.26444343]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]])\n",
      "        inputs     = [needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.2... [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56386a10>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd56385110>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.2....48014438]\n",
      " [ 0.3360021 ]\n",
      " [-0.25781852]\n",
      " [ 0.84998065]\n",
      " [ 0.19699948]\n",
      " [-0.14878936]\n",
      " [ 0.29517597]\n",
      " [ 0.7892822 ]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56386a10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.8630434  -0.7869507  -0.61796796 -0.52333295]\n",
      " [ 0.8630434  -0.7869507  -0.61796796 -0.52333295]\n",
      " [...96 -0.52333295]\n",
      " [ 0.8630434  -0.7869507  -0.61796796 -0.52333295]\n",
      " [ 0.8630434  -0.7869507  -0.61796796 -0.52333295]])\n",
      "        bias_ih    = needle.Tensor([[ 0.36739814 -0.41309643  0.01371014 -0.23764431]\n",
      " [ 0.36739814 -0.41309643  0.01371014 -0.23764431]\n",
      " [...14 -0.23764431]\n",
      " [ 0.36739814 -0.41309643  0.01371014 -0.23764431]\n",
      " [ 0.36739814 -0.41309643  0.01371014 -0.23764431]])\n",
      "        c0         = needle.Tensor([[-2.000615  ]\n",
      " [-0.12239981]\n",
      " [-1.2275628 ]\n",
      " [ 1.342174  ]\n",
      " [ 0.1939382 ]\n",
      " [ 0.6552325 ]\n",
      " [-0.721788  ]\n",
      " [ 0.48014438]\n",
      " [ 0.3360021 ]\n",
      " [-0.25781852]\n",
      " [ 0.84998065]\n",
      " [ 0.19699948]\n",
      " [-0.14878936]\n",
      " [ 0.29517597]\n",
      " [ 0.7892822 ]])\n",
      "        h          = (needle.Tensor([[ 1.1356641 ]\n",
      " [ 0.0884271 ]\n",
      " [ 0.33876866]\n",
      " [-1.2498374 ]\n",
      " [ 1.6761293 ]\n",
      " [-1.3367964 ]\n",
      " [ 1.1831032 ...0.48014438]\n",
      " [ 0.3360021 ]\n",
      " [-0.25781852]\n",
      " [ 0.84998065]\n",
      " [ 0.19699948]\n",
      " [-0.14878936]\n",
      " [ 0.29517597]\n",
      " [ 0.7892822 ]]))\n",
      "        h0         = needle.Tensor([[ 1.1356641 ]\n",
      " [ 0.0884271 ]\n",
      " [ 0.33876866]\n",
      " [-1.2498374 ]\n",
      " [ 1.6761293 ]\n",
      " [-1.3367964 ]\n",
      " [ 1.1831032 ]\n",
      " [-0.29080734]\n",
      " [ 1.6079314 ]\n",
      " [ 0.3353808 ]\n",
      " [-1.1454853 ]\n",
      " [ 0.44609627]\n",
      " [-0.21087201]\n",
      " [ 0.98961145]\n",
      " [ 0.26444343]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd56386a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.5535558  0.5487622  0.8709208  0.8417728]])\n",
      "        self       = needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.2....26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]]), needle.Tensor([[-0.5535558  0.5487622  0.8709208  0.8417728]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e8b0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.2....26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]]), needle.Tensor([[-0.5535558  0.5487622  0.8709208  0.8417728]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e8b0d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57431670>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd573f0ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd57430bf0>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd573f0ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]], device=cpu())\n",
      "        b          = NDArray([[-0.5535558  0.5487622  0.8709208  0.8417728]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd57e8b0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]], device=cpu())\n",
      "        b          = NDArray([[-0.5535558  0.5487622  0.8709208  0.8417728]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]], device=cpu())\n",
      "other = NDArray([[-0.5535558  0.5487622  0.8709208  0.8417728]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd56304070>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd56384970>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd573f2270>, 15, 1, 4\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.5535558  0.5487622  0.8709208  0.8417728]], device=cuda())\n",
      "out        = NDArray([[5.83371573e-10 4.24917598e-05 2.57848543e-09 1.17198671e-19]\n",
      " [4.00580506e-11 1.70890969e-04 4.10275014e-08 ...01562e-07 6.40969167e-10 7.21582475e+17]\n",
      " [1.24558228e-11 2.60741828e-09 1.74724264e-04 1.06503954e-32]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]\n",
      " [0.26232275]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[-0.6649252 ]\n",
      "  [ 0.38029644]\n",
      "  [-1.3894328 ]\n",
      "  [ 1.072507  ]\n",
      "  [ 0.14569615]\n",
      "  [-0.95396936]\n",
      "  [-1.19...87 ]\n",
      "  [-0.21970963]\n",
      "  [-0.7478883 ]\n",
      "  [ 0.41610444]\n",
      "  [ 0.18980184]\n",
      "  [ 0.9053593 ]\n",
      "  [-1.528875  ]\n",
      "  [ 1.7735552 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[ 1.0809662 ]\n",
      "  [-0.41086745]\n",
      "  [ 1.0544218 ]\n",
      "  [-0.75740933]\n",
      "  [-1.1105909 ]\n",
      "  [-0.10502028]\n",
      "  [-0.4...7 ]\n",
      "  [-0.21970963]\n",
      "  [-0.7478883 ]\n",
      "  [ 0.41610444]\n",
      "  [ 0.18980184]\n",
      "  [ 0.9053593 ]\n",
      "  [-1.528875  ]\n",
      "  [ 1.7735552 ]]]))\n",
      "h0         = needle.Tensor([[[ 1.0809662 ]\n",
      "  [-0.41086745]\n",
      "  [ 1.0544218 ]\n",
      "  [-0.75740933]\n",
      "  [-1.1105909 ]\n",
      "  [-0.10502028]\n",
      "  [-0.44...605]\n",
      "  [-1.1757506 ]\n",
      "  [ 2.3573835 ]\n",
      "  [-1.0817233 ]\n",
      "  [ 0.15411215]\n",
      "  [ 0.3936234 ]\n",
      "  [-0.01821619]\n",
      "  [ 1.5733694 ]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd561e6050>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]), (needle.Tensor([[[ 1.0809662 ]\n",
      "  [-0.41086745]\n",
      "  [ 1... ]\n",
      "  [-0.21970963]\n",
      "  [-0.7478883 ]\n",
      "  [ 0.41610444]\n",
      "  [ 0.18980184]\n",
      "  [ 0.9053593 ]\n",
      "  [-1.528875  ]\n",
      "  [ 1.7735552 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd561e6050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = (needle.Tensor([[[ 1.0809662 ]\n",
      "  [-0.41086745]\n",
      "  [ 1.0544218 ]\n",
      "  [-0.75740933]\n",
      "  [-1.1105909 ]\n",
      "  [-0.10502028]\n",
      "  [-0.4...7 ]\n",
      "  [-0.21970963]\n",
      "  [-0.7478883 ]\n",
      "  [ 0.41610444]\n",
      "  [ 0.18980184]\n",
      "  [ 0.9053593 ]\n",
      "  [-1.528875  ]\n",
      "  [ 1.7735552 ]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd561e6050>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.... ]\n",
      "  [-0.21970963]\n",
      "  [-0.7478883 ]\n",
      "  [ 0.41610444]\n",
      "  [ 0.18980184]\n",
      "  [ 0.9053593 ]\n",
      "  [-1.528875  ]\n",
      "  [ 1.7735552 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd561e5a10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]\n",
      "  [0.5090405]]])\n",
      "        batch_size = 15\n",
      "        c          = needle.Tensor([[-0.6649252 ]\n",
      " [ 0.38029644]\n",
      " [-1.3894328 ]\n",
      " [ 1.072507  ]\n",
      " [ 0.14569615]\n",
      " [-0.95396936]\n",
      " [-1.1968629 ]\n",
      " [-0.7026619 ]\n",
      " [ 1.6165737 ]\n",
      " [ 1.3925008 ]\n",
      " [ 0.63702786]\n",
      " [ 0.206788  ]\n",
      " [ 1.2080156 ]\n",
      " [-0.5563123 ]\n",
      " [ 1.3488485 ]])\n",
      "        c0         = (needle.Tensor([[-0.6649252 ]\n",
      " [ 0.38029644]\n",
      " [-1.3894328 ]\n",
      " [ 1.072507  ]\n",
      " [ 0.14569615]\n",
      " [-0.95396936]\n",
      " [-1.1968629 ...0.7682987 ]\n",
      " [-0.21970963]\n",
      " [-0.7478883 ]\n",
      " [ 0.41610444]\n",
      " [ 0.18980184]\n",
      " [ 0.9053593 ]\n",
      " [-1.528875  ]\n",
      " [ 1.7735552 ]]))\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[ 1.0809662 ]\n",
      " [-0.41086745]\n",
      " [ 1.0544218 ]\n",
      " [-0.75740933]\n",
      " [-1.1105909 ]\n",
      " [-0.10502028]\n",
      " [-0.4438581 ]\n",
      " [-1.1260985 ]\n",
      " [ 0.5105668 ]\n",
      " [ 2.3447285 ]\n",
      " [ 1.0160878 ]\n",
      " [ 0.6524773 ]\n",
      " [-0.93502635]\n",
      " [ 0.2202915 ]\n",
      " [ 0.11172048]])\n",
      "        h0         = (needle.Tensor([[ 1.0809662 ]\n",
      " [-0.41086745]\n",
      " [ 1.0544218 ]\n",
      " [-0.75740933]\n",
      " [-1.1105909 ]\n",
      " [-0.10502028]\n",
      " [-0.4438581 ...0.20395605]\n",
      " [-1.1757506 ]\n",
      " [ 2.3573835 ]\n",
      " [-1.0817233 ]\n",
      " [ 0.15411215]\n",
      " [ 0.3936234 ]\n",
      " [-0.01821619]\n",
      " [ 1.5733694 ]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]])\n",
      "        inputs     = [needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]])]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd561e6ad0>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd561e5a10>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]....7026619 ]\n",
      " [ 1.6165737 ]\n",
      " [ 1.3925008 ]\n",
      " [ 0.63702786]\n",
      " [ 0.206788  ]\n",
      " [ 1.2080156 ]\n",
      " [-0.5563123 ]\n",
      " [ 1.3488485 ]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd561e6ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[ 0.05499351 -0.00659436  0.11192441 -0.51542366]\n",
      " [ 0.05499351 -0.00659436  0.11192441 -0.51542366]\n",
      " [...41 -0.51542366]\n",
      " [ 0.05499351 -0.00659436  0.11192441 -0.51542366]\n",
      " [ 0.05499351 -0.00659436  0.11192441 -0.51542366]])\n",
      "        bias_ih    = needle.Tensor([[ 0.648286   -0.87528783 -0.9913548   0.2673229 ]\n",
      " [ 0.648286   -0.87528783 -0.9913548   0.2673229 ]\n",
      " [...8   0.2673229 ]\n",
      " [ 0.648286   -0.87528783 -0.9913548   0.2673229 ]\n",
      " [ 0.648286   -0.87528783 -0.9913548   0.2673229 ]])\n",
      "        c0         = needle.Tensor([[-0.6649252 ]\n",
      " [ 0.38029644]\n",
      " [-1.3894328 ]\n",
      " [ 1.072507  ]\n",
      " [ 0.14569615]\n",
      " [-0.95396936]\n",
      " [-1.1968629 ]\n",
      " [-0.7026619 ]\n",
      " [ 1.6165737 ]\n",
      " [ 1.3925008 ]\n",
      " [ 0.63702786]\n",
      " [ 0.206788  ]\n",
      " [ 1.2080156 ]\n",
      " [-0.5563123 ]\n",
      " [ 1.3488485 ]])\n",
      "        h          = (needle.Tensor([[ 1.0809662 ]\n",
      " [-0.41086745]\n",
      " [ 1.0544218 ]\n",
      " [-0.75740933]\n",
      " [-1.1105909 ]\n",
      " [-0.10502028]\n",
      " [-0.4438581 ...0.7026619 ]\n",
      " [ 1.6165737 ]\n",
      " [ 1.3925008 ]\n",
      " [ 0.63702786]\n",
      " [ 0.206788  ]\n",
      " [ 1.2080156 ]\n",
      " [-0.5563123 ]\n",
      " [ 1.3488485 ]]))\n",
      "        h0         = needle.Tensor([[ 1.0809662 ]\n",
      " [-0.41086745]\n",
      " [ 1.0544218 ]\n",
      " [-0.75740933]\n",
      " [-1.1105909 ]\n",
      " [-0.10502028]\n",
      " [-0.4438581 ]\n",
      " [-1.1260985 ]\n",
      " [ 0.5105668 ]\n",
      " [ 2.3447285 ]\n",
      " [ 1.0160878 ]\n",
      " [ 0.6524773 ]\n",
      " [-0.93502635]\n",
      " [ 0.2202915 ]\n",
      " [ 0.11172048]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd561e6ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]])\n",
      "        self       = needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]....5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]]), needle.Tensor([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58921590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]....5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]]), needle.Tensor([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58921590>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56c8c430>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd58921690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd56c8c2b0>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd58921690>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]], device=cpu())\n",
      "        b          = NDArray([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd58921590>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]], device=cpu())\n",
      "        b          = NDArray([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]], device=cpu())\n",
      "other = NDArray([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd561e6970>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd561e7df0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd58921bf0>, 15, 1, 4\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[ 0.8326824   0.75182986  0.10743284 -0.5782135 ]], device=cuda())\n",
      "out        = NDArray([[5.83371573e-10 4.24917598e-05 2.57848543e-09 1.17198671e-19]\n",
      " [4.00580506e-11 1.70890969e-04 4.10275014e-08 ...01562e-07 6.40969167e-10 7.21582475e+17]\n",
      " [1.24558228e-11 2.60741828e-09 1.74724264e-04 1.06503954e-32]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]\n",
      " [0.5090405]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 15\n",
      "c0         = needle.Tensor([[[ 0.25454342]\n",
      "  [-0.9938372 ]\n",
      "  [-1.0073295 ]\n",
      "  [ 0.8680227 ]\n",
      "  [ 0.5116187 ]\n",
      "  [-0.95611364]\n",
      "  [ 1.47...16 ]\n",
      "  [-0.5162816 ]\n",
      "  [ 1.753106  ]\n",
      "  [ 0.20653121]\n",
      "  [-0.5805698 ]\n",
      "  [ 0.55709034]\n",
      "  [-0.79481876]\n",
      "  [ 0.6110699 ]]])\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "h          = (needle.Tensor([[[-0.01510845]\n",
      "  [ 0.56155246]\n",
      "  [-0.9207827 ]\n",
      "  [-0.72145575]\n",
      "  [-0.65556836]\n",
      "  [ 0.09882833]\n",
      "  [-2.3...6 ]\n",
      "  [-0.5162816 ]\n",
      "  [ 1.753106  ]\n",
      "  [ 0.20653121]\n",
      "  [-0.5805698 ]\n",
      "  [ 0.55709034]\n",
      "  [-0.79481876]\n",
      "  [ 0.6110699 ]]]))\n",
      "h0         = needle.Tensor([[[-0.01510845]\n",
      "  [ 0.56155246]\n",
      "  [-0.9207827 ]\n",
      "  [-0.72145575]\n",
      "  [-0.65556836]\n",
      "  [ 0.09882833]\n",
      "  [-2.30...22 ]\n",
      "  [-1.1366965 ]\n",
      "  [ 0.36954856]\n",
      "  [-0.39889684]\n",
      "  [ 1.5289861 ]\n",
      "  [ 1.2018735 ]\n",
      "  [ 0.1253146 ]\n",
      "  [ 0.03699002]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd57aa7050>\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0.... ]\n",
      "  [-0.5162816 ]\n",
      "  [ 1.753106  ]\n",
      "  [ 0.20653121]\n",
      "  [-0.5805698 ]\n",
      "  [ 0.55709034]\n",
      "  [-0.79481876]\n",
      "  [ 0.6110699 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd57aa7050>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 15\n",
      "        h          = (needle.Tensor([[[-0.01510845]\n",
      "  [ 0.56155246]\n",
      "  [-0.9207827 ]\n",
      "  [-0.72145575]\n",
      "  [-0.65556836]\n",
      "  [ 0.09882833]\n",
      "  [-2.3...6 ]\n",
      "  [-0.5162816 ]\n",
      "  [ 1.753106  ]\n",
      "  [ 0.20653121]\n",
      "  [-0.5805698 ]\n",
      "  [ 0.55709034]\n",
      "  [-0.79481876]\n",
      "  [ 0.6110699 ]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd57aa7050>\n",
      "        seq_len    = 13\n",
      "        x          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. ...0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n",
      "        x_emb      = needle.Tensor([[[0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3...913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.... ]\n",
      "  [-0.5162816 ]\n",
      "  [ 1.753106  ]\n",
      "  [ 0.20653121]\n",
      "  [-0.5805698 ]\n",
      "  [ 0.55709034]\n",
      "  [-0.79481876]\n",
      "  [ 0.6110699 ]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd57aa7fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3...913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]\n",
      "  [0.3662913]]])\n",
      "        batch_size = 15\n",
      "        c          = needle.Tensor([[ 0.25454342]\n",
      " [-0.9938372 ]\n",
      " [-1.0073295 ]\n",
      " [ 0.8680227 ]\n",
      " [ 0.5116187 ]\n",
      " [-0.95611364]\n",
      " [ 1.4723058 ]\n",
      " [ 0.9138885 ]\n",
      " [-0.1164834 ]\n",
      " [ 1.1866742 ]\n",
      " [-0.09756786]\n",
      " [ 0.4259767 ]\n",
      " [-1.2724396 ]\n",
      " [-0.6016818 ]\n",
      " [ 0.58319736]])\n",
      "        c0         = (needle.Tensor([[ 0.25454342]\n",
      " [-0.9938372 ]\n",
      " [-1.0073295 ]\n",
      " [ 0.8680227 ]\n",
      " [ 0.5116187 ]\n",
      " [-0.95611364]\n",
      " [ 1.4723058 ...1.1483716 ]\n",
      " [-0.5162816 ]\n",
      " [ 1.753106  ]\n",
      " [ 0.20653121]\n",
      " [-0.5805698 ]\n",
      " [ 0.55709034]\n",
      " [-0.79481876]\n",
      " [ 0.6110699 ]]))\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[-0.01510845]\n",
      " [ 0.56155246]\n",
      " [-0.9207827 ]\n",
      " [-0.72145575]\n",
      " [-0.65556836]\n",
      " [ 0.09882833]\n",
      " [-2.3081915 ]\n",
      " [-0.6170331 ]\n",
      " [ 0.5498557 ]\n",
      " [-1.362663  ]\n",
      " [-0.25786448]\n",
      " [ 0.8096936 ]\n",
      " [-0.7055801 ]\n",
      " [-2.0190952 ]\n",
      " [-0.4075986 ]])\n",
      "        h0         = (needle.Tensor([[-0.01510845]\n",
      " [ 0.56155246]\n",
      " [-0.9207827 ]\n",
      " [-0.72145575]\n",
      " [-0.65556836]\n",
      " [ 0.09882833]\n",
      " [-2.3081915 ...1.6659322 ]\n",
      " [-1.1366965 ]\n",
      " [ 0.36954856]\n",
      " [-0.39889684]\n",
      " [ 1.5289861 ]\n",
      " [ 1.2018735 ]\n",
      " [ 0.1253146 ]\n",
      " [ 0.03699002]]))\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]])\n",
      "        inputs     = [needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]...662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]]), ...]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd57aa61d0>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd57aa7fd0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]....9138885 ]\n",
      " [-0.1164834 ]\n",
      " [ 1.1866742 ]\n",
      " [-0.09756786]\n",
      " [ 0.4259767 ]\n",
      " [-1.2724396 ]\n",
      " [-0.6016818 ]\n",
      " [ 0.58319736]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd57aa61d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n",
      "    gates = X @ \u001b[96mself\u001b[39;49;00m.W_ih + bias_ih + h0 @ \u001b[96mself\u001b[39;49;00m.W_hh + bias_hh\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]])\n",
      "        batch_size = 15\n",
      "        bias_hh    = needle.Tensor([[-0.8328347  0.2602036 -0.8232155 -0.8081079]\n",
      " [-0.8328347  0.2602036 -0.8232155 -0.8081079]\n",
      " [-0.83283... -0.8232155 -0.8081079]\n",
      " [-0.8328347  0.2602036 -0.8232155 -0.8081079]\n",
      " [-0.8328347  0.2602036 -0.8232155 -0.8081079]])\n",
      "        bias_ih    = needle.Tensor([[-0.6835693  -0.35463417 -0.93770736 -0.09833407]\n",
      " [-0.6835693  -0.35463417 -0.93770736 -0.09833407]\n",
      " [...36 -0.09833407]\n",
      " [-0.6835693  -0.35463417 -0.93770736 -0.09833407]\n",
      " [-0.6835693  -0.35463417 -0.93770736 -0.09833407]])\n",
      "        c0         = needle.Tensor([[ 0.25454342]\n",
      " [-0.9938372 ]\n",
      " [-1.0073295 ]\n",
      " [ 0.8680227 ]\n",
      " [ 0.5116187 ]\n",
      " [-0.95611364]\n",
      " [ 1.4723058 ]\n",
      " [ 0.9138885 ]\n",
      " [-0.1164834 ]\n",
      " [ 1.1866742 ]\n",
      " [-0.09756786]\n",
      " [ 0.4259767 ]\n",
      " [-1.2724396 ]\n",
      " [-0.6016818 ]\n",
      " [ 0.58319736]])\n",
      "        h          = (needle.Tensor([[-0.01510845]\n",
      " [ 0.56155246]\n",
      " [-0.9207827 ]\n",
      " [-0.72145575]\n",
      " [-0.65556836]\n",
      " [ 0.09882833]\n",
      " [-2.3081915 ...0.9138885 ]\n",
      " [-0.1164834 ]\n",
      " [ 1.1866742 ]\n",
      " [-0.09756786]\n",
      " [ 0.4259767 ]\n",
      " [-1.2724396 ]\n",
      " [-0.6016818 ]\n",
      " [ 0.58319736]]))\n",
      "        h0         = needle.Tensor([[-0.01510845]\n",
      " [ 0.56155246]\n",
      " [-0.9207827 ]\n",
      " [-0.72145575]\n",
      " [-0.65556836]\n",
      " [ 0.09882833]\n",
      " [-2.3081915 ]\n",
      " [-0.6170331 ]\n",
      " [ 0.5498557 ]\n",
      " [-1.362663  ]\n",
      " [-0.25786448]\n",
      " [ 0.8096936 ]\n",
      " [-0.7055801 ]\n",
      " [-2.0190952 ]\n",
      " [-0.4075986 ]])\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd57aa61d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[-0.6269907  -0.43706006  0.47056007  0.38247776]])\n",
      "        self       = needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]....3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]]), needle.Tensor([[-0.6269907  -0.43706006  0.47056007  0.38247776]]))\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5833f1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]....3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]]), needle.Tensor([[-0.6269907  -0.43706006  0.47056007  0.38247776]]))\n",
      "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5833f1d0>\n",
      "        tensor     = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5830e3b0>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd5833fbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError('matmul(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: nee...rray.ndarray_backend_cpu.Array object at 0x7fbd5830def0>, 15, 1, 4') raised in repr()] Tensor object at 0x7fbd5833fbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:245: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.matmul(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]], device=cpu())\n",
      "        b          = NDArray([[-0.6269907  -0.43706006  0.47056007  0.38247776]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7fbd5833f1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:690: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]], device=cpu())\n",
      "        b          = NDArray([[-0.6269907  -0.43706006  0.47056007  0.38247776]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]], device=cpu())\n",
      "other = NDArray([[-0.6269907  -0.43706006  0.47056007  0.38247776]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__matmul__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Matrix multplication of two arrays.  This requires that both arrays\u001b[39;49;00m\n",
      "    \u001b[33m    be 2D (i.e., we don't handle batch matrix multiplication), and that the\u001b[39;49;00m\n",
      "    \u001b[33m    sizes match up properly for matrix multiplication.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    In the case of the CPU backend, you will implement an efficient \"tiled\"\u001b[39;49;00m\n",
      "    \u001b[33m    version of matrix multiplication for the case when all dimensions of\u001b[39;49;00m\n",
      "    \u001b[33m    the array are divisible by self.device.__tile_size__.  In this case,\u001b[39;49;00m\n",
      "    \u001b[33m    the code below will restride and compact the matrix into tiled form,\u001b[39;49;00m\n",
      "    \u001b[33m    and then pass to the relevant CPU backend.  For the CPU version we will\u001b[39;49;00m\n",
      "    \u001b[33m    just fall back to the naive CPU implementation if the array shape is not\u001b[39;49;00m\n",
      "    \u001b[33m    a multiple of the tile size\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The GPU (and numpy) versions don't have any tiled version (or rather,\u001b[39;49;00m\n",
      "    \u001b[33m    the GPU version will just work natively by tiling any input size).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ndim == \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m other.ndim == \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m] == other.shape[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        m, n, p = \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.shape[\u001b[94m1\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# if the matrix is aligned, use tiled matrix multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.device, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatmul_tiled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            d % \u001b[96mself\u001b[39;49;00m.device.__tile_size__ == \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m d \u001b[95min\u001b[39;49;00m (m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mdef\u001b[39;49;00m \u001b[92mtile\u001b[39;49;00m(a, tile):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mreturn\u001b[39;49;00m a.as_strided(\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m0\u001b[39;49;00m] // tile, a.shape[\u001b[94m1\u001b[39;49;00m] // tile, tile, tile),\u001b[90m\u001b[39;49;00m\n",
      "                    (a.shape[\u001b[94m1\u001b[39;49;00m] * tile, tile, a.shape[\u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            t = \u001b[96mself\u001b[39;49;00m.device.__tile_size__\u001b[90m\u001b[39;49;00m\n",
      "            a = tile(\u001b[96mself\u001b[39;49;00m.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            b = tile(other.compact(), t).compact()\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((a.shape[\u001b[94m0\u001b[39;49;00m], b.shape[\u001b[94m1\u001b[39;49;00m], t, t), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.device.matmul_tiled(a._handle, b._handle, out._handle, m, n, p)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "                out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "                .compact()\u001b[90m\u001b[39;49;00m\n",
      "                .reshape((\u001b[96mself\u001b[39;49;00m.shape[\u001b[94m0\u001b[39;49;00m], other.shape[\u001b[94m1\u001b[39;49;00m]))\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.compact()._handle, other.compact()._handle, out._handle, m, n, p\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: matmul(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: needle.backend_ndarray.ndarray_backend_cpu.Array, arg3: int, arg4: int, arg5: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd57a52f70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fbd57aa41f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fbd5833ec70>, 15, 1, 4\u001b[0m\n",
      "\n",
      "m          = 15\n",
      "n          = 1\n",
      "other      = NDArray([[-0.6269907  -0.43706006  0.47056007  0.38247776]], device=cuda())\n",
      "out        = NDArray([[1.3563128e-19 1.3563156e-19 1.8168666e+31 7.0947799e+22]\n",
      " [6.8608197e+22 7.1440436e+31 2.1041306e-19 4.61681...9 2.4686988e+20 3.6713389e+03 4.7427375e+30]\n",
      " [1.9516541e-19 1.7668072e+22 2.0700511e-19 2.9588602e+21]], device=cpu())\n",
      "p          = 4\n",
      "self       = NDArray([[0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]\n",
      " [0.3662913]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:557: TypeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "                            init_hidden, output_size, seq_model, device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = (h0, c0)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                h = h0\u001b[90m\u001b[39;49;00m\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), h)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 1\n",
      "c0         = needle.Tensor([[[0.28124657]]])\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h          = (needle.Tensor([[[1.5371196]]]), needle.Tensor([[[0.28124657]]]))\n",
      "h0         = needle.Tensor([[[1.5371196]]])\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "model      = <models.LanguageModel object at 0x7fbd5705e3d0>\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_sequence_models.py\u001b[0m:207: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.]]), (needle.Tensor([[[1.5371196]]]), needle.Tensor([[[0.28124657]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705e3d0>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:89: in forward\n",
      "    out, h = \u001b[96mself\u001b[39;49;00m.seq_model(x_emb, h)\u001b[90m\u001b[39;49;00m\n",
      "        bs         = 1\n",
      "        h          = (needle.Tensor([[[1.5371196]]]), needle.Tensor([[[0.28124657]]]))\n",
      "        self       = <models.LanguageModel object at 0x7fbd5705e3d0>\n",
      "        seq_len    = 1\n",
      "        x          = needle.Tensor([[0.]])\n",
      "        x_emb      = needle.Tensor([[[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "   0.37034762 0.51891994 0.4564575 ...   0.9885166  0.67656326 0.3190259  0.11263249 0.06932371 0.19830409\n",
      "   0.77250737 0.73659325 0.13611934 0.03288761]]])\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "   0.37034762 0.51891994 0.4564575...\n",
      "   0.77250737 0.73659325 0.13611934 0.03288761]]]), (needle.Tensor([[[1.5371196]]]), needle.Tensor([[[0.28124657]]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd5705c6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:305: in forward\n",
      "    h, c = layer(\u001b[96minput\u001b[39;49;00m, (h, c))\u001b[90m\u001b[39;49;00m\n",
      "        X          = needle.Tensor([[[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "   0.37034762 0.51891994 0.4564575 ...   0.9885166  0.67656326 0.3190259  0.11263249 0.06932371 0.19830409\n",
      "   0.77250737 0.73659325 0.13611934 0.03288761]]])\n",
      "        batch_size = 1\n",
      "        c          = needle.Tensor([[0.28124657]])\n",
      "        c0         = (needle.Tensor([[0.28124657]]),)\n",
      "        c_n        = []\n",
      "        h          = needle.Tensor([[1.5371196]])\n",
      "        h0         = (needle.Tensor([[1.5371196]]),)\n",
      "        h_n        = []\n",
      "        input      = needle.Tensor([[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "  0.37034762 0.51891994 0.4564575  0...07\n",
      "  0.9885166  0.67656326 0.3190259  0.11263249 0.06932371 0.19830409\n",
      "  0.77250737 0.73659325 0.13611934 0.03288761]])\n",
      "        inputs     = [needle.Tensor([[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "  0.37034762 0.51891994 0.4564575  ...7\n",
      "  0.9885166  0.67656326 0.3190259  0.11263249 0.06932371 0.19830409\n",
      "  0.77250737 0.73659325 0.13611934 0.03288761]])]\n",
      "        layer      = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd5705e550>\n",
      "        self       = <needle.nn.nn_sequence.LSTM object at 0x7fbd5705c6d0>\n",
      "        t          = 0\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[0.13322651 0.5436364  0.00985517 0.5116362  0.5565978  0.63529396\n",
      "  0.37034762 0.51891994 0.4564575  ...830409\n",
      "  0.77250737 0.73659325 0.13611934 0.03288761]]), (needle.Tensor([[1.5371196]]), needle.Tensor([[0.28124657]])))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_sequence.LSTMCell object at 0x7fbd5705e550>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_sequence.py\u001b[0m:228: in forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.4, pluggy-1.0.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/project/10-714/hw4\n",
      "plugins: anyio-4.2.0\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/hw4/test_sequence_models.py::test_language_model_training[cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_sequence_models.py::test_language_model_training[cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 3.39s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
